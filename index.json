[{"categories":["DevOps","转载"],"content":"篇幅有限，点击下方链接进行访问。 DevOps Note-01（中文， @sunrisenan 备份站点）：http://bak.agou-ops.top/ DevOps Note-02（英文， @vinayhegde1990 转载）：http://agou-ops.top/devops ","date":"2020-10-27","objectID":"/devops-notes/:0:0","tags":["DevOps","转载"],"title":"DevOps - Notes","uri":"/devops-notes/"},{"categories":["kubernetes","prometheus","database","grafana"],"content":"预先准备 minikube For Windows(资源充足, 有集群更好不过); Helm; 科学上网能力. ","date":"2020-10-08","objectID":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/:1:0","tags":["k8s","prometheus","mongodb","grafana"],"title":"在k8s上使用Prometheus监控MongoDB","uri":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/"},{"categories":["kubernetes","prometheus","database","grafana"],"content":"整体框架 随手在线画的, 不美观, qwq. ","date":"2020-10-08","objectID":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/:2:0","tags":["k8s","prometheus","mongodb","grafana"],"title":"在k8s上使用Prometheus监控MongoDB","uri":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/"},{"categories":["kubernetes","prometheus","database","grafana"],"content":"操作步骤 ","date":"2020-10-08","objectID":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/:3:0","tags":["k8s","prometheus","mongodb","grafana"],"title":"在k8s上使用Prometheus监控MongoDB","uri":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/"},{"categories":["kubernetes","prometheus","database","grafana"],"content":"启用helm-tiller(helm服务器端) 在minikube中启用helm-tiller插件, 很简单, 只需要一条命令即可: $ minikube addons enable helm-tiller # 或者在启动 minikube 的时候直接启用 helm-tiller $ minikube start --addons=[\"helm-tiller\"] \u003cOther_Options\u003e ","date":"2020-10-08","objectID":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/:3:1","tags":["k8s","prometheus","mongodb","grafana"],"title":"在k8s上使用Prometheus监控MongoDB","uri":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/"},{"categories":["kubernetes","prometheus","database","grafana"],"content":"安装Prometheus-operator 首先添加所需仓库: $ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts # 更新仓库内容 $ helm repo update 安装/下载chart: $ helm install prometheus-community/kube-prometheus-stack --version 9.4.10 # 或者先下载下来更改所需内容之后再进行安装(推荐使用方法) $ helm pull prometheus-community/kube-prometheus-stack --version 9.4.10 --untar # 安装 $ helm install prometheus kube-prometheus-stack/ 安装完成之后, kubernetes会自动接管后续工作, 如拉取镜像等: # 观察部署进度 $ kubectl get po -w NAME READY STATUS RESTARTS AGE alertmanager-prometheus-kube-prometheus-alertmanager-0 2/2 Running 0 24m prometheus-grafana-79b677fd4b-t9kr9 2/2 Running 0 25m prometheus-kube-prometheus-operator-69cd74c99f-wltf4 2/2 Running 0 25m prometheus-kube-state-metrics-95d956569-mlfwv 1/1 Running 0 25m prometheus-prometheus-kube-prometheus-prometheus-0 3/3 Running 1 24m prometheus-prometheus-node-exporter-tvwf5 1/1 Running 0 25m # 查看service $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE alertmanager-operated ClusterIP None \u003cnone\u003e 9093/TCP,9094/TCP,9094/UDP 25m kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 44h prometheus-grafana ClusterIP 10.108.199.68 \u003cnone\u003e 80/TCP 26m prometheus-kube-prometheus-alertmanager ClusterIP 10.96.81.7 \u003cnone\u003e 9093/TCP 26m prometheus-kube-prometheus-operator ClusterIP 10.109.9.111 \u003cnone\u003e 8080/TCP,443/TCP 26m prometheus-kube-prometheus-prometheus ClusterIP 10.98.173.241 \u003cnone\u003e 9090/TCP 26m prometheus-kube-state-metrics ClusterIP 10.109.121.40 \u003cnone\u003e 8080/TCP 26m prometheus-operated ClusterIP None \u003cnone\u003e 9090/TCP 25m prometheus-prometheus-node-exporter ClusterIP 10.107.29.93 \u003cnone\u003e 9100/TCP 26m 以上信息无误则表明prometheus已成功部署. 使用minikube(kubectl亦可)的转发功能, 将svc/prometheus-kube-prometheus-prometheus 的端口映射到本地: $ minikube port-forward svc/prometheus-kube-prometheus-prometheus 9090 Forwarding from 127.0.0.1:9090 -\u003e 9090 Forwarding from [::1]:9090 -\u003e 9090 Handling connection for 9090 打开浏览器访问http://127.0.0.1:9090/查看prometheus UI: 使用以下命令查看prometheus的servicemonitor(其对应prometheus UI中的target): $ kubectl get servicemonitor NAME AGE prometheus-kube-prometheus-alertmanager 36m prometheus-kube-prometheus-apiserver 36m prometheus-kube-prometheus-coredns 36m prometheus-kube-prometheus-grafana 36m prometheus-kube-prometheus-kube-controller-manager 36m prometheus-kube-prometheus-kube-etcd 36m prometheus-kube-prometheus-kube-proxy 36m prometheus-kube-prometheus-kube-scheduler 36m prometheus-kube-prometheus-kube-state-metrics 36m prometheus-kube-prometheus-kubelet 36m prometheus-kube-prometheus-node-exporter 36m prometheus-kube-prometheus-operator 36m prometheus-kube-prometheus-prometheus 36m 查看所有crd: $ kubectl get crd NAME CREATED AT alertmanagers.monitoring.coreos.com 2020-10-08T01:10:04Z podmonitors.monitoring.coreos.com 2020-10-08T01:10:04Z prometheuses.monitoring.coreos.com 2020-10-08T01:10:04Z prometheusrules.monitoring.coreos.com 2020-10-08T01:10:04Z servicemonitors.monitoring.coreos.com 2020-10-08T01:10:04Z thanosrulers.monitoring.coreos.com 2020-10-08T01:10:04Z 通过describe子命令可以发现crd的matchLabels都包含release: prometheus标签. ","date":"2020-10-08","objectID":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/:3:2","tags":["k8s","prometheus","mongodb","grafana"],"title":"在k8s上使用Prometheus监控MongoDB","uri":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/"},{"categories":["kubernetes","prometheus","database","grafana"],"content":"部署MongoDB及其服务 参考MongoDB-depl-svc.yaml文件内容如下: apiVersion:apps/v1kind:Deploymentmetadata:name:mongodb-deploymentlabels:app:mongodbspec:replicas:2selector:matchLabels:app:mongodbtemplate:metadata:labels:app:mongodbspec:containers:- name:mongodbimage:mongoports:- containerPort:27017---apiVersion:v1kind:Servicemetadata:name:mongodb-servicespec:selector:app:mongodbports:- protocol:TCPport:27017targetPort:27017 按需进行修改, 修改完成之后应用配置清单: $ kubectl apply -f MongoDB-depl-svc.yaml 等待MongoDB部署完成… $ kubectl get deployment/mongodb-deployment -w NAME READY UP-TO-DATE AVAILABLE AGE mongodb-deployment 2/2 2 0 3m13s ","date":"2020-10-08","objectID":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/:3:3","tags":["k8s","prometheus","mongodb","grafana"],"title":"在k8s上使用Prometheus监控MongoDB","uri":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/"},{"categories":["kubernetes","prometheus","database","grafana"],"content":"部署MongoDB Exporter 添加helm repo: $ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts $ helm repo update 下载chart: $ helm pull prometheus-community/prometheus-mongodb-exporter --untar 修改value.yaml文件内容: ...# 使用kubectl get svc获取MongoDB的服务及端口mongodb:uri:\"mongodb://mongodb-service:27017\"...# 从上面可以得知servicemonitor所匹配的标签 release: prometheusserviceMonitor:additionalLabels:release:prometheus... 应用chart: $ helm install mongodb-exporter prometheus-mongodb-exporter/ 等待svc,pod部署完成之后, 映射mongodb-exporter, 检查/metrics: $ kubectl port-forward service/mongodb-exporter-prometheus-mongodb-exporter 9216 打开浏览器访问http://localhost:9216/metrics进行查看: 此时, 再打开prometheus UI查看target可以看到mongodb-exporter已经处于UP状态: ","date":"2020-10-08","objectID":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/:3:4","tags":["k8s","prometheus","mongodb","grafana"],"title":"在k8s上使用Prometheus监控MongoDB","uri":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/"},{"categories":["kubernetes","prometheus","database","grafana"],"content":"在Grafana查看 $ kubectl port-forward deployment/prometheus-grafana 3000 打开浏览器访问http://localhost:3000: :information_source:默认账号admin, 默认密码prom-operator (从官方文档中即可获取) ","date":"2020-10-08","objectID":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/:3:5","tags":["k8s","prometheus","mongodb","grafana"],"title":"在k8s上使用Prometheus监控MongoDB","uri":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/"},{"categories":["kubernetes","prometheus","database","grafana"],"content":"附录1: chart 地址 上文中所使用的chart仓库地址: kube-prometheus-stack: https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack mongodb-exporter: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-mongodb-exporter ","date":"2020-10-08","objectID":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/:4:0","tags":["k8s","prometheus","mongodb","grafana"],"title":"在k8s上使用Prometheus监控MongoDB","uri":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/"},{"categories":["kubernetes","prometheus","database","grafana"],"content":"附录2: kubectl get all ","date":"2020-10-08","objectID":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/:5:0","tags":["k8s","prometheus","mongodb","grafana"],"title":"在k8s上使用Prometheus监控MongoDB","uri":"/%E5%9C%A8k8s%E4%B8%8A%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7mongodb/"},{"categories":["Anime"],"content":"My Anime Trip.","date":"2020-09-30","objectID":"/anime/","tags":["Anime"],"title":"我的动漫之旅(My Anime Trip.)","uri":"/anime/"},{"categories":["Anime"],"content":"按照观看结束时间排序，序号仅用于计数。(2020-03-28 至 ????-??-??) 变更日志： 2020年10月01日：初始化表格； ","date":"2020-09-30","objectID":"/anime/:0:0","tags":["Anime"],"title":"我的动漫之旅(My Anime Trip.)","uri":"/anime/"},{"categories":["Anime"],"content":"二〇二〇年 序号 时间 动漫名 最喜欢的角色 制片方 0 2020/03/28 小林家的龙女仆 托尔、康娜、尔科亚 京都 1 2020/04/02 《紫罗兰永恒花园》 薇尔莉特·伊芙加登 京都 2 2020/04/06 《吹响！上低音号》 黄前久美子(黄大叔)、中世古香织(小香织) 京都 3 2020/04/18 《轻音少女》(K-ON!) 平泽唯 京都 4 2020/04/24 《日常》 长野原美绪(美绪酱) 京都 5 2020/05/02 《CLANNAD》 古河渚、冈崎汐 京都 6 2020/05/07 《中二病也要谈恋爱》 七宫智音、小鸟游六花 京都 7 2020/05/12 《境界的彼方》 栗山未来 京都 8 2020/05/17 《甘城光辉游乐园》 千斗五十铃 京都 9 2020/05/21 《龙王的工作》 雏鹤爱、夏洛特·伊佐阿尔 project No.9 10 2020/05/26 《埃罗芒阿老师》 高砂智惠(店长)、和泉纱雾 A-1 Pictures 11 2020/05/26 《珈百璃的堕落》 胡桃泽·萨塔妮娅·麦克道威尔(萨塔尼亚)、白羽·菈菲尔·恩兹沃斯(拉菲) 动画工房 12 2020/05/28 《未闻花名》 本间芽衣子(面码) A-1 Pictures 13 2020/05/31 《路人女主的养成方法》 加藤惠、霞之丘诗羽 A-1 Pictures \u0026 CloverWorks 14 2020/06/01 《这个美术社有大问题！》 宇佐美瑞希 feel. 15 2020/06/06 《LoveLive!》(μ’s) 东条希(希)、矢泽妮可(妮可) 16 2020/06/15 《恋爱研究所》 真木夏绪、仓桥莉子(李狗) 动画工房 17 2020/06/17 《终将成为你》 七海灯子、小糸侑 TROYCA 18 2020/06/19 《妖精森林的小不点》 御子地、白明 Lerche 19 2020/06/22 《邻家索菲》 天野灯 Studio五组×AXsiZ 20 2020/06/26 《tari tari》 冲田纱羽 P.A.WORKS 21 2020/06/26 《摇曳露营》 志摩凛、各务原抚子 C-Station 22 2020/07/01 《花开伊吕波》 押水菜子 P.A.WORKS 23 2020/07/12 《樱花庄的宠物女孩》 上井草美咲、青山七海 J.C.STAFF 24 2020/07/23 《悠哉日常大王》 宫内莲华(喵帕斯)、越谷夏海 SILVER LINK. 25 2020/07/27 《怕痛的我，把防御力点满就对了》 梅普露 SILVER LINK. 26 2020/08/06 《NEW GAME!》 泷本一二三、饭岛结音(关西腔i了)、樱宁宁(宁宁鸡) 动画工房 27 2020/08/10 《Slow Start》 十仓荣依子、千石冠、一之濑花名 、 百地玉手 A-1 Pictures→CloverWorks[1] 芳文社 28 2020/08/15 《Comic Girls》 萌田薰子(小混沌) Nexus 芳文社 29 2020/08/19 《恋爱小行星》 木之幡米拉 动画工房 芳文社 30 2020/08/22 《街角魔族》 吉田优子(夏美子) J.C.STAFF 芳文社 31 2020/08/27 《属性咖啡馆》(Blend·S) 日向夏帆 A-1 Pictures 芳文社 32 2020/08/30 《未确认进行式》 三峰真白、夜之森小红(小红) 动画工房 33 2020/09/02 《天体的秩序》 古宫乃乃香、诺艾尔 Studio 3Hz 34 2020/09/08 《citrus》 蓝原柚子、蓝原芽衣 Passione 35 2020/09/15 《Anne Happy♪》 花小泉杏 SILVER LINK. 36 2020/09/24 《放学后海堤日记》 鹤木阳渚 动画工房 37 2020/09/25 《请问您今天要来点兔子吗？》 桐间纱路(纱路酱)、宇治松千夜](https://zh.moegirl.org.cn/宇治松千夜) Koi(原作) 38 2020/09/25 《宇崎酱想要玩耍！》 宇崎花 39 2020/09/29 《雏子的笔记》 夏川玖井菜(奇怪的爱好：喜欢吃书) Passione ","date":"2020-09-30","objectID":"/anime/:1:0","tags":["Anime"],"title":"我的动漫之旅(My Anime Trip.)","uri":"/anime/"},{"categories":["Anime"],"content":"其他 年度最喜欢的声优(及所配音角色)： 石川由依：薇尔莉特·伊芙加登、高砂智惠； 子安武人：克劳迪亚·霍金斯； 年度最喜欢的OP/ED/OST だんご大家族（团子大家族） 明日でいいから（明天也没关系） ne！ne！ne！（呐！呐！呐！） ","date":"2020-09-30","objectID":"/anime/:2:0","tags":["Anime"],"title":"我的动漫之旅(My Anime Trip.)","uri":"/anime/"},{"categories":["生活","日记"],"content":"购置背景 　长达两个多月的菏泽实习结束了，感觉如释重负，终于摆脱了实习带来的困扰和限制，心情也不由地变好了起来。在返校的火车上，闲着没事刷了刷闲鱼，看了看自己这段时间以来卖出的虚拟商品（代搭建博客/代下载GitHub资源等），大概赚了1k块钱左右吧，呃…还是蛮不错的。 　然后不经意之间就看到我上上学期买到的蜗牛星际B款双千（矿难无情人有情），想起来机子还在宿舍桌子上，大半年没动过了，估计已经吃灰了。正当我犹豫是挂闲鱼转手再卖出去时（能小赚一笔），我看到了二手的笔记本拆机屏幕，萌发起重振蜗牛星际的想法，又想了想实习也赚了点小钱，搞！。询问了一圈之后，最终买下了这款型号为友达(AUO)-B101EW05 V1(AUO51D4)的拆机屏，大小是10.1寸，最高分辨率为1280*800，参数如下图所示： 有人会问为啥非要买一个二手拆机屏呢？（其实根本没人问，这一切都是博主自己幻想出来的） 理由如下： 手上没有现成的显示屏，之前全是借隔壁宿舍的显示屏来用的; 笔记本显示屏驱动部件嵌在主板上，无法直接使用; 可以作为笔记本拓展屏，一箭双雕; 最后一点也是最最重要的一点，穷穷穷，没钱买拓展屏，只能自己买破烂凑一个。 ","date":"2020-09-14","objectID":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/:1:0","tags":["日记","闲鱼","垃圾佬"],"title":"【日记】闲鱼垃圾佬的一天","uri":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/"},{"categories":["生活","日记"],"content":"其他硬件设备购置 ","date":"2020-09-14","objectID":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/:2:0","tags":["日记","闲鱼","垃圾佬"],"title":"【日记】闲鱼垃圾佬的一天","uri":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/"},{"categories":["生活","日记"],"content":"驱动板 　光有显示屏还不行，还需要驱动板才能正常使用，不得不说闲鱼和淘宝是万能的，什么东西都能够买到，在苦苦寻找和对比了一番之后，最终找到了一个淘宝卖家（店铺链接：https：//m.tb.cn/h.VBYebL1?sm=6c9ebe ），销量和口碑都还不错，把显示屏型号发给卖家后，卖家制作完当天晚上就发货了，真是迅速啊，点个赞:+1:。 ","date":"2020-09-14","objectID":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/:2:1","tags":["日记","闲鱼","垃圾佬"],"title":"【日记】闲鱼垃圾佬的一天","uri":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/"},{"categories":["生活","日记"],"content":"mSATA 　蜗牛星际原装自带的mSATA是16g的，只够装个pve、VMware ESXi、iKuai、openwrt、群晖啥的，根本不够用的，然后嘞，自己又想安装个Windows Server 2008 R2挂一些软件，所以就下狠心来又买了一条128g的渣士顿mSATA固态硬盘，也是闲鱼买的，大概100软妹币，MLC颗粒的，还是蛮值的。 CrystalDiskMark跑分如下图所示： 　这样一来，拆蜗牛星际多出来的那块mSATA 16g岂不是吃灰了？不，垃圾佬不会就此止步，不会。 所以就有了下面这个设备:laughing:。 ","date":"2020-09-14","objectID":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/:2:2","tags":["日记","闲鱼","垃圾佬"],"title":"【日记】闲鱼垃圾佬的一天","uri":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/"},{"categories":["生活","日记"],"content":"USB3.0 TO mSATA Box 　这小东西功能很简单，就是把mSATA接口转为USB接口，以便直接插在电脑上直接使用。实体图如下所示： 还是蛮好看的，不错，不错。 ","date":"2020-09-14","objectID":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/:2:3","tags":["日记","闲鱼","垃圾佬"],"title":"【日记】闲鱼垃圾佬的一天","uri":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/"},{"categories":["生活","日记"],"content":"HDMI 线 \u0026 亚克力壳 　主要设备买完之后，突然发现还缺一条HDMI线，回宿舍找了找没找到原来那根，于是10块钱在闲鱼又买了条绿联0.75m的HDMI线，黄色扁线型的。 亚克力壳子是买驱动板时加价10块买的，用来保护驱动板各元件和增加观赏性用的。 ","date":"2020-09-14","objectID":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/:2:4","tags":["日记","闲鱼","垃圾佬"],"title":"【日记】闲鱼垃圾佬的一天","uri":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/"},{"categories":["生活","日记"],"content":"成果展示-附全家福 桌面图： ","date":"2020-09-14","objectID":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/:3:0","tags":["日记","闲鱼","垃圾佬"],"title":"【日记】闲鱼垃圾佬的一天","uri":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/"},{"categories":["生活","日记"],"content":"未来用途 　以后有空的话会专门写一篇文章来介绍蜗牛星际的用途，敬请期待吧！ ","date":"2020-09-14","objectID":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/:4:0","tags":["日记","闲鱼","垃圾佬"],"title":"【日记】闲鱼垃圾佬的一天","uri":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/"},{"categories":["生活","日记"],"content":"参考资料 【装机日常】废旧笔记本不要扔，拆下屏幕，裹上驱动板，做成副屏，隔壁小孩都馋哭啦！： https：//www.youtube.com/watch?v=V1wC3KKF2TM ","date":"2020-09-14","objectID":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/:5:0","tags":["日记","闲鱼","垃圾佬"],"title":"【日记】闲鱼垃圾佬的一天","uri":"/%E6%97%A5%E8%AE%B0%E9%97%B2%E9%B1%BC%E5%9E%83%E5%9C%BE%E4%BD%AC%E7%9A%84%E4%B8%80%E5%A4%A9/"},{"categories":["CentOS"],"content":" 搭建内网yum源，摆脱软件包困扰。 ","date":"2020-08-04","objectID":"/%E6%90%AD%E5%BB%BA%E5%86%85%E7%BD%91yum%E6%BA%90/:0:0","tags":["yum"],"title":"搭建内网yum源","uri":"/%E6%90%AD%E5%BB%BA%E5%86%85%E7%BD%91yum%E6%BA%90/"},{"categories":["CentOS"],"content":"选择合适稳定的公网yum仓库 建议选择对运营商网络友好的yum仓库 国内可供选择的有: 阿里云：http://mirrors.aliyun.com/ (这里我使用阿里云) 网易：http://mirrors.163.com/ 更多国内镜像仓库参考：https://blog.csdn.net/wyqwilliam/article/details/90581159 ","date":"2020-08-04","objectID":"/%E6%90%AD%E5%BB%BA%E5%86%85%E7%BD%91yum%E6%BA%90/:1:0","tags":["yum"],"title":"搭建内网yum源","uri":"/%E6%90%AD%E5%BB%BA%E5%86%85%E7%BD%91yum%E6%BA%90/"},{"categories":["CentOS"],"content":"安装nginx并修改配置文件 为了方便起见，我这里使用yum仓库进行安装. [root@centos-7 ~]# yum install nginx -y [root@centos-7 ~]# vi /etc/nginx/conf.d/default.conf # -----在server段添加以下内容 # 自动在index.html的索引打开 autoindex on; # 如果有文件则显示文件大小 autoindex_exact_size on; # 显示更改时间，以当前系统时间为准 autoindex_localtime on 具体位置在下面这个位置 启动nginx，使用service nignx start 即可 ","date":"2020-08-04","objectID":"/%E6%90%AD%E5%BB%BA%E5%86%85%E7%BD%91yum%E6%BA%90/:2:0","tags":["yum"],"title":"搭建内网yum源","uri":"/%E6%90%AD%E5%BB%BA%E5%86%85%E7%BD%91yum%E6%BA%90/"},{"categories":["CentOS"],"content":"创建自定义index.html文件 修改默认index.html为以下内容： \u003cp style=\"font-weight:bolder;color:green;font-size:30px;\"\u003eALL of the packages in the below:\u003c/p\u003e \u003cbr/\u003e \u003ca href=\"http://192.168.1.128/CentOS-YUM/Aliyun\"\u003eAliyun\u003c/a\u003e\u003cbr/\u003e These packagers from of CentOS ISO.\u003cbr/\u003e \u003ca href=\"http://192.168.1.128/CentOS7-aliyun\"\u003eCentOS\u003c/a\u003e\u003cbr/\u003e These packagers from of \"Internet service provider\".\u003cbr/\u003e \u003cp style=\"font-weight:bolder;color:red;font-size:18px;\"\u003ePlease replace the file and fill in the following content:\u003c/p\u003e \u003cp style=\"font-weight:bolder;color:blue;font-size:15px;\"\u003eWay: /etc/yum.repos.d/CentOS-Base.repo\u003c/p\u003e 在站点目录中创建CentOS-YUM/Aliyun（本地ISO文件仓库）和CentOS7-aliyun目录 ","date":"2020-08-04","objectID":"/%E6%90%AD%E5%BB%BA%E5%86%85%E7%BD%91yum%E6%BA%90/:3:0","tags":["yum"],"title":"搭建内网yum源","uri":"/%E6%90%AD%E5%BB%BA%E5%86%85%E7%BD%91yum%E6%BA%90/"},{"categories":["CentOS"],"content":"添加并同步阿里云yum源 添加阿里云CentOS7 yum源 # 注意备份原仓库源 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 同步阿里云CentOS7 yum源到之前创建好的目录当中，使用命令reposync -p /usr/share/nginx/html/CentOS7-aliyun 接着，使用createrepo -p /usr/share/nginx/html/CentOS7-aliyun来创建repodata文件 ","date":"2020-08-04","objectID":"/%E6%90%AD%E5%BB%BA%E5%86%85%E7%BD%91yum%E6%BA%90/:4:0","tags":["yum"],"title":"搭建内网yum源","uri":"/%E6%90%AD%E5%BB%BA%E5%86%85%E7%BD%91yum%E6%BA%90/"},{"categories":["CentOS"],"content":"客户端配置 使用以下命令为客户端添加内网仓库地址 yum-config-manager --add-repo=\"http://192.168.1.128/CentOS7-aliyun/base/Packages\" yum makecache # 更新缓存 ","date":"2020-08-04","objectID":"/%E6%90%AD%E5%BB%BA%E5%86%85%E7%BD%91yum%E6%BA%90/:5:0","tags":["yum"],"title":"搭建内网yum源","uri":"/%E6%90%AD%E5%BB%BA%E5%86%85%E7%BD%91yum%E6%BA%90/"},{"categories":["CentOS"],"content":"在yum仓库服务器端创建定时任务 新建shell更新脚本update-aliyun.sh，内容如下： #!/bin/bash /usr/bin/reposync -np /usr/share/nginx/html/CentOS7-aliyun 使用chmod +x update-aliyun.sh为其添加执行权限 添加定时任务(每5min执行一次) 5 * * * * root run-parts /root/update-aliyun.sh 其中run-parts的意思是执行后面目录中的脚本。 ","date":"2020-08-04","objectID":"/%E6%90%AD%E5%BB%BA%E5%86%85%E7%BD%91yum%E6%BA%90/:6:0","tags":["yum"],"title":"搭建内网yum源","uri":"/%E6%90%AD%E5%BB%BA%E5%86%85%E7%BD%91yum%E6%BA%90/"},{"categories":["Git"],"content":"Git 钩子和其它版本控制系统一样，Git 能在特定的重要动作发生时触发自定义脚本。 有两组这样的钩子：客户端的和服务器端的。 客户端钩子由诸如提交和合并这样的操作所调用，而服务器端钩子作用于诸如接收被推送的提交这样的联网操作。 你可以随心所欲地运用这些钩子。 ","date":"2020-08-04","objectID":"/git-hook%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:0:0","tags":["Git","hook"],"title":"Git Hook介绍与使用","uri":"/git-hook%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["Git"],"content":"安装钩子 钩子都被存储在 Git 目录下的 hooks 子目录中。 也即绝大部分项目中的 .git/hooks 。 当你用 git init 初始化一个新版本库时，Git 默认会在这个目录中放置一些示例脚本。 这些脚本除了本身可以被调用外，它们还透露了被触发时所传入的参数。 所有的示例都是 shell 脚本，其中一些还混杂了 Perl 代码，不过，任何正确命名的可执行脚本都可以正常使用 —— 你可以用 Ruby 或 Python，或任何你熟悉的语言编写它们。 这些示例的名字都是以 .sample 结尾，如果你想启用它们，得先移除这个后缀。 把一个正确命名（不带扩展名）且可执行的文件放入 .git 目录下的 hooks 子目录中，即可激活该钩子脚本。 这样一来，它就能被 Git 调用。 创建一个空的仓库： git --bare init # 推荐使用 # git init 其中--bare参数表示创建一个裸仓库，裸仓库何意？与普通空仓库有有什么区别？参考：https://www.cnblogs.com/irockcode/p/8761954.html 但是需要注意的一点的是git仓库和git裸仓库的钩子所在位置不同: git普通仓库钩子在.git/hooks/中 git裸仓库钩子在hooks/中 裸仓库初始化完成后，在hooks文件夹中会发现一些以.sample结尾的实例文件： ","date":"2020-08-04","objectID":"/git-hook%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:1:0","tags":["Git","hook"],"title":"Git Hook介绍与使用","uri":"/git-hook%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["Git"],"content":"客户端钩子简单示例 使用commit-msg钩子实现commit信息字符数限制（实际可能没什么卵用,仅作为学习参考） 编辑.git/hooks/commit-msg文件如下： ➜ git-hook-test git:(master) ✗ cat .git/hooks/commit-msg 20:55:37 #!/bin/sh MSG=`awk '{printf(\"%s\",$0)}' $1` if [ ${#MSG} -lt 5 ] then echo \"-------------------------------------------------------------------\" echo \"commit message 只有${#MSG}字符，不符合要求\" echo \"message的长度不能小于5, 本次提交失败，请完善commit message，再提交\" echo \"-------------------------------------------------------------------\" exit 1 fi 为commit-msg文件添加执行权限： chmod u+x .git/hooks/commit-msg 生成一次提交信息： ➜ git-hook-test git:(master) ✗ git add . 21:00:48 ➜ git-hook-test git:(master) ✗ git commit -m \"test\" ------------------------------------------------------------------- commit message 只有4字符 message的长度不能小于5, 本次提交失败，请完善commit message，再提交 ------------------------------------------------------------------- 不满足最低5字符要求，提交失败。 ","date":"2020-08-04","objectID":"/git-hook%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:2:0","tags":["Git","hook"],"title":"Git Hook介绍与使用","uri":"/git-hook%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["Git"],"content":"服务端钩子简单示例 使用post-receive将代码自动部署到服务器 流程大致如下：在服务器端创建一个 Git 仓库(repository)，然后将本地的文件使用git push上传到这个仓库后，将上传到Git 仓库的文件，自动复制到服务器里网站目录文件夹下。 在服务器端初始化一个空的Git仓库 git init --bare myblog.git 在hooks目录中创建post-receive文件，并给予执行权限 cd hooks/ touch post-receive;chmod u+x post-receive 编辑post-receive文件内容如下： [root@localhost hooks]# cat post-receive #!/bin/bash -l GIT_REPO=/root/myblog.git TMP_GIT_CLONE=/tmp/myblog PUBLIC_WWW=/var/www/html rm -rf ${TMP_GIT_CLONE} git clone $GIT_REPO $TMP_GIT_CLONE cd $TMP_GIT_CLONE rm -rf ${PUBLIC_WWW}/* cp -rf ${TMP_GIT_CLONE}/* ${PUBLIC_WWW} 本地添加远程Git仓库并进行测试 ➜ test git:(master) ✗ git remote add deploy ssh://root@172.16.224.128:/root/myblog.git 21:20:35 ➜ test git:(master) ✗ git add . 21:20:52 ➜ test git:(master) ✗ git commit -m \"auto deploy test\" 21:21:01 [master (root-commit) 26fde86] auto deploy test 2 files changed, 1 insertion(+) create mode 100644 index.html create mode 100644 testfile ➜ test git:(master) git push --set-upstream deploy master 21:28:19 在git服务器端查看post-receive是否成功运行 [root@localhost ~]# cd /var/www/html/ [root@localhost html]# ls index.html testfile [root@localhost html]# cat index.html auto deploy auto deploy 将代码推送指定分支： #!/bin/bash -l while read oldrev newrev ref do branch=`echo $ref | cut -d/ -f3` if [ \"master\" == \"$branch\" ] || [ \"production\" == \"$branch\" ]; then GIT_REPO=/root/myblog.git TMP_GIT_CLONE=/tmp/myblog PUBLIC_WWW=/var/www/html rm -rf ${TMP_GIT_CLONE} git clone $GIT_REPO $TMP_GIT_CLONE cd $TMP_GIT_CLONE unset GIT_DIR git checkout $branch rm -rf ${PUBLIC_WWW}/* cp -rf ${TMP_GIT_CLONE}/* ${PUBLIC_WWW} fi done 更多钩子参考：https://git-scm.com/book/zh/v2/%E8%87%AA%E5%AE%9A%E4%B9%89-Git-Git-%E9%92%A9%E5%AD%90 ","date":"2020-08-04","objectID":"/git-hook%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:3:0","tags":["Git","hook"],"title":"Git Hook介绍与使用","uri":"/git-hook%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["Git"],"content":"参考链接 git钩子-图灵社区 git hooks with python commit-msg实用示例 ","date":"2020-08-04","objectID":"/git-hook%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:4:0","tags":["Git","hook"],"title":"Git Hook介绍与使用","uri":"/git-hook%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":[],"content":"gulp 将开发流程中让人痛苦或耗时的任务自动化，从而减少你所浪费的时间、创造更大价值。 官方站点：https://www.gulpjs.com.cn/ ","date":"2020-08-04","objectID":"/hexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8gulp%E5%8E%8B%E7%BC%A9%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90-1/:0:0","tags":["hexo","gulp"],"title":"Hexo博客使用gulp压缩静态资源 1","uri":"/hexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8gulp%E5%8E%8B%E7%BC%A9%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90-1/"},{"categories":[],"content":"1、全局安装gulp npm install gulp -g # 查看版本 gulp -v ","date":"2020-08-04","objectID":"/hexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8gulp%E5%8E%8B%E7%BC%A9%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90-1/:0:1","tags":["hexo","gulp"],"title":"Hexo博客使用gulp压缩静态资源 1","uri":"/hexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8gulp%E5%8E%8B%E7%BC%A9%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90-1/"},{"categories":[],"content":"2、安装gulp插件 在站点根目录下安装 npm install gulp --save npm install gulp-minify-css --save npm install gulp-uglify --save npm install gulp-htmlmin --save npm install gulp-htmlclean --save npm install gulp-imagemin --save # 解决【Gulp打包问题】 GulpUglifyError: unable to minify JavaScript # 解决 gulp-uglify 压缩JavaScript 不兼容 es5 语法的问题 npm install babel-core@6.26.3 --save npm install gulp-babel@7.0.1 --save npm install babel-preset-es2015@6.24.1 --save # gulp-babel 取消严格模式方法(\"use strict\") npm install babel-plugin-transform-remove-strict-mode --save 问题：如果安装gulp-imagemin错误请执行以下语句 sudo npm i gulp-imagemin --unsafe-perms ","date":"2020-08-04","objectID":"/hexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8gulp%E5%8E%8B%E7%BC%A9%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90-1/:0:2","tags":["hexo","gulp"],"title":"Hexo博客使用gulp压缩静态资源 1","uri":"/hexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8gulp%E5%8E%8B%E7%BC%A9%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90-1/"},{"categories":[],"content":"3、创建gulpfile.js文件 在 Hexo 站点下新建gulpfile.js文件，文件内容如下： var gulp = require('gulp'); var minifycss = require('gulp-minify-css'); var uglify = require('gulp-uglify'); var htmlmin = require('gulp-htmlmin'); var htmlclean = require('gulp-htmlclean'); var imagemin = require('gulp-imagemin'); var babel = require('gulp-babel'); // 压缩css文件 gulp.task('minify-css', function (done) { return gulp.src('./public/**/*.css') .pipe(minifycss()) .pipe(gulp.dest('./public')); done(); }); // 压缩html文件 gulp.task('minify-html', function (done) { return gulp.src('./public/**/*.html') .pipe(htmlclean()) .pipe(htmlmin({ removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, })) .pipe(gulp.dest('./public')); done(); }); // 压缩js文件 gulp.task('minify-js', function (done) { return gulp.src(['./public/**/*.js', '!./public/**/*.min.js']) .pipe(babel({ //将ES6代码转译为可执行的JS代码 presets: ['es2015'] // es5检查机制 })) .pipe(uglify()) .pipe(gulp.dest('./public')); done(); }); // 压缩 public/images 目录内图片(Version\u003c3) // gulp.task('minify-images', function () { // gulp.src('./public/images/**/*.*') // .pipe(imagemin({ // optimizationLevel: 5, //类型：Number 默认：3 取值范围：0-7（优化等级） // progressive: true, //类型：Boolean 默认：false 无损压缩jpg图片 // interlaced: false, //类型：Boolean 默认：false 隔行扫描gif进行渲染 // multipass: false, //类型：Boolean 默认：false 多次优化svg直到完全优化 // })) // .pipe(gulp.dest('./public/images')); // }); // 压缩 public/images 目录内图片(Version\u003e3) gulp.task('minify-images', function (done) { gulp.src('./public/images/**/*.*') .pipe(imagemin([ imagemin.gifsicle({interlaced: true}), imagemin.jpegtran({progressive: true}), imagemin.optipng({optimizationLevel: 5}), imagemin.svgo({ plugins: [ {removeViewBox: true}, {cleanupIDs: false} ] }) ])) .pipe(gulp.dest('./public/images')); done(); }); //4.0以前的写法 //gulp.task('default', [ // 'minify-html', 'minify-css', 'minify-js', 'minify-images' //]); //4.0以后的写法 // 执行 gulp 命令时执行的任务 gulp.task('default', gulp.series(gulp.parallel('minify-html', 'minify-css', 'minify-js', 'minify-images')), function () { console.log(\"----------gulp Finished----------\"); // Do something after a, b, and c are finished. }); ","date":"2020-08-04","objectID":"/hexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8gulp%E5%8E%8B%E7%BC%A9%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90-1/:0:3","tags":["hexo","gulp"],"title":"Hexo博客使用gulp压缩静态资源 1","uri":"/hexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8gulp%E5%8E%8B%E7%BC%A9%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90-1/"},{"categories":[],"content":"4、创建.babelrc文件 在 Hexo 站点下新建.babelrc文件，文件内容如下： { 'presets': ['es2015'], \"plugins\": [\"transform-remove-strict-mode\"] } ","date":"2020-08-04","objectID":"/hexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8gulp%E5%8E%8B%E7%BC%A9%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90-1/:0:4","tags":["hexo","gulp"],"title":"Hexo博客使用gulp压缩静态资源 1","uri":"/hexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8gulp%E5%8E%8B%E7%BC%A9%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90-1/"},{"categories":[],"content":"5、启用静态资源压缩 推荐姿势： hexo cl \u0026\u0026 hexo g \u0026\u0026 gulp \u0026\u0026 hexo d 可在package.json 中的 \"scripts\" 项增加一个 \"publish\" 命令： { //... \"scripts\": { //... \"publish\": \"hexo cl \u0026\u0026 hexo g \u0026\u0026 gulp \u0026\u0026 hexo d\", //... } //... } ","date":"2020-08-04","objectID":"/hexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8gulp%E5%8E%8B%E7%BC%A9%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90-1/:0:5","tags":["hexo","gulp"],"title":"Hexo博客使用gulp压缩静态资源 1","uri":"/hexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8gulp%E5%8E%8B%E7%BC%A9%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90-1/"},{"categories":["GitHub"],"content":"阿里云对象存储服务（Object Storage Service，简称OSS）为您提供基于网络的数据存取服务。使用OSS，您可以通过网络随时存储和调用包括文本、图片、音频和视频等在内的各种非结构化数据文件。 官方帮助文档：https://help.aliyun.com/document_detail/31883.html 首先打开所要使用Github Actions的仓库，点击Actions，并创建一个新的workflow 此时，github将会自动在仓库中创建一个名为.github/workflows的文件夹。 编辑配置文件oss.yml,输入以下内容 name:MainWorkflowon:[push]jobs:build:runs-on:ubuntu-lateststeps:- uses:actions/checkout@v1- uses:actions/setup-node@v1with:node-version:\"12.x\"- name:BuildBlogrun:| npm installnpminstall-ghexo-clihexogenerate- uses:manyuanrong/setup-ossutil@v1.0with:# endpoint 可以去oss控制台上查看endpoint:\"oss-cn-hangzhou.aliyuncs.com\"# 使用我们之前配置在secrets里面的accesskeys来配置ossutilaccess-key-id:${{secrets.ACCESS_KEY_ID}}access-key-secret:${{secrets.ACCESS_KEY_SECRET}}- name:DeplyToOSSrun:ossutilcppublicoss://agou-ops/-rf 其中，secrets.ACCESS_KEY_ID和secrets.ACCESS_KEY_SECRET属于github的专用秘钥，比较隐私的变量可以放置在此处，具体位置在Settings\u003eSecrets 3. 提交commit，github会自动进行部署，点击Actions可以查看部署的详细状态 ","date":"2020-08-04","objectID":"/%E4%BD%BF%E7%94%A8github-actions%E5%B0%86%E6%96%87%E4%BB%B6push%E5%88%B0oss/:0:0","tags":["GitHub Action","OSS"],"title":"使用GitHub Actions将文件push到OSS","uri":"/%E4%BD%BF%E7%94%A8github-actions%E5%B0%86%E6%96%87%E4%BB%B6push%E5%88%B0oss/"},{"categories":["GitHub"],"content":"参考链接 Github Actions入门教程,阮一峰 GitHub Pages 官方文档 Github Actions for web apps, Luke Boyle My First Week With GitHub Actions, Adam Zolyak ","date":"2020-08-04","objectID":"/%E4%BD%BF%E7%94%A8github-actions%E5%B0%86%E6%96%87%E4%BB%B6push%E5%88%B0oss/:1:0","tags":["GitHub Action","OSS"],"title":"使用GitHub Actions将文件push到OSS","uri":"/%E4%BD%BF%E7%94%A8github-actions%E5%B0%86%E6%96%87%E4%BB%B6push%E5%88%B0oss/"},{"categories":["Linux","shell"],"content":"为你的shell脚本编写帮助信息文档","date":"2020-07-14","objectID":"/%E7%BC%96%E5%86%99shell%E5%B8%AE%E5%8A%A9%E6%96%87%E6%A1%A3/","tags":["Linux","shell","脚本","帮助文档","技巧"],"title":"编写shell帮助信息通用小技巧","uri":"/%E7%BC%96%E5%86%99shell%E5%B8%AE%E5%8A%A9%E6%96%87%E6%A1%A3/"},{"categories":["Linux","shell"],"content":"废话少说， 直接上脚本： #!/bin/bash ### ### Some shell version or description here. ### ### Usage: ### test \u003cinput\u003e \u003coutput\u003e ### ### Options: ### \u003cinput\u003e Input file to read. ### \u003coutput\u003e Output file to write. Use '-' for stdout. ### -h | --help Show this message. help() { sed -rn 's/^### ?//;T;p' \"$0\" } # 如果用户输入“-h”或者无参数，执行help函数 if [[ $# == 0 ]] || [[ \"$1\" == \"-h\" ]] || [[ \"$1\" == \"--help\" ]]; then help exit 1 fi 效果： ╭─agou-ops@ideapad-15ISK ~/tmp ╰─$ bash test.sh --help Some shell version or description here. Usage: test \u003cinput\u003e \u003coutput\u003e Options: \u003cinput\u003e Input file to read. \u003coutput\u003e Output file to write. Use '-' for stdout. -h | --help Show this message. ","date":"2020-07-14","objectID":"/%E7%BC%96%E5%86%99shell%E5%B8%AE%E5%8A%A9%E6%96%87%E6%A1%A3/:0:0","tags":["Linux","shell","脚本","帮助文档","技巧"],"title":"编写shell帮助信息通用小技巧","uri":"/%E7%BC%96%E5%86%99shell%E5%B8%AE%E5%8A%A9%E6%96%87%E6%A1%A3/"},{"categories":["benchmark","web","Tools"],"content":"web benchmark tools ","date":"2020-07-14","objectID":"/web-benchmark/:1:0","tags":["压测","web","http","工具","持续更新"],"title":"Web 压测工具","uri":"/web-benchmark/"},{"categories":["benchmark","web","Tools"],"content":"Apache Bench ApacheBench 是一个用来衡量http服务器性能的单线程命令行工具。原本针对Apache http服务器，但是也适用于其他http服务器。 如果你的操作系统没有ab工具， 那么只需安装httpd-tools(CentOS)或者apache-utils(Ubuntu)即可。 常用参数说明： -n：执行的请求次数 -c：并发数量 -s：响应的超时时间 -p：post请求的数据文件路径，需要设置-T参数 -T：Content-Type -C：设置cookie，格式为\"name=zhou” 常用组合： ab -n 10000 -c 1000 http://localhost/index.html ","date":"2020-07-14","objectID":"/web-benchmark/:1:1","tags":["压测","web","http","工具","持续更新"],"title":"Web 压测工具","uri":"/web-benchmark/"},{"categories":["benchmark","web","Tools"],"content":"wrk wrk HTTP是一个现代的基准测试工具能产生显著的负载运行时在一个多核CPU。它结合了多线程设计可伸缩的系统如epoll和kqueue事件通知。 官方github仓库： https://github.com/wg/wrk 编译安装： wget https://github.com/wg/wrk/archive/4.1.0.tar.gz tar xf 4.1.0.tar.gz cd 4.1.0/deps tar xf LuaJIT-2.1.0-beta3.tar.gz -C /usr/local tar xf openssl-1.1.0g.tar.gz -C /usr/local cd .. make WITH_LUAJIT=/usr/local/LuaJIT-2.1.0-beta3 WITH_OPENSSL=/usr/local/openssl-1.1.0g make install 常用参数说明： -t：线程数 -c：http总请求数量 -d：测试时长，e.g. 2s, 2m, 2h 简单使用： wrk -t12 -c400 -d30s http://127.0.0.1:8080/index.html ","date":"2020-07-14","objectID":"/web-benchmark/:1:2","tags":["压测","web","http","工具","持续更新"],"title":"Web 压测工具","uri":"/web-benchmark/"},{"categories":["benchmark","web","Tools"],"content":"待续。。。","date":"2020-07-14","objectID":"/web-benchmark/:1:3","tags":["压测","web","http","工具","持续更新"],"title":"Web 压测工具","uri":"/web-benchmark/"},{"categories":["Linux","ECS","故障处理"],"content":"篇幅有限，转至 https://agou-ops.top/linux_sys_fix 进行查看。 ","date":"2020-07-14","objectID":"/ecs-linux-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/:0:0","tags":["Linux","故障处理"],"title":"ECS-Linux 故障处理","uri":"/ecs-linux-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"},{"categories":["Linux","Tools","转载"],"content":"nload工具 nload用于实时查看网卡流量，默认系统都没有安装，首先安装方式如下： $ yum install -y epel-release $ yum install -y nload 使用也非常简单，如下： $ nload Device eth0 [192.168.0.110] (4/5): =================================================================================== Incoming: Curr: 5.21 kBit/s # Incoming：进来的流量 Avg: 4.09 kBit/s # Outgoing：出去的流量 Min: 1.59 kBit/s # Curr：当前的流量值 Max: 12.51 kBit/s # Avg：平均值的流量值 Ttl: 4.16 GByte # Min：最小的流量值 Outgoing: # Max：最大的流量值 Curr: 16.48 kBit/s # Ttl：总的流量值 Avg: 14.38 kBit/s Min: 6.73 kBit/s Max: 28.39 kBit/s ","date":"2020-07-11","objectID":"/linux-%E5%AE%9E%E6%97%B6%E6%9F%A5%E7%9C%8B%E7%BD%91%E5%8D%A1%E6%B5%81%E9%87%8F/:1:0","tags":["Linux","工具","网卡"],"title":"Linux 实时查看网卡流量","uri":"/linux-%E5%AE%9E%E6%97%B6%E6%9F%A5%E7%9C%8B%E7%BD%91%E5%8D%A1%E6%B5%81%E9%87%8F/"},{"categories":["Linux","Tools","转载"],"content":"iftop工具 默认系统没有安装，需要安装，如下： # 需要epel环境 $ yum install -y epel-release $ yum install -y iftop 安装完成之后，输入iftop便可看到如下界面 其中，相关参数解释如下： 界面上面显示的是类似刻度尺的刻度范围，为显示流量图形的长条作标尺用的。 中间的\u003c= =\u003e这两个左右箭头，表示的是流量的方向。 TX：发送流量 RX：接收流量5.TOTAL：总流量6.Cumm：运行iftop到目前时间的总流量 peak：流量峰值 rates：分别表示过去 2s 10s 40s 的平均流量 ","date":"2020-07-11","objectID":"/linux-%E5%AE%9E%E6%97%B6%E6%9F%A5%E7%9C%8B%E7%BD%91%E5%8D%A1%E6%B5%81%E9%87%8F/:2:0","tags":["Linux","工具","网卡"],"title":"Linux 实时查看网卡流量","uri":"/linux-%E5%AE%9E%E6%97%B6%E6%9F%A5%E7%9C%8B%E7%BD%91%E5%8D%A1%E6%B5%81%E9%87%8F/"},{"categories":["Linux","Tools","转载"],"content":"sar命令 sar命令包含在sysstat工具包中，提供系统的众多统计数据。其在不同的系统上命令有些差异，某些系统提供的sar支持基于网络接口的数据统计，也可以查看设备上每秒收发包的个数和流量。 # DEV显示网络接口信息 # 命令后面1 2 意思是：每一秒钟取1次值，取2次 $ sar -n DEV 1 2 另外，-n参数很有用，他有6个不同的开关：DEV | EDEV | NFS | NFSD | SOCK | ALL ，其代表的含义如下： DEV显示网络接口信息。 EDEV显示关于网络错误的统计数据。 NFS统计活动的NFS客户端的信息。 NFSD统计NFS服务器的信息 SOCK显示套接字信息 ALL显示所有5个开关 $ sar -n DEV 1 2 Linux 3.10.0-514.26.2.el7.x86_64 (localhost) 08/31/2019 _x86_64_ (1 CPU) 09:52:28 AM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s 09:52:29 AM eth0 2.02 1.01 0.13 0.16 0.00 0.00 0.00 09:52:29 AM lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 09:52:29 AM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s 09:52:30 AM eth0 1.02 1.02 0.07 0.23 0.00 0.00 0.00 09:52:30 AM lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Average: IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s Average: eth0 1.52 1.02 0.10 0.19 0.00 0.00 0.00 Average: lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 参数说明： IFACE：LAN接口 rxpck/s：每秒钟接收的数据包 txpck/s：每秒钟发送的数据包 rxbyt/s：每秒钟接收的字节数 txbyt/s：每秒钟发送的字节数 rxcmp/s：每秒钟接收的压缩数据包 txcmp/s：每秒钟发送的压缩数据包 rxmcst/s：每秒钟接收的多播数据包 rxerr/s：每秒钟接收的坏数据包 txerr/s：每秒钟发送的坏数据包 coll/s：每秒冲突数 rxdrop/s：因为缓冲充满，每秒钟丢弃的已接收数据包数 txdrop/s：因为缓冲充满，每秒钟丢弃的已发送数据包数 txcarr/s：发送数据包时，每秒载波错误数 rxfram/s：每秒接收数据包的帧对齐错误数 rxfifo/s：接收的数据包每秒FIFO过速的错误数 txfifo/s：发送的数据包每秒FIFO过速的错误数 这种方式简单，直观，推荐使用。 ","date":"2020-07-11","objectID":"/linux-%E5%AE%9E%E6%97%B6%E6%9F%A5%E7%9C%8B%E7%BD%91%E5%8D%A1%E6%B5%81%E9%87%8F/:3:0","tags":["Linux","工具","网卡"],"title":"Linux 实时查看网卡流量","uri":"/linux-%E5%AE%9E%E6%97%B6%E6%9F%A5%E7%9C%8B%E7%BD%91%E5%8D%A1%E6%B5%81%E9%87%8F/"},{"categories":["Linux","Tools","转载"],"content":"cat /proc/net/dev Linux 内核提供了一种通过 /proc 文件系统，在运行时访问内核内部数据结构、改变内核设置的机制。proc文件系统是一个伪文件系统，它只存在内存当中，而不占用外存空间。它以文件系统的方式为访问系统内核数据的操作提供接口。用户和应用程序可以通过proc得到系统的信息，并可以改变内核的某些参数。由于系统的信息，如进程，是动态改变的，所以用户或应用程序读取proc文件时，proc文件系统是动态从系统内核读出所需信息并提交的。/proc文件系统中包含了很多目录，其中/proc/net/dev 保存了网络适配器及统计信息。 [root@agou ~]\\# cat /proc/net/dev Inter-| Receive | Transmit face |bytes packets errs drop fifo frame compressed multicast|bytes packets errs drop fifo colls carrier compressed ens32: 11196722 21042 0 0 0 0 0 0 4605085 14465 0 0 0 0 0 0 lo: 1934784 7162 0 0 0 0 0 0 1934784 7162 0 0 0 0 0 0 说明： 最左边的表示接口的名字，Receive表示收包，Transmit表示发送包； bytes表示收发的字节数； packets表示收发正确的包量； errs表示收发错误的包量； drop表示收发丢弃的包量； 其实，我们平时经常用的很多查看网卡实时流量的命令，都是通过读取该目录下的实时流量，并通过简单计算得到的。 ","date":"2020-07-11","objectID":"/linux-%E5%AE%9E%E6%97%B6%E6%9F%A5%E7%9C%8B%E7%BD%91%E5%8D%A1%E6%B5%81%E9%87%8F/:4:0","tags":["Linux","工具","网卡"],"title":"Linux 实时查看网卡流量","uri":"/linux-%E5%AE%9E%E6%97%B6%E6%9F%A5%E7%9C%8B%E7%BD%91%E5%8D%A1%E6%B5%81%E9%87%8F/"},{"categories":["Linux","Tools","转载"],"content":"实时监控脚本1 ifconfig可以查看的是从连上网开始的流量总和，cat /proc/net/dev记录的值也是总流量，那么可以计算一下，实时流量=当前流量-上一秒的流量。 [root@localhost ~]\\# cat network.sh # 传入网卡参数 ethn=$1 while true do RX_pre=$(cat /proc/net/dev | grep $ethn | sed 's/:/ /g' | awk '{print $2}') TX_pre=$(cat /proc/net/dev | grep $ethn | sed 's/:/ /g' | awk '{print $10}') sleep 1 RX_next=$(cat /proc/net/dev | grep $ethn | sed 's/:/ /g' | awk '{print $2}') TX_next=$(cat /proc/net/dev | grep $ethn | sed 's/:/ /g' | awk '{print $10}') clear # echo -e激活转义符 # 输出时间的标题 echo -e \"t RX `date +%k:%M:%S` TX\" RX=$((${RX_next}-${RX_pre})) TX=$((${TX_next}-${TX_pre})) if [[ $RX -lt 1024 ]];then RX=\"${RX}B/s\" elif [[ $RX -gt 1048576 ]];then RX=$(echo $RX | awk '{print $1/1048576 \"MB/s\"}') else RX=$(echo $RX | awk '{print $1/1024 \"KB/s\"}') fi if [[ $TX -lt 1024 ]];then TX=\"${TX}B/s\" elif [[ $TX -gt 1048576 ]];then TX=$(echo $TX | awk '{print $1/1048576 \"MB/s\"}') else TX=$(echo $TX | awk '{print $1/1024 \"KB/s\"}') fi # 输出流量 echo -e \"$ethnt $RX$TX\" done 执行结果如下： [root@localhost ~]\\# ./network.sh eth0 RX 20:23:38 TX eth0 66B/s 0B/s RX 20:23:39 TX eth0 132B/s 0B/s RX 20:23:40 TX eth0 186B/s 194B/s RX 20:23:41 TX eth0 240B/s 194B/s RX 20:23:42 TX eth0 132B/s 0B/s RX 20:23:43 TX eth0 240B/s 194B/s RX 20:23:44 TX eth0 396B/s 4.19727KB/s RX 20:23:45 TX eth0 276B/s 178B/s ","date":"2020-07-11","objectID":"/linux-%E5%AE%9E%E6%97%B6%E6%9F%A5%E7%9C%8B%E7%BD%91%E5%8D%A1%E6%B5%81%E9%87%8F/:5:0","tags":["Linux","工具","网卡"],"title":"Linux 实时查看网卡流量","uri":"/linux-%E5%AE%9E%E6%97%B6%E6%9F%A5%E7%9C%8B%E7%BD%91%E5%8D%A1%E6%B5%81%E9%87%8F/"},{"categories":["Linux","Tools","转载"],"content":"实时监控脚本2 $ cat network_flow.sh # 监控实时网卡流量 # $1 接收所传第一个参数 即要监控的网卡 NIC=$1 # echo -e \"traffic in --- traffic out\" while true;do # $0 命令输出结果 ~ 匹配模式 OLD_IN=`awk '$0~\"'$NIC'\"{print $2}' /proc/net/dev` OLD_OUT=`awk '$0~\"'$NIC'\"{print $10}' /proc/net/dev` sleep 1 NEW_IN=`awk '$0~\"'$NIC'\"{print $2}' /proc/net/dev` NEW_OUT=`awk '$0~\"'$NIC'\"{print $10}' /proc/net/dev` clear # printf不换行 %s占位符 IN=$(printf \"%.1f%s\" \"$(($NEW_IN-$OLD_IN))\" \"B/s\") OUT=$(printf \"%.1f%s\" \"$(($NEW_OUT-$OLD_OUT))\" \"B/s\") echo \" traffic in `date +%k:%M:%S` traffic out \" echo \"$NIC$IN$OUT\" done 执行结果如下： $ ./network_flow.sh eth0 traffic in 11:15:02 traffic out eth0 732.0B/s 948.0B/s traffic in 11:15:03 traffic out eth0 132.0B/s 0.0B/s traffic in 11:15:04 traffic out eth0 132.0B/s 0.0B/s traffic in 11:15:05 traffic out eth0 132.0B/s 0.0B/s traffic in 11:15:06 traffic out eth0 186.0B/s 242.0B/s traffic in 11:15:07 traffic out eth0 132.0B/s 0.0B/s traffic in 11:15:08 traffic out eth0 132.0B/s 0.0B/s traffic in 11:15:09 traffic out eth0 132.0B/s 0.0B/s traffic in 11:15:10 traffic out eth0 240.0B/s 242.0B/s traffic in 11:15:11 traffic out eth0 132.0B/s 0.0B/s 该文章内容收集于网络。 ","date":"2020-07-11","objectID":"/linux-%E5%AE%9E%E6%97%B6%E6%9F%A5%E7%9C%8B%E7%BD%91%E5%8D%A1%E6%B5%81%E9%87%8F/:6:0","tags":["Linux","工具","网卡"],"title":"Linux 实时查看网卡流量","uri":"/linux-%E5%AE%9E%E6%97%B6%E6%9F%A5%E7%9C%8B%E7%BD%91%E5%8D%A1%E6%B5%81%E9%87%8F/"},{"categories":["Linux","CentOS","Ubuntu"],"content":"Mondo Rescue Mondo Rescue（简称 Mondo）：是一款开源免费的故障恢复和备份工具，可以说是 Linux 操作系统下的 Ghost ，你可以轻松地创建系统（Linux 或 Windows）克隆或备份的 ISO 镜像，可以将这些镜像存放在 CD、DVD、磁带、USB 设备、硬盘和 NFS 上。 官方站点:http://www.mondorescue.org/ ","date":"2020-07-10","objectID":"/linux-%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/:1:0","tags":["Linux","系统封装"],"title":"Linux 系统封装","uri":"/linux-%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/"},{"categories":["Linux","CentOS","Ubuntu"],"content":"安装 mondo 从官方指定的仓库下载 wget http://mondo.mirror.pclark.com/ftp/pub/mondorescue/centos/7/x86_64/mondorescue.repo -O /etc/yum.repos.d/mondorescue.repo yum install -y mondo 或者直接安装rpm包: yum install -y http://mondo.mirror.pclark.com/ftp/pub/mondorescue/centos/7/x86_64/mondo-3.3.0-1.centos7.x86_64.rpm 使用该方法安装可能会缺少依赖包文件,在这里手动解决依赖关系即可(太多了,说着玩的,还是推荐用在线仓库安装吧…): yum install -y http://ftp.mondorescue.org/centos/7/x86_64/afio-2.5-1.centos7.x86_64.rpm yum install -y http://ftp.mondorescue.org/centos/7/x86_64/mindi-3.3.0-1.centos7.x86_64.rpm yum install -y http://ftp.mondorescue.org/centos/7/x86_64/buffer-1.19-8.centos7.x86_64.rpm ... ","date":"2020-07-10","objectID":"/linux-%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/:1:1","tags":["Linux","系统封装"],"title":"Linux 系统封装","uri":"/linux-%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/"},{"categories":["Linux","CentOS","Ubuntu"],"content":"制作光盘镜像 安装完毕后，在终端下输入mondoarchive，即可进入图形安装界面。 mondoarchive See /var/log/mondoarchive.log for details of backup run. Checking sanity of your Linux distribution ....... 随后,按照引导一步步设置即可. ","date":"2020-07-10","objectID":"/linux-%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/:1:2","tags":["Linux","系统封装"],"title":"Linux 系统封装","uri":"/linux-%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/"},{"categories":["Linux","CentOS","Ubuntu"],"content":"Remastersys Remastersys 可以将你安装的 Ubuntu、Debian 及其衍生版打包成一个可以用来安装的 Live CD/DVD 的 ISO 镜像文件，可打包一个包含个人数据的 ISO 镜像文件作为操作系统备份，也可以打包一个不包含个人数据的 ISO 镜像文件发布给其他人安装。 ","date":"2020-07-10","objectID":"/linux-%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/:2:0","tags":["Linux","系统封装"],"title":"Linux 系统封装","uri":"/linux-%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/"},{"categories":["Linux","CentOS","Ubuntu"],"content":"Remastersys 包下载 $ mkdir tools $ cd tools/ $ wget ftp://ftp.gwdg.de/pub/linux/easyvdr/mirror/remastersys/ubuntu/remastersys/remastersys_3.0.4-2_all.deb $ wget ftp://ftp.gwdg.de/pub/linux/easyvdr/mirror/remastersys/ubuntu/remastersys-gui/remastersys-gui_3.0.4-1_amd64.deb ","date":"2020-07-10","objectID":"/linux-%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/:2:1","tags":["Linux","系统封装"],"title":"Linux 系统封装","uri":"/linux-%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/"},{"categories":["Linux","CentOS","Ubuntu"],"content":"安装 Remastersys 依赖 $ sudo apt-get install syslinux-utils isolinux squashfs-tools casper libdebian-installer4 ubiquity-frontend-debconf user-setup discover xresprobe systemd ","date":"2020-07-10","objectID":"/linux-%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/:2:2","tags":["Linux","系统封装"],"title":"Linux 系统封装","uri":"/linux-%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/"},{"categories":["Linux","CentOS","Ubuntu"],"content":"安装 Remastersys-gui 依赖 $ sudo apt-get install libvte-common libvte9 plymouth-x11 ","date":"2020-07-10","objectID":"/linux-%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/:2:3","tags":["Linux","系统封装"],"title":"Linux 系统封装","uri":"/linux-%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/"},{"categories":["Linux","CentOS","Ubuntu"],"content":"开始安装 Remastersys $ cd tools/ $ sudo dpkg -i remastersys_3.0.4-2_all.deb $ sudo dpkg -i remastersys-gui_3.0.4-1_amd64.deb Remastersys 备份操作系统 命令行模式： $ sudo remastersys backup 图形化模式： $ sudo remastersys-gui ","date":"2020-07-10","objectID":"/linux-%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/:2:4","tags":["Linux","系统封装"],"title":"Linux 系统封装","uri":"/linux-%E7%B3%BB%E7%BB%9F%E5%B0%81%E8%A3%85/"},{"categories":["Linux"],"content":"Apache HTTPD使用Let's Encrypt实现安全连接(https) 安装cerbot: yum update -y yum install -y cerbot 生成高安全性的DH秘钥到/etc/ssl/certs/目录当中去: sudo openssl dhparam -out /etc/ssl/certs/dhparam.pem 2048 运行以下命令创建目录, 并使它可写为Apache服务器: sudo mkdir -p /var/lib/letsencrypt/.well-known sudo chgrp www-data /var/lib/letsencrypt sudo chmod g+s /var/lib/letsencrypt 为了避免复制代码和配置更易于维护, 创建以下两个配置代码片段: 编辑/etc/apache2/conf-available/letsencrypt.conf Alias /.well-known/acme-challenge/ \"/var/lib/letsencrypt/.well-known/acme-challenge/\" \u003cDirectory \"/var/lib/letsencrypt/\"\u003e AllowOverride None Options MultiViews Indexes SymLinksIfOwnerMatch IncludesNoExec Require method GET POST OPTIONS \u003c/Directory\u003e 编辑/etc/apache2/conf-available/ssl-params.conf: SSLProtocol all -SSLv3 -TLSv1 -TLSv1.1 SSLCipherSuite ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384 SSLHonorCipherOrder off SSLSessionTickets off SSLUseStapling On SSLStaplingCache \"shmcb:logs/ssl_stapling(32768)\" SSLOpenSSLConfCmd DHParameters \"/etc/ssl/certs/dhparam.pem\" Header always set Strict-Transport-Security \"max-age=63072000\" 进行启用配置文件之前, 确保mod_ssl 和mod_headers已经被启用: sudo a2enmod ssl sudo a2enmod headers 然后, 启用SSL配置文件, 运行以下命令即可: sudo a2enconf letsencrypt sudo a2enconf ssl-params 启用HTTP/2模块: sudo a2enmod http2 重启httpd让配置生效: systemctl restart httpd 现在，我们可以使用webroot插件运行Certbot工具并获取SSL证书文件： sudo certbot certonly --agree-tos --email agou-ops@foxmail.com --webroot -w /var/lib/letsencrypt/ -d agou-ops.top -d www.agou-ops.top 如果输出以下信息, 则表已经成功申请到SSL证书文件: IMPORTANT NOTES: - Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/agou-ops.top/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/agou-ops.top/privkey.pem Your cert will expire on 2020-10-06. To obtain a new or tweaked version of this certificate in the future, simply run certbot again. To non-interactively renew *all* of your certificates, run \"certbot renew\" - Your account credentials have been saved in your Certbot configuration directory at /etc/letsencrypt. You should make a secure backup of this folder now. This configuration directory will also contain certificates and private keys obtained by Certbot so making regular backups of this folder is ideal. - If you like Certbot, please consider supporting our work by: Donating to ISRG / Let's Encrypt: https://letsencrypt.org/donate Donating to EFF: https://eff.org/donate-le ","date":"2020-07-10","objectID":"/apache-httpd%E4%BD%BF%E7%94%A8lets-encrypt/:1:0","tags":["Linux","Apache","httpd","SSL"],"title":"Apache HTTPD使用Let's Encrypt","uri":"/apache-httpd%E4%BD%BF%E7%94%A8lets-encrypt/"},{"categories":["Linux"],"content":"配置虚拟主机 创建一个虚拟主机conf.d/agou-ops-top.conf(仅为示例): \u003cVirtualHost *:80\u003e ServerName mail.agou-ops.top Redirect permanent / https://mail.agou-ops.top/ \u003c/VirtualHost\u003e \u003cVirtualHost *:443\u003e ServerName mail.agou-ops.top Protocols h2 http:/1.1 \u003cIf \"%{HTTP_HOST} == 'www.mail.agou-ops.top'\"\u003e Redirect permanent / https://mail.agou-ops.top/ \u003c/If\u003e DocumentRoot /var/www/mail.agou-ops.top/public_html ErrorLog ${APACHE_LOG_DIR}/mail.agou-ops.top-error.log CustomLog ${APACHE_LOG_DIR}/mail.agou-ops.top-access.log combined SSLEngine On SSLCertificateFile /etc/letsencrypt/live/mail.agou-ops.top/fullchain.pem SSLCertificateKeyFile /etc/letsencrypt/live/mail.agou-ops.top/privkey.pem # Other Apache Configuration \u003c/VirtualHost\u003e 重载httpd使虚拟主机配置生效: systemctl reload httpd 现在, 就可以通过https访问你的站点了: https://agou-ops.top ","date":"2020-07-10","objectID":"/apache-httpd%E4%BD%BF%E7%94%A8lets-encrypt/:2:0","tags":["Linux","Apache","httpd","SSL"],"title":"Apache HTTPD使用Let's Encrypt","uri":"/apache-httpd%E4%BD%BF%E7%94%A8lets-encrypt/"},{"categories":["Linux"],"content":"自动更新Let’s Encrypt证书 Let’s Encrypt的证书有效期为90天, 自动更新证书到期前,certbot包创建一个计划, 一天两次, 并自动更新任何证书到期前30天. 解决方法, 添加定时任务, /etc/cron.d/cerbot, 内容如下所示: 0 */12 * * * root test -x /usr/bin/certbot -a \\! -d /run/systemd/system \u0026\u0026 perl -e 'sleep int(rand(3600))' \u0026\u0026 certbot -q renew --renew-hook \"systemctl reload apache2\" 测试更新, 使用certbot的干跑模式进行测试: sudo certbot renew --dry-run ","date":"2020-07-10","objectID":"/apache-httpd%E4%BD%BF%E7%94%A8lets-encrypt/:3:0","tags":["Linux","Apache","httpd","SSL"],"title":"Apache HTTPD使用Let's Encrypt","uri":"/apache-httpd%E4%BD%BF%E7%94%A8lets-encrypt/"},{"categories":["Linux"],"content":"参考链接 SSL lab: https://www.ssllabs.com/ Let’s Encrypt: https://letsencrypt.org/ ","date":"2020-07-10","objectID":"/apache-httpd%E4%BD%BF%E7%94%A8lets-encrypt/:4:0","tags":["Linux","Apache","httpd","SSL"],"title":"Apache HTTPD使用Let's Encrypt","uri":"/apache-httpd%E4%BD%BF%E7%94%A8lets-encrypt/"},{"categories":["生活","日记","吐槽"],"content":"​ 2020年06月28日，端午“三天小短假”已经过去了，按常理说应当放假，哪怕一天也好，但我们没有放假，一行五人还是像往常一样出外业，经理呢也完全没有任何表示，当然我也不奢望有何表示，毕竟这个东西看个人，对我来说也就无所谓了。 可今天是周日，又让我们出外业了，早上七点就把我们叫起来（其实八点半才上班），等了一个多小时才出发。 欸，这时候可能就有小伙伴要问了，为什么周日也要上班呢？没有休息的的时间么，这里引用经理的一些话，大概意思是： 项目比较“紧张”，休息的话视情况而言，比如说项目有可能进度快，不到周末就会休息，有时候呢项目“紧张”，也就不休息了。 ​ 嘿嘿，翻译过来就是，除非外面下大雨或者考试与工作时间冲突才会放假休息，否则就别想了，哦，说不定心情好会给放半天假。 嘿嘿\" 嘿嘿 ​ 你也别跟我说来实习就是来锻炼自己的吃苦精神的，那都是放狗臭屁，至少对于我个人而言，不需要。 要不是为了那点学分，学校又强制要求来实习（虽然表面没直接说），扪心自问，谁又会闲着没事来实习呢？这里我可能言重了，但对于我或者我这类人来说，是这样的，毕竟我/我们的志向不在于此，对于以后要混这碗饭的人来说，实习“或许”是个好选择，为什么用“或许”这个词呢，因为决定性因素太多，学校，指导老师，还有项目经理等等。。。 ​ 是啊，他们都能坚持七天无休上班更何况我们这些小青年呢？能这么单纯想的也真是天真烂漫，是的，我们实习生一天在外面到处跑一天加上餐费（算上补贴）工资才六十块左右，和他们一天二三百块，三四百块相比。。。 啊，果然大学生是廉价劳动力，可恶！ 要是我有这工资，不说和他们一样工资待遇，有一半，就算早上七点开始工作到晚上九点半十点下班我也愿意，迫真，哈哈。 ​ 另外呢，我现在十分讨厌别人拿他那一辈的艰辛和难处“教导/开导”我们这些后辈，那时候确实很难，这是毋庸置疑的，我也十分感激前辈们的努力和付出，这才有了当今这样美好的社会。 也许初中高中前辈们的这些话能感染到我们，但现在长大了，有些道理不言而喻，不知不觉之间也开始厌恶这些话，甚至麻痹，对于这些，我只想说，时代变了，每一代人都有每一代人的烦恼和艰辛之处，或许不是身体上的，但精神和身心上的烦恼与伤害，我觉得要比身体上的严重百倍不止！ ​ 这里又扯远了，进入正题。:next_track_button::next_track_button: 上面这些都不是今天真正让我愤懑写下这篇博客的原因，不止我一人给他说过28号下午1:30-3:00考VR，并且需要提前至少半小时来准备考试。 ​ 可是呢，中午十一点半下班了还没见到他人，开车也不知道去哪办些重要的业务了，果然大学生就是大学生，可能我们是一些无所谓的人吧，过了十分钟，也就是十一点四十左右，我让和我一起实习的同班同学打电话给他，哈哈，真是没想到，打过去，这边响了两声，那边直接给挂了！我敲，我还以为在路上呢，快要到了才没接电话，结果呢，又过了十几二十分钟吧，大概十一点五十多，他打电话给另外一个和我们一起实习的女生，说不过来了，让我们打车回去，？？？！！！真是个小机灵鬼呢…个屁。 我们在郊区又在一个位置比较靠大路远的一个村里，能打到车，我估计考试都考完了，随后，果然，在滴滴上等了十分钟没见个车影。 最后，这里要十分感谢马岭岗村里的那个书记大哥把我俩送回去，还有一同实习的那个女生帮我们说清楚原因让大哥带我们回去考试，真是十分感谢。 以上都只是我的个人观点，想扯淡的话，可以在下方评论区和我一起交流交流这方面的一些东西。 在此，最后一次向万恶的资本主义低头。 ","date":"2020-06-28","objectID":"/%E6%97%A5%E8%AE%B0%E6%9E%9C%E7%84%B6%E5%A4%A7%E5%AD%A6%E7%94%9F%E9%83%BD%E6%98%AF%E5%BB%89%E4%BB%B7%E5%8A%B3%E5%8A%A8%E5%8A%9B%E5%90%97%E5%8F%AF%E6%81%B6/:0:0","tags":["日记","实习","吐槽"],"title":"【日记】果然大学生都是廉价劳动力吗？可恶！","uri":"/%E6%97%A5%E8%AE%B0%E6%9E%9C%E7%84%B6%E5%A4%A7%E5%AD%A6%E7%94%9F%E9%83%BD%E6%98%AF%E5%BB%89%E4%BB%B7%E5%8A%B3%E5%8A%A8%E5%8A%9B%E5%90%97%E5%8F%AF%E6%81%B6/"},{"categories":["生活","日记"],"content":"苦逼的实习已经进行到第三天，天气一天比一天热，不过值得高兴的是，不用再一一进户对所有房屋进行测量了，减少了了不少的工作量，因为昨天下午已经用RTK测量好了控制点，那些无数木遮挡的建筑物就可以直接用无人机进行测量了。无需人工再测量。 测量ing测量ing \" 测量ing 另外呢，这里的村民十分的热心好客，天气炎热，给我们切西瓜和冰棍吃，哈哈，我们三人吃了一路（当然没这么夸张），真是不错呢。 另附一张村民家里遇到的山羊兄弟： 山羊兄弟山羊兄弟 \" 山羊兄弟 啊，还有好多户要量，坚持坚持，也就这几天了，港巴嘚！ ","date":"2020-06-25","objectID":"/%E6%97%A5%E8%AE%B0%E9%A9%AC%E5%B2%AD%E5%B2%97%E6%9D%91%E5%AE%9E%E4%B9%A0/:0:0","tags":["日记","实习"],"title":"【日记】马岭岗村实习","uri":"/%E6%97%A5%E8%AE%B0%E9%A9%AC%E5%B2%AD%E5%B2%97%E6%9D%91%E5%AE%9E%E4%B9%A0/"},{"categories":["linux","Tools"],"content":"本文主要介绍Linux系统下的日志分割工具，Split 和 Logrotate。 ","date":"2020-06-22","objectID":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/:0:0","tags":["Linux","日志分割","工具"],"title":"Linux 日志分割工具-split和logrotate","uri":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/"},{"categories":["linux","Tools"],"content":"Split Linux 系统自带简单日志文件分割工具。 常用参数列表： 选项 含义 -b 分割后的文档大小，单位是byte -C 分割后的文档，单行最大byte数 -d 使用数字作为后缀(default: 字母)，同时使用-a length(default: 2)指定后缀长度 -l 分割后文档的行数 –verbose 显示输出详细信息 示例： split -l --verbose 1000000 split.test -d -a 3 split.log ╰─$ ls split.test split.test008 split.test017 split.test026 split.test000 split.test009 split.test018 split.test027 split.test001 split.test010 split.test019 split.test028 split.test002 split.test011 split.test020 split.test029 split.test003 split.test012 split.test021 split.test030 split.test004 split.test013 split.test022 split.test031 split.test005 split.test014 split.test023 split.test032 split.test006 split.test015 split.test024 split.test033 split.test007 split.test016 split.test025 ... ","date":"2020-06-22","objectID":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/:1:0","tags":["Linux","日志分割","工具"],"title":"Linux 日志分割工具-split和logrotate","uri":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/"},{"categories":["linux","Tools"],"content":"Logrotate ","date":"2020-06-22","objectID":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/:2:0","tags":["Linux","日志分割","工具"],"title":"Linux 日志分割工具-split和logrotate","uri":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/"},{"categories":["linux","Tools"],"content":"简介及解决方案 Logrotate，即Log rotation日志滚动，属于Linux系统自带工具，基于crontab实现时间点滚动日志，计划每天运行的脚本位于 /etc/cron.daily/logrotate。 GitHub 地址：https://github.com/logrotate/logrotate logrotate 是怎么做到滚动日志时不影响程序正常的日志输出呢？logrotate 提供了两种解决方案。 create copytruncate ","date":"2020-06-22","objectID":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/:2:1","tags":["Linux","日志分割","工具"],"title":"Linux 日志分割工具-split和logrotate","uri":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/"},{"categories":["linux","Tools"],"content":"create 这也就是默认的方案，可以通过 create 命令配置文件的权限和属组设置；这个方案的思路是重命名原日志文件，创建新的日志文件。详细步骤如下： 重命名正在输出日志文件，因为重命名只修改目录以及文件的名称，而进程操作文件使用的是 inode，所以并不影响原程序继续输出日志。 创建新的日志文件，文件名和原日志文件一样，注意，此时只是文件名称一样，而 inode 编号不同，原程序输出的日志还是往原日志文件输出。 最后通过某些方式通知程序，重新打开日志文件；由于重新打开日志文件会用到文件路径而非 inode 编号，所以打开的是新的日志文件。 如上也就是 logrotate 的默认操作方式，也就是 mv+create 执行完之后，通知应用重新在新文件写入即可。mv+create 成本都比较低，几乎是原子操作，如果应用支持重新打开日志文件，如 syslog, nginx, mysql 等，那么这是最好的方式。 不过，有些程序并不支持这种方式，压根没有提供重新打开日志的接口；而如果重启应用程序，必然会降低可用性，为此引入了如下方式。 ","date":"2020-06-22","objectID":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/:2:2","tags":["Linux","日志分割","工具"],"title":"Linux 日志分割工具-split和logrotate","uri":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/"},{"categories":["linux","Tools"],"content":"copytruncate 该方案是把正在输出的日志拷 (copy) 一份出来，再清空 (trucate) 原来的日志；详细步骤如下： 将当前正在输出的日志文件复制为目标文件，此时程序仍然将日志输出到原来文件中，此时，原文件名也没有变。 清空日志文件，原程序仍然还是输出到预案日志文件中，因为清空文件只把文件的内容删除了，而 inode 并没改变，后续日志的输出仍然写入该文件中。 如上所述，对于 copytruncate 也就是先复制一份文件，然后清空原有文件。 通常来说，清空操作比较快，但是如果日志文件太大，那么复制就会比较耗时，从而可能导致部分日志丢失。不过这种方式不需要应用程序的支持即可。 执行文件： /usr/sbin/logrotate 主配置文件: /etc/logrotate.conf 自定义配置文件: /etc/logrotate.d/*.conf 执行状态文件：/var/lib/logrotate/logrotate.status ","date":"2020-06-22","objectID":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/:2:3","tags":["Linux","日志分割","工具"],"title":"Linux 日志分割工具-split和logrotate","uri":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/"},{"categories":["linux","Tools"],"content":"运行 logrotate logrotate 命令格式及常用参数如下所示： logrotate [-dv] [-f|--force] [-s|--state file] config_file .. -d, --debug ：debug 模式，测试配置文件是否有错误，并不会真正执行 rorate 和 compose 操作，但是会打印出整个执行的流程，和调用的脚本等详细信息。 -f, --force ：强制转储文件。 -m, --mail=command ：压缩日志后，发送日志到指定邮箱。 -s, --state=statefile ：使用指定的状态文件。 -v, --verbose ：显示详细转储过程。 通常使用的方法是配合crontab来运行： crontab -e */30 * * * * /usr/sbin/logrotate /etc/logrotate.d/rsyslog \u003e /dev/null 2\u003e\u00261 \u0026 手动调用 logrotate： # 调用 /etc/lograte.d/ 下配置的所有日志 logrotate /etc/logrotate.conf # 要为某个特定的配置调用 logrotate logrotate -d /etc/logrotate.d/log_file # 开启debug模式，不实际生成日志文件。 ","date":"2020-06-22","objectID":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/:2:4","tags":["Linux","日志分割","工具"],"title":"Linux 日志分割工具-split和logrotate","uri":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/"},{"categories":["linux","Tools"],"content":"配置文件详解 这里以系统自带的部分/etc/logrotate.d/rsyslog配置文件为例： /var/log/syslog { rotate 7 daily missingok notifempty delaycompress compress postrotate /usr/lib/rsyslog/rsyslog-rotate endscript } monthly: 日志文件将按月轮循。其它可用值为 daily，weekly 或者 yearly。 rotate 5: 一次将存储 5 个归档日志。对于第六个归档，时间最久的归档将被删除。 compress: 在轮循任务完成后，已轮循的归档将使用 gzip 进行压缩。 delaycompress: 总是与 compress 选项一起用，delaycompress 选项指示 logrotate 不要将最近的归档压缩，压缩 将在下一次轮循周期进行。这在你或任何软件仍然需要读取最新归档时很有用。 missingok: 在日志轮循期间，任何错误将被忽略，例如 “文件无法找到” 之类的错误。 notifempty: 如果日志文件为空，轮循不会进行。 create 644 root root: 以指定的权限创建全新的日志文件，同时 logrotate 也会重命名原始日志文件。 postrotate/endscript: 在所有其它指令完成后，postrotate 和 endscript 里面指定的命令将被执行。在这种情况下，rsyslogd 进程将立即再次读取其配置并继续运行。 其他常用参数： tabooext [+] list:让 logrotate 不转储指定扩展名的文件，缺省的扩展名是.rpm-orig, .rpmsave, v, 和～ missingok:在日志轮循期间，任何错误将被忽略，例如 “文件无法找到” 之类的错误。 size size:当日志文件到达指定的大小时才转储，bytes (缺省) 及 KB (sizek) 或 MB (sizem) copytruncate:用于还在打开中的日志文件，把当前日志备份并截断 nocopytruncate: 备份日志文件但是不截断 create mode owner group : 转储文件，使用指定的文件模式创建新的日志文件 nocreate: 不建立新的日志文件 nodelaycompress: 覆盖 delaycompress 选项，转储同时压缩。 errors address : 专储时的错误信息发送到指定的 Email 地址 ifempty :即使是空文件也转储，这个是 logrotate 的缺省选项。 mail address : 把转储的日志文件发送到指定的 E-mail 地址 nomail : 转储时不发送日志文件 olddir directory:储后的日志文件放入指定的目录，必须和当前日志文件在同一个文件系统 noolddir: 转储后的日志文件和当前日志文件放在同一个目录下 ","date":"2020-06-22","objectID":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/:2:5","tags":["Linux","日志分割","工具"],"title":"Linux 日志分割工具-split和logrotate","uri":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/"},{"categories":["linux","Tools"],"content":"logrotate 日志切割轮询 由于 logrotate 是基于cron运行的，所以这个日志轮转的时间是由 cron 控制的，具体可以查询 cron 的配置文件 /etc/anacrontab，过往的老版本的文件为（/etc/crontab） 使用 crontab 来作为日志轮转的触发容器来修改 logrotate 默认执行时间： $ vim /etc/crontab # 切割时间为每天晚上的12点钟 SHELL=/bin/bash PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=root HOME=/ # run-parts 01 * * * * root run-parts /etc/cron.hourly 59 23 * * * root run-parts /etc/cron.daily 22 4 * * 0 root run-parts /etc/cron.weekly 42 4 1 * * root run-parts /etc/cron.monthly ","date":"2020-06-22","objectID":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/:2:6","tags":["Linux","日志分割","工具"],"title":"Linux 日志分割工具-split和logrotate","uri":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/"},{"categories":["linux","Tools"],"content":"附：logrotate 配置文件示例 syslog [root@gop-sg-192-168-56-103 logrotate.d]# cat syslog /var/log/cron /var/log/maillog /var/log/messages /var/log/secure /var/log/spooler { missingok sharedscripts postrotate /bin/kill -HUP `cat /var/run/syslogd.pid 2\u003e /dev/null` 2\u003e /dev/null || true endscript } zabbix-agent [root@gop-sg-192-168-56-103 logrotate.d]# cat zabbix-agent /var/log/zabbix/zabbix_agentd.log { weekly rotate 12 compress delaycompress missingok notifempty create 0664 zabbix zabbix } nginx [root@gop-sg-192-168-56-103 logrotate.d]# cat nginx /var/log/nginx/*.log /var/log/nginx/*/*.log{ daily missingok rotate 14 compress delaycompress notifempty create 640 root adm sharedscripts postrotate [ ! -f /var/run/nginx.pid ] || kill -USR1 `cat /var/run/nginx.pid` endscript } influxdb [root@gop-sg-192-168-56-103 logrotate.d]# cat influxdb /var/log/influxdb/access.log { daily rotate 7 missingok dateext copytruncate compress } ","date":"2020-06-22","objectID":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/:2:7","tags":["Linux","日志分割","工具"],"title":"Linux 日志分割工具-split和logrotate","uri":"/linux-%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7/"},{"categories":["Linux"],"content":"WTF和bashtop是我个人目前在用的系统资源管理面板，界面非常的美观，极具极客风范，在这里我分别做简单介绍。 ","date":"2020-06-12","objectID":"/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF/:0:0","tags":["Linux","监控"],"title":"Linux 系统资源监控面板","uri":"/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF/"},{"categories":["Linux"],"content":"WTF WTF（又名“wtfutil”）是终端的个人信息仪表板，可快速访问您非常重要但不常需要的统计信息和数据。 项目地址：https://github.com/wtfutil/wtf 官方站点：https://wtfutil.com/ 先放一张截图吧： WTF截图\" WTF截图 面板中有以下展示模块： 世界时间 我的IP地址信息 TODO List（待办事件） 系统安全状态 当前城市天气 电源使用情况 docker 运行状态 资源使用情况 硬盘占用情况 开机时长 好了，废话不多说，直接开始吧。 ","date":"2020-06-12","objectID":"/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF/:1:0","tags":["Linux","监控"],"title":"Linux 系统资源监控面板","uri":"/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF/"},{"categories":["Linux"],"content":"安装与使用 首先打开项目的 release仓库 下载可直接使用的二进制包. wget https://github.com/wtfutil/wtf/releases/download/v0.30.0/wtf_0.30.0_linux_amd64.tar.gz tar xf wtf_0.30.0_linux_amd64.tar.gz 解压完成之后，直接运行./wtfutil即可，为了日后方便使用，可以将该程序软连接到用户bin目录进行使用： ln -sv /root/wtfutil /usr/bin/ ","date":"2020-06-12","objectID":"/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF/:1:1","tags":["Linux","监控"],"title":"Linux 系统资源监控面板","uri":"/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF/"},{"categories":["Linux"],"content":"配置文件 下面是我的配置文件，配置文件的目录是$HOME/.config/wtf/config.yml，可以拿来参考参考： :information_source:文件比较长，单击»\u003e 这里 «\u003c可直接跳到下一节 wtf:colors:background:blackborder:focusable:darkslatebluefocused:orangenormal:graychecked:yellowhighlight:fore:blackback:grayrows:even:yellowodd:whitegrid:columns:[40,35,35,55]rows:[10,10,10,10,4]refreshInterval:1openFileUtil:\"open\"mods:digitalclock:color:redenabled:truefont:digitalfonthourFormat:24position:top:0left:0height:1width:1refreshInterval:1title:\"big clock\"type:\"digitalclock\"world_time:title:\"World Time\"type:clockscolors:rows:even:\"lightblue\"odd:\"white\"enabled:truelocations:UTC:\"Etc/UTC\"London:\"Europe/London\"Berlin:\"Europe/Berlin\"New_York:\"America/New_York\"China:\"Asia/Shanghai\"position:top:0left:1height:1width:1refreshInterval:15sort:\"alphabetical\"battery:type:powertitle:\"⚡️\"enabled:trueposition:top:1left:3height:1width:1refreshInterval:15todolist:type:todocheckedIcon:\"X\"colors:checked:grayhighlight:fore:\"black\"back:\"orange\"enabled:truefilename:\"todo.yml\"position:top:1left:0height:2width:1refreshInterval:3600ip:type:ipinfotitle:\"My IP\"colors:name:\"lightblue\"value:\"white\"enabled:trueposition:top:0left:2height:1width:2refreshInterval:150prettyweather:enabled:truecity:\"临沂\"position:top:1left:2height:1width:1refreshInterval:300unit:\"m\"view:0language:\"en\"security:enabled:trueposition:top:1left:1height:1width:1refreshInterval:3600docker:type:dockerenabled:truelabelColor:lightblueposition:top:2left:1height:1width:3refreshInterval:1resources:type:resourceusageenabled:trueposition:top:3left:0height:2width:1refreshInterval:1uptime:type:cmdrunnerargs:[\"\"]cmd:\"uptime\"enabled:trueposition:top:4left:1height:1width:3refreshInterval:30disks:type:cmdrunnercmd:\"df\"args:[\"-h\"]enabled:trueposition:top:3left:1height:1width:3refreshInterval:3600 ","date":"2020-06-12","objectID":"/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF/:1:2","tags":["Linux","监控"],"title":"Linux 系统资源监控面板","uri":"/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF/"},{"categories":["Linux"],"content":"官方支持模块 Azure DevOps BambooHR Buildkite CDS CircleCI Clocks CmdRunner Crypto Currencies Datadog DEV (dev.to) Digital Clock DigitalOcean Docker Exchange Rates Feed Reader Gerrit Git GitHub GitLab Gitter Google Apps Hacker News Have I Been Pwned (HIBP) IP Addresses Jenkins Jira Kubernetes Logger Mercurial New Relic OpsGenie Pagerduty Pi-hole Power Resource Usage Rollbar Security Sports Spotify Subreddit Textfile Todo Todoist Transmission TravisCI Trello Twitter VictorOps OnCall Weather Services Zendesk ","date":"2020-06-12","objectID":"/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF/:1:3","tags":["Linux","监控"],"title":"Linux 系统资源监控面板","uri":"/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF/"},{"categories":["Linux"],"content":"bashtop 官方开源仓库：https://github.com/aristocratos/bashtos 截图\" 截图 主菜单： 主菜单\" 主菜单 选项菜单： 选项菜单\" 选项菜单 ","date":"2020-06-12","objectID":"/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF/:2:0","tags":["Linux","监控"],"title":"Linux 系统资源监控面板","uri":"/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF/"},{"categories":["Linux"],"content":"安装与使用 Add PPA repository and install bashtop sudo add-apt-repository ppa:bashtop-monitor/bashtop sudo apt update sudo apt install bashtop 安装完毕之后，直接在终端运行bashtop即可。 使用ESC键可以呼出菜单，按下q键退出。 ","date":"2020-06-12","objectID":"/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF/:2:1","tags":["Linux","监控"],"title":"Linux 系统资源监控面板","uri":"/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF/"},{"categories":["Linux"],"content":"配置文件 All options changeable from within UI. Config files stored in “$HOME/.config/bashtop” folder bashtop.cfg: (auto generated if not found) #? Config file for bashtop v. 0.9.9 #* Color theme, looks for a .theme file in \"$HOME/.config/bashtop/themes\" and \"$HOME/.config/bashtop/user_themes\", \"Default\" for builtin default theme color_theme=\"Default\" #* Update time in milliseconds, increases automatically if set below internal loops processing time, recommended 2000 ms or above for better sample times for graphs update_ms=\"2500\" #* Processes sorting, \"pid\" \"program\" \"arguments\" \"threads\" \"user\" \"memory\" \"cpu lazy\" \"cpu responsive\" \"tree\" #* \"cpu lazy\" updates sorting over time, \"cpu responsive\" updates sorting directly at a cpu usage cost proc_sorting=\"cpu lazy\" #* Reverse sorting order, \"true\" or \"false\" proc_reversed=\"false\" #* Check cpu temperature, only works if \"sensors\", \"vcgencmd\" or \"osx-cpu-temp\" commands is available check_temp=\"true\" #* Draw a clock at top of screen, formatting according to strftime, empty string to disable draw_clock=\"%X\" #* Update main ui when menus are showing, set this to false if the menus is flickering too much for comfort background_update=\"true\" #* Custom cpu model name, empty string to disable custom_cpu_name=\"\" #* Enable error logging to \"$HOME/.config/bashtop/error.log\", \"true\" or \"false\" error_logging=\"true\" #* Show color gradient in process list, \"true\" or \"false\" proc_gradient=\"true\" #* If process cpu usage should be of the core it's running on or usage of the total available cpu power proc_per_core=\"false\" #* Optional filter for shown disks, should be names of mountpoints, \"root\" replaces \"/\", separate multiple values with space disks_filter=\"\" #* Enable check for new version from github.com/aristocratos/bashtop at start update_check=\"true\" #* Enable graphs with double the horizontal resolution, increases cpu usage hires_graphs=\"false\" #* Enable the use of psutil python3 module for data collection, default on OSX use_psutil=\"true\" ","date":"2020-06-12","objectID":"/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF/:2:2","tags":["Linux","监控"],"title":"Linux 系统资源监控面板","uri":"/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF/"},{"categories":["Linux"],"content":"[TOC] ","date":"2020-06-01","objectID":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/:0:0","tags":["Linux","随机数","openssl"],"title":"Linux 生成随机数","uri":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/"},{"categories":["Linux"],"content":"通过 openssl 生成 [root@myhost ~]\\# openssl rand -base64 3 xsSp [root@myhost ~]\\# openssl rand -base64 8 8SlHCsBAiYw= [root@myhost ~]\\# openssl rand -base64 10 nUzBw8ngKGGqWw== `openssl rand -base64 32|tr A-Z a-z|cut -c 1-10` ","date":"2020-06-01","objectID":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/:1:0","tags":["Linux","随机数","openssl"],"title":"Linux 生成随机数","uri":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/"},{"categories":["Linux"],"content":"使用 date 命令生成 [root@myhost ~]\\# date +%s 1539071518 [root@myhost ~]\\# date +%N 801398716 [root@myhost ~]\\# date +%s%N 1539071555311467855 `date +%s |sha256sum |base64 |head -c 10 ;echo` ","date":"2020-06-01","objectID":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/:2:0","tags":["Linux","随机数","openssl"],"title":"Linux 生成随机数","uri":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/"},{"categories":["Linux"],"content":"使用 md5sum 生成随机数 date | md5sum ","date":"2020-06-01","objectID":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/:3:0","tags":["Linux","随机数","openssl"],"title":"Linux 生成随机数","uri":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/"},{"categories":["Linux"],"content":"使用 dd 命令生成随机数 dd if=/dev/urandom bs=1 count=15|base64 -w 0 ","date":"2020-06-01","objectID":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/:4:0","tags":["Linux","随机数","openssl"],"title":"Linux 生成随机数","uri":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/"},{"categories":["Linux"],"content":"通过/dev/random设备产生uuid [root@myhost ~]\\# cat /proc/sys/kernel/random/uuid a3dfb0f2-f893-4e57-9d67-184a88d4cb5d [root@myhost ~]\\# cat /proc/sys/kernel/random/uuid |cut -c 1-8 b80c60d8 ","date":"2020-06-01","objectID":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/:5:0","tags":["Linux","随机数","openssl"],"title":"Linux 生成随机数","uri":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/"},{"categories":["Linux"],"content":"使用系统环境变量 RANDOM [root@myhost ~]\\# echo $RANDOM 14535 ##说明：linux系统下的环境变量$RANDOM的取值范围是：0–32767 。 产生0-25范围内的数，用这个环境变量对26取余即可。 [root@myhost ~]\\# echo $(($RANDOM%26)) 6 [root@myhost ~]\\# echo $((RANDOM%26)) 11 ##说明：第二个表达式RANDOM前面无$符号好像也可以。 产生6位数的整数，用这个环境变量加上100000即可。 [root@myhost ~]\\# echo $(($RANDOM+100000)) 117482 [root@myhost ~]\\# echo $((RANDOM+100000)) 126058 ##说明：第二个表达式RANDOM前面无$符号好像也可以。 产生加密的随机数码，将随机数管道给命令md5sum命令即可。 [root@myhost ~]\\# echo $RANDOM |md5sum 6ee8cd13547eb044ad13ba014573af6f - 需要固定位数的随机数码，将随机数管道给命令md5sum命令再管道给cut命令即可。 [root@myhost ~]\\# echo $RANDOM |md5sum|cut -c 1-8 de3cfe23 ","date":"2020-06-01","objectID":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/:6:0","tags":["Linux","随机数","openssl"],"title":"Linux 生成随机数","uri":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/"},{"categories":["Linux"],"content":"使用第三方工具 expect 非交互式程序控制下用mkpasswd命令： yum install -y expect mkpasswd -l 7 # 生成七位密码包含大小写加特殊字符 使用pwgen生成随机可读的密码： yum install pwgen # 生成长度8，含有数字，含有大小写字母的密码4个，列打印 pwgen -ncC 8 4 # 生成长度8，含有数字，含有小写字母，不包含歧义的密码4个，列打印 pwgen -nABC 8 4 # 生成长度16，含有数字，含有大小写字母，含有特殊字符的密码8个，行打印 pwgen -ncy1 16 8 参数说明： -c or –capitalize密码中至少包含一个大写字母 -A or –no-capitalize密码中不包含大写字母 -n or –numerals密码中至少包含一个数字 -0 or –no-numerals密码中不包含数字 -y or –symbols密码中至少包含一个特殊符号 -s or –secure生成完全随机密码 -B or –ambiguous密码中不包含歧义字符（例如1,l,O,0） -H or –sha1=path/to/file[#seed]使用SHA1 hash给定的文件作为一个随机种子 -C在列中打印生成的密码 -1不要在列中打印生成的密码，即一行一个密码 -v or –no-vowels不要使用任何元音，以避免偶然的脏话 其他第三方工具： randpw、spw、gpg、xkcdpass ","date":"2020-06-01","objectID":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/:7:0","tags":["Linux","随机数","openssl"],"title":"Linux 生成随机数","uri":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/"},{"categories":["Linux"],"content":"参考链接 https://blog.csdn.net/yuki5233/article/details/82997001 ","date":"2020-06-01","objectID":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/:8:0","tags":["Linux","随机数","openssl"],"title":"Linux 生成随机数","uri":"/linux%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/"},{"categories":["Tools","Android","Ubuntu"],"content":"跨平台安卓投屏神器","date":"2020-05-28","objectID":"/scrcpy-android%E6%8A%95%E5%B1%8F%E7%A5%9E%E5%99%A8/","tags":["Scrcpy","Android"],"title":"Scrcpy Android投屏神器","uri":"/scrcpy-android%E6%8A%95%E5%B1%8F%E7%A5%9E%E5%99%A8/"},{"categories":["Tools","Android","Ubuntu"],"content":"Scrcpy 简介 应用程序可显示和控制通过USB（或通过TCP / IP）连接的Android设备。它不需要任何根访问权限。它适用于GNU / Linux，Windows和macOS。 github地址：https://github.com/Genymobile/scrcpy ","date":"2020-05-28","objectID":"/scrcpy-android%E6%8A%95%E5%B1%8F%E7%A5%9E%E5%99%A8/:1:0","tags":["Scrcpy","Android"],"title":"Scrcpy Android投屏神器","uri":"/scrcpy-android%E6%8A%95%E5%B1%8F%E7%A5%9E%E5%99%A8/"},{"categories":["Tools","Android","Ubuntu"],"content":"配置环境 安装adb服务，使用数据线连接自己的安卓设备，并查看自己的安卓设备： sudo apt-get install android-tools-adb adb start-server $ lsusb Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 001 Device 005: ID 0bda:0821 Realtek Semiconductor Corp. Bus 001 Device 004: ID 0bda:0129 Realtek Semiconductor Corp. RTS5129 Card Reader Controller Bus 001 Device 003: ID 13d3:a745 IMC Networks Bus 001 Device 054: ID `24ae:1813` ... 找到自己的安卓设备哪一行，24ae、1813在下面会用到。 创建设备文件： 下面命令中的名称71-android是自定义的，我的这个代表安卓7.1的意思。 echo 0x12d1 \u003e ~/.android/adb_usb.ini touch /etc/udev/rules.d/71-android.rules gedit /etc/udev/rules.d/71-android.rules 将以下内容写入刚刚创建的文件，注意，下面的24ae、1813要改成自己的安卓设备的id（见上）： SUBSYSTEM\"usb”, ATTRS{idVendor}“24ae”, ATTRS{idProduct}==\"1813”, MODE=\"0666” 更改文件权限： chmod 666 /etc/udev/rules.d/90-android.rules 重启 ADB 服务： service udev restart adb kill-server adb start-server ","date":"2020-05-28","objectID":"/scrcpy-android%E6%8A%95%E5%B1%8F%E7%A5%9E%E5%99%A8/:2:0","tags":["Scrcpy","Android"],"title":"Scrcpy Android投屏神器","uri":"/scrcpy-android%E6%8A%95%E5%B1%8F%E7%A5%9E%E5%99%A8/"},{"categories":["Tools","Android","Ubuntu"],"content":"Scrcpy 安装与使用 安装： # 使用snap安装 sudo snap install scrcpy # 或者 apt install scrcpy 列出设备： scrcpy.adb devices 开始投屏： scrcpy ","date":"2020-05-28","objectID":"/scrcpy-android%E6%8A%95%E5%B1%8F%E7%A5%9E%E5%99%A8/:3:0","tags":["Scrcpy","Android"],"title":"Scrcpy Android投屏神器","uri":"/scrcpy-android%E6%8A%95%E5%B1%8F%E7%A5%9E%E5%99%A8/"},{"categories":["Tools","Android","Ubuntu"],"content":"设置无线连接Android设备 :information_source:先使用数据线将手机和电脑连接并在手机端开启「开发者选项」及「USB 调试」 # 开启手机端口 adb tcpip 6666 拔出数据线，开启无线连接： adb connect 192.168.8.154:6666 192.168.8.154为你Android设备的IP地址，可以通过路由器后台获取或者在手机上使用相关软件进行获取。 启动 scrcpy ： scrcpy ","date":"2020-05-28","objectID":"/scrcpy-android%E6%8A%95%E5%B1%8F%E7%A5%9E%E5%99%A8/:4:0","tags":["Scrcpy","Android"],"title":"Scrcpy Android投屏神器","uri":"/scrcpy-android%E6%8A%95%E5%B1%8F%E7%A5%9E%E5%99%A8/"},{"categories":["Tools","Android","Ubuntu"],"content":"Scrcpy 快捷键 Action Shortcut Shortcut (macOS) Switch fullscreen mode Ctrl+f Cmd+f Rotate display left Ctrl+← (left) Cmd+← (left) Rotate display right Ctrl+→ (right) Cmd+→ (right) Resize window to 1:1 (pixel-perfect) Ctrl+g Cmd+g Resize window to remove black borders Ctrl+x | Double-click¹ Cmd+x | Double-click¹ Click on HOME Ctrl+h | Middle-click Ctrl+h | Middle-click Click on BACK Ctrl+b | Right-click² Cmd+b | Right-click² Click on APP_SWITCH Ctrl+s Cmd+s Click on MENU Ctrl+m Ctrl+m Click on VOLUME_UP Ctrl+↑ (up) Cmd+↑ (up) Click on VOLUME_DOWN Ctrl+↓ (down) Cmd+↓ (down) Click on POWER Ctrl+p Cmd+p Power on Right-click² Right-click² Turn device screen off (keep mirroring) Ctrl+o Cmd+o Turn device screen on Ctrl+Shift+o Cmd+Shift+o Rotate device screen Ctrl+r Cmd+r Expand notification panel Ctrl+n Cmd+n Collapse notification panel Ctrl+Shift+n Cmd+Shift+n Copy device clipboard to computer Ctrl+c Cmd+c Paste computer clipboard to device Ctrl+v Cmd+v Copy computer clipboard to device and paste Ctrl+Shift+v Cmd+Shift+v Enable/disable FPS counter (on stdout) Ctrl+i Cmd+i ¹Double-click on black borders to remove them. ²Right-click turns the screen on if it was off, presses BACK otherwise. ","date":"2020-05-28","objectID":"/scrcpy-android%E6%8A%95%E5%B1%8F%E7%A5%9E%E5%99%A8/:5:0","tags":["Scrcpy","Android"],"title":"Scrcpy Android投屏神器","uri":"/scrcpy-android%E6%8A%95%E5%B1%8F%E7%A5%9E%E5%99%A8/"},{"categories":["Tools","Android","Ubuntu"],"content":"参考链接 Scrcpy Documentation: https://github.com/Genymobile/scrcpy ","date":"2020-05-28","objectID":"/scrcpy-android%E6%8A%95%E5%B1%8F%E7%A5%9E%E5%99%A8/:6:0","tags":["Scrcpy","Android"],"title":"Scrcpy Android投屏神器","uri":"/scrcpy-android%E6%8A%95%E5%B1%8F%E7%A5%9E%E5%99%A8/"},{"categories":["VPN","CentOS"],"content":"OpenVPN 简介 OpenVPN 是一个基于 OpenSSL 库的应用层 VPN 实现。和传统 VPN 相比，它的优点是简单易用。 [1] OpenVPN允许参与建立VPN的单点使用共享金钥，电子证书，或者用户名/密码来进行身份验证。它大量使用了OpenSSL加密库中的SSLv3/TLSv1 协议函式库。OpenVPN能在Solaris、Linux、OpenBSD、FreeBSD、NetBSD、Mac OS X与Windows 2000/XP/Vista上运行，并包含了许多安全性的功能。它并不是一个基于Web的VPN软件，也不与IPsec及其他VPN软件包兼容。 ","date":"2020-05-21","objectID":"/openvpn-for-centos/:1:0","tags":["OpenVPN","CentOS"],"title":"OpenVPN for CentOS","uri":"/openvpn-for-centos/"},{"categories":["VPN","CentOS"],"content":"OpenVPN 安装 ","date":"2020-05-21","objectID":"/openvpn-for-centos/:2:0","tags":["OpenVPN","CentOS"],"title":"OpenVPN for CentOS","uri":"/openvpn-for-centos/"},{"categories":["VPN","CentOS"],"content":"使用EasyRSA构建 CA 首先从项目Github存储库下载EasyRSA的最新版本并解压： cd \u0026\u0026 wget https://github.com/OpenVPN/easy-rsa/releases/download/v3.0.5/EasyRSA-nix-3.0.7.tgz tar xzf EasyRSA-3.0.7.tgz 复制模板文件，并进行适当修改： mv EasyRSA-3.0.7 EasyRSA-CA; cd EasyRSA-CA cp vars.example vars # ---------- 编辑 vars 文件内容`95`行左右 ---------- set_var EASYRSA_REQ_COUNTRY \"CN\" set_var EASYRSA_REQ_PROVINCE \"Shandong\" set_var EASYRSA_REQ_CITY \"Jinan\" set_var EASYRSA_REQ_ORG \"AGou\" set_var EASYRSA_REQ_EMAIL \"AGou-ops@foxmail.com\" set_var EASYRSA_REQ_OU \"Community\" 修改完成之后保存退出。 初始化PKI： [root@test EasyRSA-CA]\\# ./easyrsa init-pki Note: using Easy-RSA configuration from: /root/EasyRSA-3.0.7/vars init-pki complete; you may now create a CA or requests. Your newly created PKI dir is: /root/EasyRSA-3.0.7/pki 接下来建立CA： [root@test EasyRSA-CA]\\# ./easyrsa build-ca nopass Note: using Easy-RSA configuration from: /root/EasyRSA-3.0.7/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Generating RSA private key, 2048 bit long modulus ...............+++ ............................................................................................................................+++ e is 65537 (0x10001) You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Common Name (eg: your user, host, or server name) [Easy-RSA CA]: CA creation complete and you may now import and sign cert requests. Your new CA certificate file for publishing is at: /root/EasyRSA-3.0.7/pki/ca.crt 完成后，该脚本将创建两个文件-CA公共证书PKI/ca.crt和CA私钥PKI/private/ca.key ","date":"2020-05-21","objectID":"/openvpn-for-centos/:2:1","tags":["OpenVPN","CentOS"],"title":"OpenVPN for CentOS","uri":"/openvpn-for-centos/"},{"categories":["VPN","CentOS"],"content":"安装 OpenVPN 和 EasyRSA 可以在 OpenVPN 的官方 github 仓库 下载最新源码包进行编译安装，在这里为了方便，我使用epel仓库进行安装： yum install epel-release -y yum install openvpn -y 获取最新版本的EasyRSA： cd \u0026\u0026 wget https://github.com/OpenVPN/easy-rsa/releases/download/v3.0.5/EasyRSA-nix-3.0.7.tgz tar xzf EasyRSA-3.0.7.tgz mv EasyRSA-3.0.7 EasyRSA-Sever1 cd EasyRSA-Sever1 尽管我们已经在 CA 主机上初始化了PKI，但是我们还需要在 OpenVPN 服务器上创建一个新的 PKI ： [root@test EasyRSA-Server1]\\# ./easyrsa init-pki Note: using Easy-RSA configuration from: /root/EasyRSA-Server1/vars init-pki complete; you may now create a CA or requests. Your newly created PKI dir is: /root/EasyRSA-Server1/pki ","date":"2020-05-21","objectID":"/openvpn-for-centos/:2:2","tags":["OpenVPN","CentOS"],"title":"OpenVPN for CentOS","uri":"/openvpn-for-centos/"},{"categories":["VPN","CentOS"],"content":"创建Diffie-Hellman和HMAC密钥 生成一个Diffie-Hellman密钥，该密钥将在密钥交换期间使用，并使用HMAC签名文件为连接添加附加的安全层。 在OpenVPN服务器上，生成Diffie-Hellman密钥： cd ~/EasyRSA-Server1 ./easyrsa gen-dh 复制该文件到/etc/openvpn目录中去： cp /root/EasyRSA-Server1/pki/dh.pem /etc/openvpn 接下来，使用openvpn二进制文件生成HMAC签名： openvpn --genkey --secret ta.key 然后将生成的ta.key复制到/etc/openvpn目录中去： cp ta.key /etc/openvpn ","date":"2020-05-21","objectID":"/openvpn-for-centos/:2:3","tags":["OpenVPN","CentOS"],"title":"OpenVPN for CentOS","uri":"/openvpn-for-centos/"},{"categories":["VPN","CentOS"],"content":"创建服务器证书和私钥 进入OpenVPN服务器上的EasyRSA目录，并为服务器和证书请求文件生成一个新的私钥： cd ~/EasyRSA-Server1 ./easyrsa gen-req server1 nopass 该命令将创建两个文件，一个私钥（server1.key）和一个证书请求文件（server1.req） 将生成的私钥复制到/etc/openvpn目录： cp /root/EasyRSA-Server1/pki/private/server1.key /etc/openvpn/ 将证书请求发送到CA主机（这里我CA主机与OpenVPN为同一主机）： cp ~/EasyRSA-Server1/pki/reqs/server1.req /tmp 登录CA主机，切换到 EasyRSA 目录并导入证书请求文件： cd ~/EasyRSA-CA ./easyrsa import-req /tmp/server1.req server1 此命令只是将请求文件复制到pki/reqs目录中。 在CA主机上，签署证书： ./easyrsa sign-req server server1 确认信息无误之后，输入 yes 然后点击回车即可。 在CA主机上，将签名的证书server1.crt和ca.crt文件传发送回OpenVPN主机： cp ~/EasyRSA-CA/pki/issued/server1.crt /etc/openvpn/ cp ~/EasyRSA-CA/pki/ca.crt /etc/openvpn/ :warning: 这里需要注意的是：我的CA主机和OpenVPN主机是同一主机，所以上面的/etc/openvpn目录是OpenVPN主机的。 所有步骤都完成之后，在/etc/openvpn目录下应当有这些文件存在： [root@test EasyRSA-CA]\\# ls /etc/openvpn/ ca.crt client dh.pem server server1.crt server1.key ta.key ","date":"2020-05-21","objectID":"/openvpn-for-centos/:2:4","tags":["OpenVPN","CentOS"],"title":"OpenVPN for CentOS","uri":"/openvpn-for-centos/"},{"categories":["VPN","CentOS"],"content":"配置OpenVPN服务 将 OpenVPN 的模板配置文件复制到/etc/openvpn： cp /usr/share/doc/openvpn-2.4.9/sample/sample-config-files/server.conf /etc/openvpn/server1.conf 修改server1.conf，找到证书，密钥和DH参数指令并更改文件名： user nobody group nogroup # ---------- 大约在78行左右 ---------- cert server1.crt key server1.key dh dh.pem # 在文件末尾添加以下行。该指令会将消息身份验证算法（HMAC）从SHA1更改为SHA256 auth SHA256 :information_source:可选： 要通过VPN重定向客户端流量，请找到并取消注释redirect-gateway和dhcp-option选项： /etc/openvpn/server1.conf push \"redirect-gateway def1 bypass-dhcp\" push \"dhcp-option DNS 208.67.222.222\" push \"dhcp-option DNS 208.67.220.220\" 默认情况下，使用OpenDNS解析器。您可以更改它并使用CloudFlare，Google或您想要的任何其他DNS解析器。 完整配置文件参考： port 1194 proto udp dev tun ca ca.crt cert server1.crt key server1.key # This file should be kept secret dh dh.pem server 10.8.0.0 255.255.255.0 ifconfig-pool-persist ipp.txt push \"redirect-gateway def1 bypass-dhcp\" push \"dhcp-option DNS 208.67.222.222\" push \"dhcp-option DNS 208.67.220.220\" keepalive 10 120 tls-auth ta.key 0 # This file is secret cipher AES-256-CBC user nobody group nobody persist-key persist-tun status openvpn-status.log verb 3 explicit-exit-notify 1 auth SHA256 ","date":"2020-05-21","objectID":"/openvpn-for-centos/:2:5","tags":["OpenVPN","CentOS"],"title":"OpenVPN for CentOS","uri":"/openvpn-for-centos/"},{"categories":["VPN","CentOS"],"content":"启动 OpenVPN 服务 systemctl start openvpn@server1 成功启动之后，OpenVPN Server 会创建一个 tun 设备，即tun0： [root@test openvpn]\\# ip a show tun0 3: tun0: \u003cPOINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 100 link/none inet 10.8.0.1 peer 10.8.0.2/32 scope global tun0 valid_lft forever preferred_lft forever inet6 fe80::afaa:c6f7:7ae8:3a76/64 scope link flags 800 valid_lft forever preferred_lft forever 为了正确转发网络数据包，我们需要启用IP转发： echo \"net.ipv4.ip_forward = 1\" \u003e\u003e /etc/sysctl.conf sysctl -p ","date":"2020-05-21","objectID":"/openvpn-for-centos/:2:6","tags":["OpenVPN","CentOS"],"title":"OpenVPN for CentOS","uri":"/openvpn-for-centos/"},{"categories":["VPN","CentOS"],"content":"配置客户端接口 创建一个单独的SSL证书，并为每个VPN客户端生成一个不同的配置文件。 客户端私钥和证书请求可以在客户端计算机或服务器上生成。为简单起见，我们将在服务器上生成证书请求，然后将其发送到CA进行签名。 生成客户端证书和配置文件的整个过程如下： 在OpenVPN服务器上生成私钥和证书请求； 将请求发送到要签名的CA计算机； 将签名的SSL证书复制到OpenVPN服务器并生成配置文件； 将配置文件发送到VPN客户端的计算机。 首先，在OpenVPN主机上创建一组目录来存放客户端文件： mkdir -p ~/openvpn-clients/{configs,base,files} base 目录将存储将在所有客户端文件之间共享的基本文件和配置； configs 目录将存储生成的客户端配置； files 目录将存储特定于客户端的证书/密钥对。 复制ca.crt和ta.key文件到~/openvpn-clients/base目录： cp ~/EasyRSA-Server1/ta.key ~/openvpn-clients/base/ cp /etc/openvpn/ca.crt ~/openvpn-clients/base/ 将示例客户端配置文件复制到~/openvpn-clients/base/目录下： cp /usr/share/doc/openvpn-2.4.9/sample/sample-config-files/client.conf ~/openvpn-clients/base/ 编辑client.conf文件以匹配我们的服务器配置： remote 172.16.1.131 1194 # 在文件结尾添加以下内容 auth SHA256 key-direction 1 完整配置文件参考如下所示： client dev tun proto udp remote 172.16.1.131 1194 resolv-retry infinite nobind persist-key persist-tun remote-cert-tls server cipher AES-256-CBC verb 3 auth SHA256 key-direction 1 接下来，创建一个简单的bash脚本，它将基本配置和文件与客户端证书和密钥合并，并将生成的配置存储在~/openvpn-clients/configs目录中： vim ~/openvpn-clients/gen_config.sh #!/bin/bash FILES_DIR=$HOME/openvpn-clients/files BASE_DIR=$HOME/openvpn-clients/base CONFIGS_DIR=$HOME/openvpn-clients/configs BASE_CONF=${BASE_DIR}/client.conf CA_FILE=${BASE_DIR}/ca.crt TA_FILE=${BASE_DIR}/ta.key CLIENT_CERT=${FILES_DIR}/${1}.crt CLIENT_KEY=${FILES_DIR}/${1}.key # Test for files for i in \"$BASE_CONF\" \"$CA_FILE\" \"$TA_FILE\" \"$CLIENT_CERT\" \"$CLIENT_KEY\"; do if [[ ! -f $i ]]; then echo \" The file $idoes not exist\" exit 1 fi if [[ ! -r $i ]]; then echo \" The file $iis not readable.\" exit 1 fi done # Generate client config cat \u003e ${CONFIGS_DIR}/${1}.ovpn \u003c\u003cEOF $(cat ${BASE_CONF}) \u003ckey\u003e $(cat ${CLIENT_KEY}) \u003c/key\u003e \u003ccert\u003e $(cat ${CLIENT_CERT}) \u003c/cert\u003e \u003cca\u003e $(cat ${CA_FILE}) \u003c/ca\u003e \u003ctls-auth\u003e $(cat ${TA_FILE}) \u003c/tls-auth\u003e EOF 赋予执行权限： chmod +x ~/openvpn-clients/gen_config.sh ","date":"2020-05-21","objectID":"/openvpn-for-centos/:3:0","tags":["OpenVPN","CentOS"],"title":"OpenVPN for CentOS","uri":"/openvpn-for-centos/"},{"categories":["VPN","CentOS"],"content":"创建客户端证书私钥和配置 生成客户端私钥和证书请求的过程与生成服务器密钥和证书请求的过程相同。 登录OpenVPN主机为客户端生成一个新的私钥和一个证书请求： cd ~/EasyRSA-Server1 ./easyrsa gen-req client1 nopass 将私钥client1.key复制到~/openvpn-clients/files中： cp ~/EasyRSA-Server1/pki/private/client1.key ~/openvpn-clients/files/ 将证书请求文件传输到CA主机： cp ~/EasyRSA-Server1/pki/reqs/client1.req /tmp 进入CA主机，导入证书请求并进行签署： cd ~/EasyRSA-CA ./easyrsa import-req /tmp/client1.req client1 ./easyrsa sign-req client client1 接下来，将签好名的证书client1.crt文件发送回OpenVPN主机： cp ~/EasyRSA-CA/pki/issued/client1.crt ~/openvpn-clients/files 最后一步是使用gen_config.sh脚本生成客户端配置，切换到~/openvpn-clients目录并使用客户端名称作为参数运行脚本： cd ~/openvpn-clients ./gen_config.sh client1 该脚本将在~/client-configs/configs目录中创建一个名为client1.ovpn的文件： [root@test openvpn-clients]\\# ls ~/openvpn-clients/configs client1.ovpn 此时，客户端配置已创建，可以直接将配置文件传输到要用作客户端的设备上。 如果想要添加其他的客户端，重复这些步骤即可。 ","date":"2020-05-21","objectID":"/openvpn-for-centos/:4:0","tags":["OpenVPN","CentOS"],"title":"OpenVPN for CentOS","uri":"/openvpn-for-centos/"},{"categories":["VPN","CentOS"],"content":"使用 OpenVPN 在Debian系系统下： sudo apt update -y \u0026\u0026 sudo apt instal openvpn 安装完软件包之后，使用以下命令连接到OpenVPN服务器： sudo openvpn --config client1.ovpn ","date":"2020-05-21","objectID":"/openvpn-for-centos/:5:0","tags":["OpenVPN","CentOS"],"title":"OpenVPN for CentOS","uri":"/openvpn-for-centos/"},{"categories":["VPN","CentOS"],"content":"快速安装 OpenVPN sudo apt update -y sudo apt install -y openvpn 从 github 获取一键安装脚本: git clone https://github.com/angristan/openvpn-install.git cd openvpn-install 赋予脚本可执行权限: chmod +x openvpn-install 执行脚本: AUTO_INSTALL=y ./openvpn-install.sh :warning: 注意: 在客户端使用 OpenVPN 时, 要确保服务器端的 OpenVPN 监听的端口处于放行状态. Linux 客户端使用: sudo apt install network-manager-openvpn sudo openvpn ./client.ovpn Windows 客户端使用: 直接去OpenVPN 官方下载站点下载然后导入客户端配置文件 client.ovpn, 然后启动服务即可. ","date":"2020-05-21","objectID":"/openvpn-for-centos/:6:0","tags":["OpenVPN","CentOS"],"title":"OpenVPN for CentOS","uri":"/openvpn-for-centos/"},{"categories":["VPN","Ubuntu"],"content":"WireGuard 简介 ​ WireGuard是具有最新加密技术的现代VPN（虚拟专用网）技术。与IPsec和OpenVPN等其他类似解决方案相比，WireGuard更快，更易于配置且性能更高。它是一个跨平台，几乎可以在任何地方运行，包括Linux，Windows，Android和macOS。 Wireguard是对等VPN。它不使用客户端-服务器模型。根据其配置，对等方可以充当传统的服务器或客户端。 WireGuard通过在充当隧道的每个对等设备上创建网络接口来工作。对等体通过交换和验证公共密钥（类似于SSH模型）来相互认证。公钥与隧道中允许的IP地址列表进行映射。 VPN流量封装在UDP中。在本教程中，我们将在充当VPN服务器的Ubuntu 18.04计算机上设置WireGuard。我们还将向您展示如何将WireGuard配置为客户端。客户端的流量将通过Ubuntu 18.04服务器进行路由。此设置可用于防御中间人攻击，匿名浏览网络，绕过受地域限制的内容，或允许您的同事在远程工作时安全地连接到公司网络。 ","date":"2020-05-19","objectID":"/wireguard-vpn-for-ubuntu/:1:0","tags":["VPN","WireGuard","Ubuntu"],"title":"WireGuard VPN for Ubuntu","uri":"/wireguard-vpn-for-ubuntu/"},{"categories":["VPN","Ubuntu"],"content":"WG安装 更新软件包列表，并安装管理系统存储库所需的工具： sudo apt update sudo apt install software-properties-common 添加 WireGuard 仓库： sudo add-apt-repository ppa:wireguard/wireguard 出现提示时，按Enter键继续。 add-apt-repository也将自动更新软件包列表。 安装WireGuard软件包： sudo apt install wireguard WireGuard作为内核模块运行，该模块被编译为DKMS模块。成功完成后，您将看到以下输出： wireguard: Running module version sanity check. - Original module - No original module exists within this kernel - Installation - Installing to /lib/modules/4.15.0-88-generic/updates/dkms/ depmod... DKMS: install completed. 更新内核时，需要将针对新内核编译WireGuard模块。 ","date":"2020-05-19","objectID":"/wireguard-vpn-for-ubuntu/:2:0","tags":["VPN","WireGuard","Ubuntu"],"title":"WireGuard VPN for Ubuntu","uri":"/wireguard-vpn-for-ubuntu/"},{"categories":["VPN","Ubuntu"],"content":"配置WG WireGuard 程序包附带了两个名为wg和wg-quick的命令行工具，可用于配置和管理WireGuard接口。 运行以下命令以生成公钥和私钥： wg genkey | sudo tee /etc/wireguard/privatekey | wg pubkey | sudo tee /etc/wireguard/publickey :information_source: 公私钥放置于/etc/wireguard目录,其中私钥绝对不能与任何人共享 生成密钥后，我们需要配置路由VPN的隧道设备 可以使用ip和wg从命令行设置设备，也可以使用文本编辑器创建配置文件。 创建一个名为wg0.conf(文件名可以随意)的新文件，并添加以下内容： cat \u003e\u003e /etc/wireguard/wg0.conf \u003c\u003c EOF [Interface] Address = 192.168.159.0/24 # 填写网络地址段 SaveConfig = true # 关闭时，接口的当前状态将保存到配置文件中 ListenPort = 51820 # 监听端口 PrivateKey = QNKQCtPo2E5saDnXORaIORhZH6NtcvIJPHqF9EdEL1o= # 私钥文件,即/etc/wireguard/privatekey PostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -t nat -A POSTROUTING -o ens33 -j MASQUERADE # 在启动之前执行的命令或脚本,使用iptables启用伪装,允许流量离开服务器，从而使VPN客户端可以访问Internet。 PostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -t nat -D POSTROUTING -o ens33 -j MASQUERADE # 在启动之后执行的命令或脚本,接口关闭后，iptables规则将被删除 EOF 修改privatekey和wg0.conf的权限, 保证其安全性： sudo chmod 600 /etc/wireguard/{privatekey,wg0.conf} 完成后，使用配置文件中指定的属性启动wg0接口： $ sudo wg-quick up wg0 * 输出内容如下所示: [#] ip link add wg0 type wireguard [#] wg setconf wg0 /dev/fd/63 [#] ip -4 address add 192.168.159.0/24 dev wg0 [#] ip link set mtu 1420 up dev wg0 [#] iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o ens33 -j MASQUERADE 运行wg show wg0检查接口状态和配置： $ sudo wg show wg0 * 输出内容如下所示: interface: wg0 public key: uD6Xex2eP5CEUVcVP3EZB5csh2JReWXthoVZMzURwCQ= private key: (hidden) listening port: 51820 也可以运行ip a show wg0来验证接口状态： $ ip a show wg0 * 输出内容如下所示: 4: wg0: \u003cPOINTOPOINT,NOARP,UP,LOWER_UP\u003e mtu 1420 qdisc noqueue state UNKNOWN group default qlen 1000 link/none inet 192.168.159.0/24 scope global wg0 valid_lft forever preferred_lft forever 设置wireguard 开机自启: sudo systemctl enable wg-quick@wg0 ","date":"2020-05-19","objectID":"/wireguard-vpn-for-ubuntu/:3:0","tags":["VPN","WireGuard","Ubuntu"],"title":"WireGuard VPN for Ubuntu","uri":"/wireguard-vpn-for-ubuntu/"},{"categories":["VPN","Ubuntu"],"content":"服务器网络和防火墙配置 为了使NAT正常工作，我们需要启用IP转发，打开/etc/sysctl.conf文件，并添加或取消注释以下行： ... net.ipv4.ip_forward=1 ... 修改完成之后, 执行sudo sysctl -p使配置永久生效 如果你使用UFW来管理防火墙，则需要在端口51820上打开UDP通信： sudo ufw allow 51820/udp 至此,WG SERVER 已完成配置 ","date":"2020-05-19","objectID":"/wireguard-vpn-for-ubuntu/:4:0","tags":["VPN","WireGuard","Ubuntu"],"title":"WireGuard VPN for Ubuntu","uri":"/wireguard-vpn-for-ubuntu/"},{"categories":["VPN","Ubuntu"],"content":"客户端配置 ","date":"2020-05-19","objectID":"/wireguard-vpn-for-ubuntu/:5:0","tags":["VPN","WireGuard","Ubuntu"],"title":"WireGuard VPN for Ubuntu","uri":"/wireguard-vpn-for-ubuntu/"},{"categories":["VPN","Ubuntu"],"content":"Linux \u0026 macOS 去往官方下载站点(https://wireguard.com/install/), 查看如何安装WireGuard macOS App Store: https://apps.apple.com/us/app/wireguard/id1441195209?ls=1 Ubuntu ≤ 19.04 [module – v1.0.20200413 \u0026 tools – v1.0.20200319] $ sudo add-apt-repository ppa:wireguard/wireguard $ sudo apt-get update $ sudo apt-get install wireguard 设置Linux和macOS客户端过程基本相同, 首先生成公钥与私钥 wg genkey | sudo tee /etc/wireguard/privatekey | wg pubkey | sudo tee /etc/wireguard/publickey 创建/etc/wireguard/wg0.conf并添加以下内容: [Interface] PrivateKey = CLIENT_PRIVATE_KEY # /etc/wireguard/privatekey Address = 192.168.43.0/24 [Peer] PublicKey = SERVER_PUBLIC_KEY # /etc/wireguard/publickey Endpoint = SERVER_IP_ADDRESS:51820 # 对方wireguard server的ip和端口 AllowedIPs = 0.0.0.0/0 ","date":"2020-05-19","objectID":"/wireguard-vpn-for-ubuntu/:5:1","tags":["VPN","WireGuard","Ubuntu"],"title":"WireGuard VPN for Ubuntu","uri":"/wireguard-vpn-for-ubuntu/"},{"categories":["VPN","Ubuntu"],"content":"Windows 直接下载客户端工具: https://download.wireguard.com/windows-client/wireguard-amd64-0.1.0.msi 安装好软件之后, 点击左下角的Add Tunnel, 创建一个新的隧道Create new tunnel name随便起喽, 内容如下: [Interface] PrivateKey = MOeXEby5OG1xQBCP9AJEJEsxmxYDG1FHHzlcOgi/ClI= Address = 192.168.43.0/24 [Peer] PublicKey = uD6Xex2eP5CEUVcVP3EZB5csh2JReWXthoVZMzURwCQ= # 服务器端公钥 Endpoint = 192.168.159.132:51820 # 服务器ip以及端口 AllowedIPs = 0.0.0.0/0 ","date":"2020-05-19","objectID":"/wireguard-vpn-for-ubuntu/:5:2","tags":["VPN","WireGuard","Ubuntu"],"title":"WireGuard VPN for Ubuntu","uri":"/wireguard-vpn-for-ubuntu/"},{"categories":["VPN","Ubuntu"],"content":"将客户端对等方添加到服务器 最后一步是将客户端公钥和IP地址添加到服务器： # sudo wg set wg0 peer CLIENT_PUBLIC_KEY allowed-ips 10.0.0.2 sudo wg set wg0 peer T5ZTibLaWh9/3EzA1ZfCdiojM0HfXvh99mfVlqHpaU0= allowed-ips 192.168.43.0/24 sudo wg set wg0 peer UqF/BDwShHFulAUN4yx0latMIiIW0Cbb+IuNHEYEBj0= allowed-ips 192.168.43.0/24 确保使用在客户端计算机上生成的公用密钥·（sudo cat /etc/wireguard/publickey）·更改CLIENT_PUBLIC_KEY并调整客户端IP地址（如果不同的话），Windows用户可以从WireGuard软件当中复制公钥 ","date":"2020-05-19","objectID":"/wireguard-vpn-for-ubuntu/:6:0","tags":["VPN","WireGuard","Ubuntu"],"title":"WireGuard VPN for Ubuntu","uri":"/wireguard-vpn-for-ubuntu/"},{"categories":["VPN","Ubuntu"],"content":"Linux 和 macos 客户端 在Linux客户端上，运行以下命令以打开界面： sudo wg-quick up wg0 现在，应该已连接到Ubuntu服务器，并且来自客户端计算机的流量应通过该服务器进行路由，可以使用以下方法检查连接： $ sudo wg * 输出内容如下所示： interface: wg0 public key: sZThYo/0oECwzUsIKTa6LYXLhk+Jb/nqK4kCCP2pyFg= private key: (hidden) listening port: 48052 fwmark: 0xca6c peer: r3imyh3MCYggaZACmkx+CxlD6uAmICI8pe/PGq8+qCg= endpoint: XXX.XXX.XXX.XXX:51820 allowed ips: 0.0.0.0/0 latest handshake: 1 minute, 22 seconds ago transfer: 58.43 KiB received, 70.82 KiB sent 停止和关闭隧道： sudo wg-quick down wg0 ","date":"2020-05-19","objectID":"/wireguard-vpn-for-ubuntu/:6:1","tags":["VPN","WireGuard","Ubuntu"],"title":"WireGuard VPN for Ubuntu","uri":"/wireguard-vpn-for-ubuntu/"},{"categories":["VPN","Ubuntu"],"content":"Windows 客户端 点击软件上的Activate 即可 ","date":"2020-05-19","objectID":"/wireguard-vpn-for-ubuntu/:6:2","tags":["VPN","WireGuard","Ubuntu"],"title":"WireGuard VPN for Ubuntu","uri":"/wireguard-vpn-for-ubuntu/"},{"categories":["VPN","Ubuntu"],"content":"一键安装 wireguard https://github.com/angristan/wireguard-install https://github.com/l-n-s/wireguard-install ","date":"2020-05-19","objectID":"/wireguard-vpn-for-ubuntu/:7:0","tags":["VPN","WireGuard","Ubuntu"],"title":"WireGuard VPN for Ubuntu","uri":"/wireguard-vpn-for-ubuntu/"},{"categories":["VPN","Ubuntu"],"content":"参考链接 wireguard Quick Start : https://www.wireguard.com/quickstart/ wireguard Install : https://www.wireguard.com/install/ ","date":"2020-05-19","objectID":"/wireguard-vpn-for-ubuntu/:8:0","tags":["VPN","WireGuard","Ubuntu"],"title":"WireGuard VPN for Ubuntu","uri":"/wireguard-vpn-for-ubuntu/"},{"categories":[],"content":"Markdown is created by Daring Fireball, the original guideline is here. Its syntax, however, varies between different parsers or editors. Please note that HTML fragments in markdown source will be recognized but not parsed or rendered. Also, there may be small reformatting on the original markdown source code after saving. ","date":"2020-04-16","objectID":"/hello-world/:0:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Paragraph and line breaks(段落与换行) A paragraph is simply one or more consecutive lines of text. In markdown source code, paragraphs are separated by more than one blank lines. In Typora, you only need to press Return to create a new paragraph. Press Shift + Return to create a single line break. However, most markdown parser will ignore single line break, to make other markdown parsers recognize your line break, you can leave two whitespace at the end of the line, or insert \u003cbr/\u003e. ","date":"2020-04-16","objectID":"/hello-world/:1:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Headers(标题) Headers use 1-6 hash characters at the start of the line, corresponding to header levels 1-6. For example: # This is an H1 ## This is an H2 ###### This is an H6 In typora, input ‘#’s followed by title content, and press Return key will create a header. ","date":"2020-04-16","objectID":"/hello-world/:2:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Blockquotes(引言) Markdown uses email-style \u003e characters for block quoting. They are presented as: This is a blockquote with two paragraphs. This is first paragraph. This is second pragraph.Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. This is another blockquote with one paragraph. There is three empty line to seperate two blockquote. 这是一段中文测试。 In typora, just input ‘\u003e’ followed by quote contents a block quote is generated. Typora will insert proper ‘\u003e’ or line break for you. Block quote inside anther block quote is allowed by adding additional levels of ‘\u003e’. ","date":"2020-04-16","objectID":"/hello-world/:3:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Lists(有序无序列表) Input * list item 1 will create an un-ordered list, the * symbol can be replace with + or -. Input 1. list item 1 will create an ordered list, their markdown source code is like: Red Green Blue Red Green Blue ","date":"2020-04-16","objectID":"/hello-world/:4:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Task List(任务列表) Task lists are lists with items marked as either [ ] or [x] (incomplete or complete). For example: a task list item list syntax required normal formatting, @mentions, #1234 refs incomplete completed You can change the complete/incomplete state by click the checkbox before the item. ","date":"2020-04-16","objectID":"/hello-world/:5:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Syntax Highlighting(代码高亮) function helloWorld () { alert(\"Hello, World!\") } public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello, World!\"); } } ","date":"2020-04-16","objectID":"/hello-world/:6:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Math Blocks(数学块) You can render LaTeX mathematical expressions using MathJax. Input $$, then press ‘Return’ key will trigger an input field which accept Tex/LaTex source. Following is an example: $$ \\mathbf{V}_1 \\times \\mathbf{V}_2 = \\begin{vmatrix} \\mathbf{i} \u0026 \\mathbf{j} \u0026 \\mathbf{k} \\ \\frac{\\partial X}{\\partial u} \u0026 \\frac{\\partial Y}{\\partial u} \u0026 0 \\ \\frac{\\partial X}{\\partial v} \u0026 \\frac{\\partial Y}{\\partial v} \u0026 0 \\ \\end{vmatrix} $$ In markdown source file, math block is LaTeX expression wrapped by ‘$$’ mark: $$ \\mathbf{V}_1 \\times \\mathbf{V}_2 = \\begin{vmatrix} \\mathbf{i} \u0026 \\mathbf{j} \u0026 \\mathbf{k} \\\\ \\frac{\\partial X}{\\partial u} \u0026 \\frac{\\partial Y}{\\partial u} \u0026 0 \\\\ \\frac{\\partial X}{\\partial v} \u0026 \\frac{\\partial Y}{\\partial v} \u0026 0 \\\\ \\end{vmatrix} $$ ","date":"2020-04-16","objectID":"/hello-world/:7:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Tables(表格) Input | First Header | Second Header | and press return key will create a table with two column. After table is created, focus on that table will pop up a toolbar for table, where you can resize, align, or delete table. You can also use context menu to copy and add/delete column/row. Following descriptions can be skipped, as markdown source code for tables are generated by typora automatically. In markdown source code, they look like: | Name | Markdown | HTML tag | | ----------------- | ------------------- | -------------------- | | *Emphasis* | `*Emphasis*` | `\u003cem\u003e\u003c/em\u003e` | | **Strong** | `**Strong**` | `\u003cstrong\u003e\u003c/strong\u003e` | | `code` | ``code`` | `\u003ccode\u003e\u003c/code\u003e` | | ~~Strikethrough~~ | `~~Strikethrough~~` | `\u003cdel\u003e\u003c/del` | | \u003cu\u003eUnderline\u003c/u\u003e | `\u003cu\u003eunderline\u003c/u\u003e` | `\u003cu\u003e\u003c/u\u003e` | Name Markdown HTML tag Emphasis *Emphasis* \u003cem\u003e\u003c/em\u003e Strong **Strong** \u003cstrong\u003e\u003c/strong\u003e code `code` \u003ccode\u003e\u003c/code\u003e Strikethrough ~~Strikethrough~~ \u003cdel\u003e\u003c/del Underline \u003cu\u003eunderline\u003c/u\u003e \u003cu\u003e\u003c/u\u003e ","date":"2020-04-16","objectID":"/hello-world/:8:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Footnotes(注释) You can create footnotes like this[^footnote]. [^footnote]: Here is the *text* of the **footnote**. will produce: You can create footnotes like this1. Mouse on the ‘footnote’ superscript to see content of the footnote. ","date":"2020-04-16","objectID":"/hello-world/:9:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Horizontal Rules(水平线) Input *** or --- on a blank line and press return will draw a horizontal line. ","date":"2020-04-16","objectID":"/hello-world/:10:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Links(超链接) Markdown supports two style of links: inline and reference. In both styles, the link text is delimited by [square brackets]. To create an inline link, use a set of regular parentheses immediately after the link text’s closing square bracket. Inside the parentheses, put the URL where you want the link to point, along with an optional title for the link, surrounded in quotes. For example: This is [an example](http://example.com/ \"Title\") inline link. [This link](http://example.net/) has no title attribute. will produce: This is an example inline link. (\u003cp\u003eThis is \u003ca href=\"http://example.com/\" title=\"Title\"\u003e) This link has no title attribute. (\u003cp\u003e\u003ca href=\"http://example.net/\"\u003eThis link\u003c/a\u003e has no) ","date":"2020-04-16","objectID":"/hello-world/:11:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Internal Links(页内链接) You can set the href to headers, which will create a bookmark that allow you to jump to that section after clicking. For example: Command(on Windows: Ctrl) + Click This link will jump to header Block Elements. To see how to write that, please move cursor or click that link with ⌘ key pressed to expand the element into markdown source. ","date":"2020-04-16","objectID":"/hello-world/:11:1","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Reference Links(参考链接) Reference-style links use a second set of square brackets, inside which you place a label of your choosing to identify the link: This is [an example][id] reference-style link. Then, anywhere in the document, you define your link label like this, on a line by itself: [id]: http://example.com/ \"Optional Title Here\" In typora, they will be rendered like: This is an example reference-style link. The implicit link name shortcut allows you to omit the name of the link, in which case the link text itself is used as the name. Just use an empty set of square brackets — e.g., to link the word “Google” to the google.com web site, you could simply write: [Google][] And then define the link: [Google]: http://google.com/ In typora click link will expand it for editing, command+click will open the hyperlink in web browser. ","date":"2020-04-16","objectID":"/hello-world/:11:2","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"URLs Typora allows you to insert urls as links, wrapped by \u003cbrackets\u003e. \u003ci@typora.io\u003e becomes i@typora.io. Typora will aslo auto link standard URLs. e.g: www.google.com. ","date":"2020-04-16","objectID":"/hello-world/:12:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Images(图片) Image looks similar with links, but it requires an additional ! char before the start of link. Image syntax looks like this: ![Alt text](/path/to/img.jpg) ![Alt text](/path/to/img.jpg \"Optional title\") You are able to use drag \u0026 drop to insert image from image file or we browser. And modify the markdown source code by clicking on the image. Relative path will be used if image is in same directory or sub-directory with current editing document when drag \u0026 drop. For more tips on images, please read http://support.typora.io//Images/ ","date":"2020-04-16","objectID":"/hello-world/:13:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Emphasis(突出强调) Markdown treats asterisks (*) and underscores (_) as indicators of emphasis. Text wrapped with one * or _ will be wrapped with an HTML \u003cem\u003e tag. E.g: *single asterisks* _single underscores_ output: single asterisks single underscores GFM will ignores underscores in words, which is commonly used in code and names, like this: wow_great_stuff do_this_and_do_that_and_another_thing. To produce a literal asterisk or underscore at a position where it would otherwise be used as an emphasis delimiter, you can backslash escape it: \\*this text is surrounded by literal asterisks\\* Typora recommends to use * symbol. ","date":"2020-04-16","objectID":"/hello-world/:14:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Strong(加粗) double *’s or _’s will be wrapped with an HTML \u003cstrong\u003e tag, e.g: **double asterisks** __double underscores__ output: double asterisks double underscores Typora recommends to use ** symbol. ","date":"2020-04-16","objectID":"/hello-world/:15:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Code(代码块) To indicate a span of code, wrap it with backtick quotes (`). Unlike a pre-formatted code block, a code span indicates code within a normal paragraph. For example: Use the `printf()` function. will produce: Use the printf() function. ","date":"2020-04-16","objectID":"/hello-world/:16:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"highlight shortcode(高亮代码块) example: {{\u003c highlight go \"linenos=table,hl_lines=8 15-17,linenostart=199\" \u003e}} // ... code {{\u003c /highlight \u003e}} result: // GetTitleFunc returns a func that can be used to transform a string to // title case. // // The supported styles are // // - \"Go\" (strings.Title) // - \"AP\" (see https://www.apstylebook.com/) // - \"Chicago\" (see http://www.chicagomanualofstyle.org/home.html) // // If an unknown or empty style is provided, AP style is what you get. func GetTitleFunc(style string) func(s string) string { switch strings.ToLower(style) { case \"go\": return strings.Title case \"chicago\": tc := transform.NewTitleConverter(transform.ChicagoStyle) return tc.Title default: tc := transform.NewTitleConverter(transform.APStyle) return tc.Title } } ","date":"2020-04-16","objectID":"/hello-world/:17:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Strikethrough(删除线) GFM adds syntax to create strikethrough text, which is missing from standard Markdown. ~~Mistaken text.~~ becomes Mistaken text. ","date":"2020-04-16","objectID":"/hello-world/:18:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Underline(下划线) Underline is powered by raw HTML. \u003cu\u003eUnderline\u003c/u\u003e becomes Underline. ","date":"2020-04-16","objectID":"/hello-world/:19:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Emoji :happy:(表情) Input emoji with syntax :smile:. User can trigger auto-complete suggestions for emoji by pressing ESC key, or trigger it automatically after enable it on preference panel. Also, input UTF8 emoji char directly from Edit -\u003e Emoji \u0026 Symbols from menu bar is also supported. ","date":"2020-04-16","objectID":"/hello-world/:20:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":[],"content":"Inline Math To use this feature, first, please enable it in Preference Panel -\u003e Markdown Tab. Then use $ to wrap TeX command, for example: $\\lim_{x \\to \\infty} \\exp(-x) = 0$ will be rendered as LaTeX command. To trigger inline preview for inline math: input “$”, then press ESC key, then input TeX command, a preview tooltip will be visible like below: Here is the text of the footnote. ↩︎ ","date":"2020-04-16","objectID":"/hello-world/:21:0","tags":[],"title":"Hello World","uri":"/hello-world/"},{"categories":["Markdown"],"content":"这篇文章展示了基本的 Markdown 语法和格式.","date":"2019-12-01","objectID":"/basic-markdown-syntax/","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"这篇文章提供了可以在 Hugo 的文章中使用的基本 Markdown 语法示例. 注意 这篇文章借鉴了一篇很棒的来自 Grav 的文章. 如果你想了解 Loveit 主题的扩展 Markdown 语法, 请阅读扩展 Markdown 语法页面. 事实上, 编写 Web 内容很麻烦. WYSIWYG所见即所得 编辑器帮助减轻了这一任务. 但通常会导致代码太糟, 或更糟糕的是, 网页也会很丑. 没有通常伴随的所有复杂和丑陋的问题, Markdown 是一种更好的生成 HTML 内容的方式. 一些主要好处是: Markdown 简单易学, 几乎没有多余的字符, 因此编写内容也更快. 用 Markdown 书写时出错的机会更少. 可以产生有效的 XHTML 输出. 将内容和视觉显示保持分开, 这样就不会打乱网站的外观. 可以在你喜欢的任何文本编辑器或 Markdown 应用程序中编写内容. Markdown 使用起来很有趣! John Gruber, Markdown 的作者如是说: Markdown 格式的首要设计目标是更具可读性. 最初的想法是 Markdown 格式的文档应当以纯文本形式发布, 而不会看起来像被标签或格式说明所标记. 虽然 Markdown 的语法受到几种现有的文本到 HTML 转换工具的影响, 但 Markdown 语法的最大灵感来源是纯文本电子邮件的格式. – John Gruber 话不多说, 我们来回顾一下 Markdown 的主要语法以及生成的 HTML 样式! 技巧  将此页保存为书签，以备将来参考! ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:0:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"1 标题 从 h2 到 h6 的标题在每个级别上都加上一个 ＃: ## h2 标题 ### h3 标题 #### h4 标题 ##### h5 标题 ###### h6 标题 输出的 HTML 看起来像这样: \u003ch2\u003eh2 标题\u003c/h2\u003e \u003ch3\u003eh3 标题\u003c/h3\u003e \u003ch4\u003eh4 标题\u003c/h4\u003e \u003ch5\u003eh5 标题\u003c/h5\u003e \u003ch6\u003eh6 标题\u003c/h6\u003e 标题 ID 要添加自定义标题 ID, 请在与标题相同的行中将自定义 ID 放在花括号中: ### 一个很棒的标题 {#custom-id} 输出的 HTML 看起来像这样: \u003ch3 id=\"custom-id\"\u003e一个很棒的标题\u003c/h3\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:1:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"2 注释 注释是和 HTML 兼容的： \u003c!-- 这是一段注释 --\u003e 不能看到以下的注释: ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:2:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"3 水平线 HTML 中的 \u003chr\u003e 标签是用来在段落元素之间创建一个 “专题间隔” 的. 使用 Markdown, 你可以用以下方式创建一个 \u003chr\u003e 标签: ___: 三个连续的下划线 ---: 三个连续的破折号 ***: 三个连续的星号 呈现的输出效果如下: ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:3:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"4 段落 按照纯文本的方式书写段落, 纯文本在呈现的 HTML 中将用 \u003cp\u003e/\u003c/p\u003e 标签包裹. 如下段落: Lorem ipsum dolor sit amet, graecis denique ei vel, at duo primis mandamus. Et legere ocurreret pri, animal tacimates complectitur ad cum. Cu eum inermis inimicus efficiendi. Labore officiis his ex, soluta officiis concludaturque ei qui, vide sensibus vim ad. 输出的 HTML 看起来像这样: \u003cp\u003eLorem ipsum dolor sit amet, graecis denique ei vel, at duo primis mandamus. Et legere ocurreret pri, animal tacimates complectitur ad cum. Cu eum inermis inimicus efficiendi. Labore officiis his ex, soluta officiis concludaturque ei qui, vide sensibus vim ad.\u003c/p\u003e 可以使用一个空白行进行换行. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:4:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"5 内联 HTML 元素 如果你需要某个 HTML 标签 (带有一个类), 则可以简单地像这样使用: Markdown 格式的段落. \u003cdiv class=\"class\"\u003e 这是 \u003cb\u003eHTML\u003c/b\u003e \u003c/div\u003e Markdown 格式的段落. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:5:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"6 强调 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"加粗 用于强调带有较粗字体的文本片段. 以下文本片段会被 渲染为粗体. **渲染为粗体** __渲染为粗体__ 输出的 HTML 看起来像这样: \u003cstrong\u003e渲染为粗体\u003c/strong\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:1","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"斜体 用于强调带有斜体的文本片段. 以下文本片段被 渲染为斜体. *渲染为斜体* _渲染为斜体_ 输出的 HTML 看起来像这样: \u003cem\u003e渲染为斜体\u003c/em\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:2","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"删除线 按照 GFMGitHub flavored Markdown 你可以使用删除线. ~~这段文本带有删除线.~~ 呈现的输出效果如下: 这段文本带有删除线. 输出的 HTML 看起来像这样: \u003cdel\u003e这段文本带有删除线.\u003c/del\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:3","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"组合 加粗, 斜体, 和删除线可以 组合使用. ***加粗和斜体*** ~~**删除线和加粗**~~ ~~*删除线和斜体*~~ ~~***加粗, 斜体和删除线***~~ 呈现的输出效果如下: 加粗和斜体 删除线和加粗 删除线和斜体 加粗, 斜体和删除线 输出的 HTML 看起来像这样: \u003cem\u003e\u003cstrong\u003e加粗和斜体\u003c/strong\u003e\u003c/em\u003e \u003cdel\u003e\u003cstrong\u003e删除线和加粗\u003c/strong\u003e\u003c/del\u003e \u003cdel\u003e\u003cem\u003e删除线和斜体\u003c/em\u003e\u003c/del\u003e \u003cdel\u003e\u003cem\u003e\u003cstrong\u003e加粗, 斜体和删除线\u003c/strong\u003e\u003c/em\u003e\u003c/del\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:4","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"7 引用 用于在文档中引用其他来源的内容块. 在要引用的任何文本之前添加 \u003e: \u003e **Fusion Drive** combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. 呈现的输出效果如下: Fusion Drive combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. 输出的 HTML 看起来像这样: \u003cblockquote\u003e \u003cp\u003e \u003cstrong\u003eFusion Drive\u003c/strong\u003e combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. \u003c/p\u003e \u003c/blockquote\u003e 引用也可以嵌套: \u003e Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Nunc augue augue, aliquam non hendrerit ac, commodo vel nisi. \u003e\u003e Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctor odio non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam. 呈现的输出效果如下: Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Nunc augue augue, aliquam non hendrerit ac, commodo vel nisi. Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctor odio non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:7:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"8 列表 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"无序列表 一系列项的列表, 其中项的顺序没有明显关系. 你可以使用以下任何符号来表示无序列表中的项: * 一项内容 - 一项内容 + 一项内容 例如: * Lorem ipsum dolor sit amet * Consectetur adipiscing elit * Integer molestie lorem at massa * Facilisis in pretium nisl aliquet * Nulla volutpat aliquam velit * Phasellus iaculis neque * Purus sodales ultricies * Vestibulum laoreet porttitor sem * Ac tristique libero volutpat at * Faucibus porta lacus fringilla vel * Aenean sit amet erat nunc * Eget porttitor lorem 呈现的输出效果如下: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Phasellus iaculis neque Purus sodales ultricies Vestibulum laoreet porttitor sem Ac tristique libero volutpat at Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem 输出的 HTML 看起来像这样: \u003cul\u003e \u003cli\u003eLorem ipsum dolor sit amet\u003c/li\u003e \u003cli\u003eConsectetur adipiscing elit\u003c/li\u003e \u003cli\u003eInteger molestie lorem at massa\u003c/li\u003e \u003cli\u003eFacilisis in pretium nisl aliquet\u003c/li\u003e \u003cli\u003eNulla volutpat aliquam velit \u003cul\u003e \u003cli\u003ePhasellus iaculis neque\u003c/li\u003e \u003cli\u003ePurus sodales ultricies\u003c/li\u003e \u003cli\u003eVestibulum laoreet porttitor sem\u003c/li\u003e \u003cli\u003eAc tristique libero volutpat at\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003eFaucibus porta lacus fringilla vel\u003c/li\u003e \u003cli\u003eAenean sit amet erat nunc\u003c/li\u003e \u003cli\u003eEget porttitor lorem\u003c/li\u003e \u003c/ul\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:1","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"有序列表 一系列项的列表, 其中项的顺序确实很重要. 1. Lorem ipsum dolor sit amet 2. Consectetur adipiscing elit 3. Integer molestie lorem at massa 4. Facilisis in pretium nisl aliquet 5. Nulla volutpat aliquam velit 6. Faucibus porta lacus fringilla vel 7. Aenean sit amet erat nunc 8. Eget porttitor lorem 呈现的输出效果如下: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem 输出的 HTML 看起来像这样: \u003col\u003e \u003cli\u003eLorem ipsum dolor sit amet\u003c/li\u003e \u003cli\u003eConsectetur adipiscing elit\u003c/li\u003e \u003cli\u003eInteger molestie lorem at massa\u003c/li\u003e \u003cli\u003eFacilisis in pretium nisl aliquet\u003c/li\u003e \u003cli\u003eNulla volutpat aliquam velit\u003c/li\u003e \u003cli\u003eFaucibus porta lacus fringilla vel\u003c/li\u003e \u003cli\u003eAenean sit amet erat nunc\u003c/li\u003e \u003cli\u003eEget porttitor lorem\u003c/li\u003e \u003c/ol\u003e 技巧 如果你对每一项使用 1., Markdown 将自动为每一项编号. 例如: 1. Lorem ipsum dolor sit amet 1. Consectetur adipiscing elit 1. Integer molestie lorem at massa 1. Facilisis in pretium nisl aliquet 1. Nulla volutpat aliquam velit 1. Faucibus porta lacus fringilla vel 1. Aenean sit amet erat nunc 1. Eget porttitor lorem 呈现的输出效果如下: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:2","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"任务列表 任务列表使你可以创建带有复选框的列表. 要创建任务列表, 请在任务列表项之前添加破折号 (-) 和带有空格的方括号 ([ ]). 要选择一个复选框，请在方括号之间添加 x ([x]). - [x] Write the press release - [ ] Update the website - [ ] Contact the media 呈现的输出效果如下: Write the press release Update the website Contact the media ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:3","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"9 代码 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"行内代码 用 ` 包装行内代码段. 在这个例子中, `\u003csection\u003e\u003c/section\u003e` 会被包裹成 **代码**. 呈现的输出效果如下: 在这个例子中, \u003csection\u003e\u003c/section\u003e 会被包裹成 代码. 输出的 HTML 看起来像这样: \u003cp\u003e 在这个例子中, \u003ccode\u003e\u0026lt;section\u0026gt;\u0026lt;/section\u0026gt;\u003c/code\u003e 会被包裹成 \u003cstrong\u003e代码\u003c/strong\u003e. \u003c/p\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:1","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"缩进代码 将几行代码缩进至少四个空格，例如: // Some comments line 1 of code line 2 of code line 3 of code 呈现的输出效果如下: // Some comments line 1 of code line 2 of code line 3 of code 输出的 HTML 看起来像这样: \u003cpre\u003e \u003ccode\u003e // Some comments line 1 of code line 2 of code line 3 of code \u003c/code\u003e \u003c/pre\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:2","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"围栏代码块 使用 “围栏” ``` 来生成一段带有语言属性的代码块. ```markdown Sample text here... ``` 输出的 HTML 看起来像这样: \u003cpre language-html\u003e \u003ccode\u003eSample text here...\u003c/code\u003e \u003c/pre\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:3","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"语法高亮 GFMGitHub Flavored Markdown 也支持语法高亮. 要激活它，只需在第一个代码 “围栏” 之后直接添加你要使用的语言的文件扩展名, ```js, 语法高亮显示将自动应用于渲染的 HTML 中. 例如, 在以下 JavaScript 代码中应用语法高亮: ```js grunt.initConfig({ assemble: { options: { assets: 'docs/assets', data: 'src/data/*.{json,yml}', helpers: 'src/custom-helpers.js', partials: ['src/partials/**/*.{hbs,md}'] }, pages: { options: { layout: 'default.hbs' }, files: { './': ['src/templates/pages/index.hbs'] } } } }; ``` 呈现的输出效果如下: grunt.initConfig({ assemble: { options: { assets: 'docs/assets', data: 'src/data/*.{json,yml}', helpers: 'src/custom-helpers.js', partials: ['src/partials/**/*.{hbs,md}'] }, pages: { options: { layout: 'default.hbs' }, files: { './': ['src/templates/pages/index.hbs'] } } } }; 注意 Hugo 文档中的 语法高亮页面 介绍了有关语法高亮的更多信息, 包括语法高亮的 shortcode. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:4","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"10 表格 通过在每个单元格之间添加竖线作为分隔线, 并在标题下添加一行破折号 (也由竖线分隔) 来创建表格. 注意, 竖线不需要垂直对齐. | Option | Description | | ------ | ----------- | | data | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | 呈现的输出效果如下: Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. 输出的 HTML 看起来像这样: \u003ctable\u003e \u003cthead\u003e \u003ctr\u003e \u003cth\u003eOption\u003c/th\u003e \u003cth\u003eDescription\u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd\u003edata\u003c/td\u003e \u003ctd\u003epath to data files to supply the data that will be passed into templates.\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003eengine\u003c/td\u003e \u003ctd\u003eengine to be used for processing templates. Handlebars is the default.\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003eext\u003c/td\u003e \u003ctd\u003eextension to be used for dest files.\u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/table\u003e 文本右对齐或居中对齐 在任何标题下方的破折号右侧添加冒号将使该列的文本右对齐. 在任何标题下方的破折号两边添加冒号将使该列的对齐文本居中. | Option | Description | |:------:| -----------:| | data | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | 呈现的输出效果如下: Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:10:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"11 链接 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"基本链接 \u003chttps://assemble.io\u003e \u003ccontact@revolunet.com\u003e [Assemble](https://assemble.io) 呈现的输出效果如下 (将鼠标悬停在链接上，没有提示): https://assemble.io contact@revolunet.com Assemble 输出的 HTML 看起来像这样: \u003ca href=\"https://assemble.io\"\u003ehttps://assemble.io\u003c/a\u003e \u003ca href=\"mailto:contact@revolunet.com\"\u003econtact@revolunet.com\u003c/a\u003e \u003ca href=\"https://assemble.io\"\u003eAssemble\u003c/a\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:1","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"添加一个标题 [Upstage](https://github.com/upstage/ \"Visit Upstage!\") 呈现的输出效果如下 (将鼠标悬停在链接上，会有一行提示): Upstage 输出的 HTML 看起来像这样: \u003ca href=\"https://github.com/upstage/\" title=\"Visit Upstage!\"\u003eUpstage\u003c/a\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:2","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"定位标记 定位标记使你可以跳至同一页面上的指定锚点. 例如, 每个章节: ## Table of Contents * [Chapter 1](#chapter-1) * [Chapter 2](#chapter-2) * [Chapter 3](#chapter-3) 将跳转到这些部分: ## Chapter 1 \u003ca id=\"chapter-1\"\u003e\u003c/a\u003e Content for chapter one. ## Chapter 2 \u003ca id=\"chapter-2\"\u003e\u003c/a\u003e Content for chapter one. ## Chapter 3 \u003ca id=\"chapter-3\"\u003e\u003c/a\u003e Content for chapter one. 注意 定位标记的位置几乎是任意的. 因为它们并不引人注目, 所以它们通常被放在同一行了. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:3","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"12 脚注 脚注使你可以添加注释和参考, 而不会使文档正文混乱. 当你创建脚注时, 会在添加脚注引用的位置出现带有链接的上标编号. 读者可以单击链接以跳至页面底部的脚注内容. 要创建脚注引用, 请在方括号中添加插入符号和标识符 ([^1]). 标识符可以是数字或单词, 但不能包含空格或制表符. 标识符仅将脚注引用与脚注本身相关联 - 在脚注输出中, 脚注按顺序编号. 在中括号内使用插入符号和数字以及用冒号和文本来添加脚注内容 ([^1]：这是一段脚注). 你不一定要在文档末尾添加脚注. 可以将它们放在除列表, 引用和表格等元素之外的任何位置. 这是一个数字脚注[^1]. 这是一个带标签的脚注[^label] [^1]: 这是一个数字脚注 [^label]: 这是一个带标签的脚注 这是一个数字脚注1. 这是一个带标签的脚注2 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:12:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"13 图片 图片的语法与链接相似, 但包含一个在前面的感叹号. ![Minion](https://octodex.github.com/images/minion.png) 或者: ![Alt text](https://octodex.github.com/images/stormtroopocat.jpg \"The Stormtroopocat\") The StormtroopocatAlt text \" The Stormtroopocat 像链接一样, 图片也具有脚注样式的语法: ![Alt text][id] The DojocatAlt text \" The Dojocat 稍后在文档中提供参考内容, 用来定义 URL 的位置: [id]: https://octodex.github.com/images/dojocat.jpg \"The Dojocat\" 技巧 LoveIt 主题提供了一个包含更多功能的 图片的 shortcode. 这是一个数字脚注 ↩︎ 这是一个带标签的脚注 ↩︎ ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:13:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["转载","kubernetes","基础教程"],"content":"写在前面 前面的kubernets系列文章介绍了通过命令行和yaml文件的方式对kubernetes中资源的管理，命令行和yaml文件方式管理对于管理员来说无疑是利器，而对于普通大众来说，图形管理界面需求则为更迫切的方式，本章介绍kubernetes社区提供的一个图形界面管理工具：kubernetes-dashboard，通过一个WebUI管理kubernetes的资源。 1. kubernetes-dashboard简介 kubernetes中管理集群中资源的方式通常有四种：命令行、YAML、API和图形界面，四种不同的方式适用于不同的人群和场景，对比如下： 命令行kubectl，kubectl提供了命令行管理kubernetes资源 优点：使用方便、便捷、快速管理集群资源 缺点：功能相对有限，部分操作无法支持，有一定的门槛 YAML资源定义，kubernetes中最终转换形式，推荐使用方式 优点：功能齐备，能够定义kubernetes的所有对象和资源 缺点：门槛较高，需要具备专业技术能力，使用排障难度大 API管理接入，提供各种编程语言SDK接口，方便各种编程语言应用程序接入 优点：适配各种编程语言，如Java，Go，Python，C等，方便开发kubernetes 缺点：门槛较高，适用于开发人员 图形kubernetes-dashboard，提供图形化管理界面，能够利用metric-server实现node和pod的监控 优点：使用简单，便捷，适合大众。 缺点：功能相对简单，功能原生，适用于demo 2. **kubernetes-**dashboard安装 社区提供了kubernetes-dashbaord的YAML资源定义文件，直接下载YAML文件安装即可实现dashboard的安装接入，需要准备条件如下： 已安装好的kubernetes集群，本文环境为1.15.3 metric-server监控，node监控和pod监控视图需依赖于监控系统 RBAC认证授权，设置好账户并授予访问权限 1、下载kubernetes-dashboard安装文件并应用YAML资源定义 [root@node-1 ~]# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml namespace/kubernetes-dashboard created serviceaccount/kubernetes-dashboard created service/kubernetes-dashboard created secret/kubernetes-dashboard-certs created secret/kubernetes-dashboard-csrf created secret/kubernetes-dashboard-key-holder created configmap/kubernetes-dashboard-settings created role.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard configured rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard unchanged deployment.apps/kubernetes-dashboard created service/dashboard-metrics-scraper created deployment.apps/dashboard-metrics-scraper created 安装文件中定义了dashboard相关的资源，可以查阅YAML文件，资源包含有： kubernetes-dashboard命名空间 ServiceAccount访问用户 Service服务访问应用，默认为ClusterIP Secrets，存放有kubernetes-dashboard-certs，kubernetes-dashboard-csrf，kubernetes-dashboard-key-holder证书 ConfigMap配置文件 RBAC认证授权，包含有Role，ClusterRole，RoleBinding，ClusterRoleBinding Deployments应用，kubernetes-dashboard核心镜像，还有一个和监控集成的dashboard-metrics-scraper 2、校验资源的安装情况，kubernetes-dashbaord的资源都安装在kubernetes-dashboard命名空间下,包含有Deployments，Services，Secrets，ConfigMap等 3、kubernetes-dashbaord安装完毕后，kubernetes-dashboard默认service的类型为ClusterIP，为了从外部访问控制面板，开放为NodePort类型 4、此时通过https协议访问30433端口即可打开dashboard的控制台，为了保护数据安全性，集群默认开启了RBAC认证授权，需要授予权限的用户才可以访问到kubernetes集群，因此需要授权用户访问集群，集群中已定有了cluster-admin的角色和相关的Role，ClusterRole和ClusterRoleBinding角色，定义ServiceAccount将其关联即可，如下: [root@node-1 ~]# cat dashboard-rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: happylau namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: happylau roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: happylau namespace: kubernetes-dashboard 5、应用RBAC规则，创建一个happylau的用户，并和cluster-admin的角色关联 [root@node-1 ~]# kubectl apply -f dashboard-rbac.yaml serviceaccount/happylau created clusterrolebinding.rbac.authorization.k8s.io/happylau created [root@node-1 ~]# kubectl get serviceaccounts -n kubernetes-dashboard NAME SECRETS AGE default 1 114m happylau 1 8s kubernetes-dashboard 1 114m 6、此时通过kubernetes-dashboard-csrf服务会自动创建一个和用户名关联的Secrets，通过token字段来登陆，token通过base64加密，解密后即可登录，如下演示登录的过程 自此，kubernetes-dashboard安装完毕，通过RBAC认证授权特定用户访问集群权限，接下来一起探索dashboard带来的魔力吧。 3. 探索kubernetes-dashboard kubernetes-dashboard图形工具能提供以下功能： 查看kubernetes中的资源对象，包含kubernetes中各种资源 Cluster 集群级别的资源，如命名空间，节点，PV，StorageClass，ClusterRole等 Workloads，不同类型的工作负载，包含Deployments，StatefulSets，DaemonSets，Jobs等 Discovery and LoadBalancing，服务发现和负载均衡，包含service和ingress ConfigMap and Storage，包含ConfigMap，Secrets和PVC Costom Resource Definition，自定义资源定义 kubernetes资源监控，调用metric-server监控系统，实现Cluster集群，Workloads应用负载，存储等资源的监控 管理资源对象，包含创建，编辑yaml，删除负载等，主要是以Deployments等应用为主的管理 1、查看集群整体概览资源，可以看到整体集群，应用负载，Pod资源的资源使用情况 2、Cluster集群资源管理，包含还有Nodes，Namespace，StorageClass等，提供在线编辑yaml方式 3、查看应用工作负载Wor","date":"2019-08-04","objectID":"/19-dashboard%E4%BD%BFk8s%E6%99%AE%E5%8F%8A%E5%A4%A7%E4%BC%97/:0:0","tags":["kubernetes"],"title":"19 Dashboard使k8s普及大众","uri":"/19-dashboard%E4%BD%BFk8s%E6%99%AE%E5%8F%8A%E5%A4%A7%E4%BC%97/"},{"categories":["转载","kubernetes","基础教程"],"content":"写在前面 上一篇文章中kubernetes系列教程（七）深入玩转pod调度介绍了kubernetes中Pod的调度机制，通过实战演练介绍Pod调度到node的几种方法：1. 通过nodeName固定选择调度，2. 通过nodeSelector定向选择调度，3. 通过node Affinity亲和力调度，接下来介绍kubernetes系列教程pod的健康检查机制。 1. 健康检查 ","date":"2019-08-04","objectID":"/08-pod%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E6%9C%BA%E5%88%B6/:0:0","tags":["kubernetes"],"title":"08 Pod健康检查机制","uri":"/08-pod%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E6%9C%BA%E5%88%B6/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.1 健康检查概述 应用在运行过程中难免会出现错误，如程序异常，软件异常，硬件故障，网络故障等，kubernetes提供Health Check健康检查机制，当发现应用异常时会自动重启容器，将应用从service服务中剔除，保障应用的高可用性。k8s定义了三种探针Probe： readiness probes 准备就绪检查，通过readiness是否准备接受流量，准备完毕加入到endpoint，否则剔除 liveness probes 在线检查机制，检查应用是否可用，如死锁，无法响应，异常时会自动重启容器 startup probes 启动检查机制，应用一些启动缓慢的业务，避免业务长时间启动而被前面的探针kill掉 每种探测机制支持三种健康检查方法，分别是命令行exec，httpGet和tcpSocket，其中exec通用性最强，适用与大部分场景，tcpSocket适用于TCP业务，httpGet适用于web业务。 exec 提供命令或shell的检测，在容器中执行命令检查，返回码为0健康，非0异常 httpGet http协议探测，在容器中发送http请求，根据http返回码判断业务健康情况 tcpSocket tcp协议探测，向容器发送tcp建立连接，能建立则说明正常 每种探测方法能支持几个相同的检查参数，用于设置控制检查时间： initialDelaySeconds 初始第一次探测间隔，用于应用启动的时间，防止应用还没启动而健康检查失败 periodSeconds 检查间隔，多久执行probe检查，默认为10s timeoutSeconds 检查超时时长，探测应用timeout后为失败 successThreshold 成功探测阈值，表示探测多少次为健康正常，默认探测1次 ","date":"2019-08-04","objectID":"/08-pod%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E6%9C%BA%E5%88%B6/:1:0","tags":["kubernetes"],"title":"08 Pod健康检查机制","uri":"/08-pod%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E6%9C%BA%E5%88%B6/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2 exec命令行健康检查 许多应用程序运行过程中无法检测到内部故障，如死锁，出现故障时通过重启业务可以恢复，kubernetes提供liveness在线健康检查机制，我们以exec为例，创建一个容器启动过程中创建一个文件/tmp/liveness-probe.log，10s后将其删除，定义liveness健康检查机制在容器中执行命令ls -l /tmp/liveness-probe.log，通过文件的返回码判断健康状态，如果返回码非0，暂停20s后kubelet会自动将该容器重启。 定义一个容器，启动时创建一个文件，健康检查时ls -l /tmp/liveness-probe.log返回码为0，健康检查正常，10s后将其删除，返回码为非0，健康检查异常 [root@node-1 demo]# cat centos-exec-liveness-probe.yaml apiVersion: v1 kind: Pod metadata: name: exec-liveness-probe annotations: kubernetes.io/description: \"exec-liveness-probe\" spec: containers: - name: exec-liveness-probe image: centos:latest imagePullPolicy: IfNotPresent args: #容器启动命令，生命周期为30s - /bin/sh - -c - touch /tmp/liveness-probe.log \u0026\u0026 sleep 10 \u0026\u0026 rm -f /tmp/liveness-probe.log \u0026\u0026 sleep 20 livenessProbe: exec: #健康检查机制，通过ls -l /tmp/liveness-probe.log返回码判断容器的健康状态 command: - ls - l - /tmp/liveness-probe.log initialDelaySeconds: 1 periodSeconds: 5 timeoutSeconds: 1 应用配置生成容器 [root@node-1 demo]# kubectl apply -f centos-exec-liveness-probe.yaml pod/exec-liveness-probe created 查看容器的event日志，容器启动后，10s以内容器状态正常，11s开始执行liveness健康检查，检查异常，触发容器重启 [root@node-1 demo]# kubectl describe pods exec-liveness-probe | tail Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 28s default-scheduler Successfully assigned default/exec-liveness-probe to node-3 Normal Pulled 27s kubelet, node-3 Container image \"centos:latest\" already present on machine Normal Created 27s kubelet, node-3 Created container exec-liveness-probe Normal Started 27s kubelet, node-3 Started container exec-liveness-probe #容器已启动 Warning Unhealthy 20s (x2 over 25s) kubelet, node-3 Liveness probe failed: /tmp/liveness-probe.log ls: cannot access l: No such file or directory #执行健康检查，检查异常 Warning Unhealthy 15s kubelet, node-3 Liveness probe failed: ls: cannot access l: No such file or directory ls: cannot access /tmp/liveness-probe.log: No such file or directory Normal Killing 15s kubelet, node-3 Container exec-liveness-probe failed liveness probe, will be restarted #重启容器 查看容器重启次数，容器不停的执行，重启次数会响应增加，可以看到RESTARTS的次数在持续增加 [root@node-1 demo]# kubectl get pods exec-liveness-probe NAME READY STATUS RESTARTS AGE exec-liveness-probe 1/1 Running 6 5m19s ","date":"2019-08-04","objectID":"/08-pod%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E6%9C%BA%E5%88%B6/:2:0","tags":["kubernetes"],"title":"08 Pod健康检查机制","uri":"/08-pod%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E6%9C%BA%E5%88%B6/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.3 httpGet健康检查 httpGet probe主要主要用于web场景，通过向容器发送http请求，根据返回码判断容器的健康状态，返回码小于4xx即表示健康，如下定义一个nginx应用，通过探测http://:port/index.html的方式判断健康状态 [root@node-1 demo]# cat nginx-httpGet-liveness-readiness.yaml apiVersion: v1 kind: Pod metadata: name: nginx-httpget-livess-readiness-probe annotations: kubernetes.io/description: \"nginx-httpGet-livess-readiness-probe\" spec: containers: - name: nginx-httpget-livess-readiness-probe image: nginx:latest ports: - name: http-80-port protocol: TCP containerPort: 80 livenessProbe: #健康检查机制，通过httpGet实现实现检查 httpGet: port: 80 scheme: HTTP path: /index.html initialDelaySeconds: 3 periodSeconds: 10 timeoutSeconds: 3 生成pod并查看健康状态 [root@node-1 demo]# kubectl apply -f nginx-httpGet-liveness-readiness.yaml pod/nginx-httpget-livess-readiness-probe created [root@node-1 demo]# kubectl get pods nginx-httpget-livess-readiness-probe NAME READY STATUS RESTARTS AGE nginx-httpget-livess-readiness-probe 1/1 Running 0 6s 模拟故障，将pod中的path文件所属文件删除，此时发送http请求时会健康检查异常，会触发容器自动重启 查询pod所属的节点 [root@node-1 demo]# kubectl get pods nginx-httpget-livess-readiness-probe -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-httpget-livess-readiness-probe 1/1 Running 1 3m9s 10.244.2.19 node-3 \u003cnone\u003e \u003cnone\u003e 登录到pod中将文件删除 [root@node-1 demo]# kubectl exec -it nginx-httpget-livess-readiness-probe /bin/bash root@nginx-httpget-livess-readiness-probe:/# ls -l /usr/share/nginx/html/index.html -rw-r--r-- 1 root root 612 Sep 24 14:49 /usr/share/nginx/html/index.html root@nginx-httpget-livess-readiness-probe:/# rm -f /usr/share/nginx/html/index.html 再次查看pod的列表，此时会RESTART的次数会增加1，表示重启重启过一次，AGE则多久前重启的时间 [root@node-1 demo]# kubectl get pods nginx-httpget-livess-readiness-probe NAME READY STATUS RESTARTS AGE nginx-httpget-livess-readiness-probe 1/1 Running 1 4m22s 查看pod的详情，观察容器重启的情况，通过Liveness 检查容器出现404错误，触发重启。 [root@node-1 demo]# kubectl describe pods nginx-httpget-livess-readiness-probe | tail Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 5m45s default-scheduler Successfully assigned default/nginx-httpget-livess-readiness-probe to node-3 Normal Pulling 3m29s (x2 over 5m45s) kubelet, node-3 Pulling image \"nginx:latest\" Warning Unhealthy 3m29s (x3 over 3m49s) kubelet, node-3 Liveness probe failed: HTTP probe failed with statuscode: 404 Normal Killing 3m29s kubelet, node-3 Container nginx-httpget-livess-readiness-probe failed liveness probe, will be restarted Normal Pulled 3m25s (x2 over 5m41s) kubelet, node-3 Successfully pulled image \"nginx:latest\" Normal Created 3m25s (x2 over 5m40s) kubelet, node-3 Created container nginx-httpget-livess-readiness-probe Normal Started 3m25s (x2 over 5m40s) kubelet, node-3 Started container nginx-httpget-livess-readiness-probe ","date":"2019-08-04","objectID":"/08-pod%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E6%9C%BA%E5%88%B6/:3:0","tags":["kubernetes"],"title":"08 Pod健康检查机制","uri":"/08-pod%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E6%9C%BA%E5%88%B6/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.4 tcpSocket健康检查 tcpsocket健康检查适用于TCP业务，通过向指定容器建立一个tcp连接，可以建立连接则健康检查正常，否则健康检查异常，依旧以nignx为例使用tcp健康检查机制，探测80端口的连通性 [root@node-1 demo]# cat nginx-tcp-liveness.yaml apiVersion: v1 kind: Pod metadata: name: nginx-tcp-liveness-probe annotations: kubernetes.io/description: \"nginx-tcp-liveness-probe\" spec: containers: - name: nginx-tcp-liveness-probe image: nginx:latest ports: - name: http-80-port protocol: TCP containerPort: 80 livenessProbe: #健康检查为tcpSocket，探测TCP 80端口 tcpSocket: port: 80 initialDelaySeconds: 3 periodSeconds: 10 timeoutSeconds: 3 应用配置创建容器 [root@node-1 demo]# kubectl apply -f nginx-tcp-liveness.yaml pod/nginx-tcp-liveness-probe created [root@node-1 demo]# kubectl get pods nginx-tcp-liveness-probe NAME READY STATUS RESTARTS AGE nginx-tcp-liveness-probe 1/1 Running 0 6s 模拟故障，获取pod所属节点，登录到pod中，安装查看进程工具htop 获取pod所在node [root@node-1 demo]# kubectl get pods nginx-tcp-liveness-probe -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-tcp-liveness-probe 1/1 Running 0 99s 10.244.2.20 node-3 \u003cnone\u003e \u003cnone\u003e 登录到pod中 [root@node-1 demo]# kubectl exec -it nginx-httpget-livess-readiness-probe /bin/bash #执行apt-get update更新和apt-get install htop安装工具 root@nginx-httpget-livess-readiness-probe:/# apt-get update Get:1 http://cdn-fastly.deb.debian.org/debian buster InRelease [122 kB] Get:2 http://security-cdn.debian.org/debian-security buster/updates InRelease [39.1 kB] Get:3 http://cdn-fastly.deb.debian.org/debian buster-updates InRelease [49.3 kB] Get:4 http://security-cdn.debian.org/debian-security buster/updates/main amd64 Packages [95.7 kB] Get:5 http://cdn-fastly.deb.debian.org/debian buster/main amd64 Packages [7899 kB] Get:6 http://cdn-fastly.deb.debian.org/debian buster-updates/main amd64 Packages [5792 B] Fetched 8210 kB in 3s (3094 kB/s) Reading package lists... Done root@nginx-httpget-livess-readiness-probe:/# apt-get install htop Reading package lists... Done Building dependency tree Reading state information... Done Suggested packages: lsof strace The following NEW packages will be installed: htop 0 upgraded, 1 newly installed, 0 to remove and 5 not upgraded. Need to get 92.8 kB of archives. After this operation, 230 kB of additional disk space will be used. Get:1 http://cdn-fastly.deb.debian.org/debian buster/main amd64 htop amd64 2.2.0-1+b1 [92.8 kB] Fetched 92.8 kB in 0s (221 kB/s) debconf: delaying package configuration, since apt-utils is not installed Selecting previously unselected package htop. (Reading database ... 7203 files and directories currently installed.) Preparing to unpack .../htop_2.2.0-1+b1_amd64.deb ... Unpacking htop (2.2.0-1+b1) ... Setting up htop (2.2.0-1+b1) ... 运行htop查看进程，容器进程通常为1 kill掉进程观察容器状态，观察RESTART次数重启次数增加 root@nginx-httpget-livess-readiness-probe:/# kill 1 root@nginx-httpget-livess-readiness-probe:/# command terminated with exit code 137 查看pod情况 [root@node-1 demo]# kubectl get pods nginx-tcp-liveness-probe NAME READY STATUS RESTARTS AGE nginx-tcp-liveness-probe 1/1 Running 1 13m 查看容器详情，发现容器有重启的记录 [root@node-1 demo]# kubectl describe pods nginx-tcp-liveness-probe | tail Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 14m default-scheduler Successfully assigned default/nginx-tcp-liveness-probe to node-3 Normal Pulling 44s (x2 over 14m) kubelet, node-3 Pulling image \"nginx:latest\" Normal Pulled 40s (x2 over 14m) kubelet, node-3 Successfully pulled image \"nginx:latest\" Normal Created 40s (x2 over 14m) kubelet, node-3 Created container nginx-tcp-liveness-probe Normal Started 40s (x2 over 14m) kubelet, node-3 Started container nginx-tcp-liveness-probe ","date":"2019-08-04","objectID":"/08-pod%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E6%9C%BA%E5%88%B6/:4:0","tags":["kubernetes"],"title":"08 Pod健康检查机制","uri":"/08-pod%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E6%9C%BA%E5%88%B6/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.5 readiness健康就绪 就绪检查用于应用接入到service的场景，用于判断应用是否已经就绪完毕，即是否可以接受外部转发的流量，健康检查正常则将pod加入到service的endpoints中，健康检查异常则从service的endpoints中删除，避免影响业务的访问。 创建一个pod，使用httpGet的健康检查机制，定义readiness就绪检查探针检查路径/test.html [root@node-1 demo]# cat httpget-liveness-readiness-probe.yaml apiVersion: v1 kind: Pod metadata: name: nginx-tcp-liveness-probe annotations: kubernetes.io/description: \"nginx-tcp-liveness-probe\" labels: #需要定义labels，后面定义的service需要调用 app: nginx spec: containers: - name: nginx-tcp-liveness-probe image: nginx:latest ports: - name: http-80-port protocol: TCP containerPort: 80 livenessProbe: #存活检查探针 httpGet: port: 80 path: /index.html scheme: HTTP initialDelaySeconds: 3 periodSeconds: 10 timeoutSeconds: 3 readinessProbe: #就绪检查探针 httpGet: port: 80 path: /test.html scheme: HTTP initialDelaySeconds: 3 periodSeconds: 10 timeoutSeconds: 3 定义一个service，将上述的pod加入到service中，注意使用上述定义的labels，app=nginx [root@node-1 demo]# cat nginx-service.yaml apiVersion: v1 kind: Service metadata: labels: app: nginx name: nginx-service spec: ports: - name: http port: 80 protocol: TCP targetPort: 80 selector: app: nginx type: ClusterIP 生成配置 [root@node-1 demo]# kubectl apply -f httpget-liveness-readiness-probe.yaml pod/nginx-tcp-liveness-probe created [root@node-1 demo]# kubectl apply -f nginx-service.yaml service/nginx-service created 此时pod状态正常，此时readiness健康检查异常 [root@node-1 ~]# kubectl get pods nginx-httpget-livess-readiness-probe NAME READY STATUS RESTARTS AGE nginx-httpget-livess-readiness-probe 1/1 Running 2 153m #readiness健康检查异常，404报错（最后一行） [root@node-1 demo]# kubectl describe pods nginx-tcp-liveness-probe | tail node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m6s default-scheduler Successfully assigned default/nginx-tcp-liveness-probe to node-3 Normal Pulling 2m5s kubelet, node-3 Pulling image \"nginx:latest\" Normal Pulled 2m1s kubelet, node-3 Successfully pulled image \"nginx:latest\" Normal Created 2m1s kubelet, node-3 Created container nginx-tcp-liveness-probe Normal Started 2m1s kubelet, node-3 Started container nginx-tcp-liveness-probe Warning Unhealthy 2s (x12 over 112s) kubelet, node-3 Readiness probe failed: HTTP probe failed with statuscode: 404 查看services的endpoints，发现此时endpoints为空,因为readiness就绪检查异常，kubelet认为此时pod并未就绪，因此并未将其加入到endpoints中。 [root@node-1 ~]# kubectl describe services nginx-service Name: nginx-service Namespace: default Labels: app=nginx Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"nginx\"},\"name\":\"nginx-service\",\"namespace\":\"default\"},\"s... Selector: app=nginx Type: ClusterIP IP: 10.110.54.40 Port: http 80/TCP TargetPort: 80/TCP Endpoints: \u003cnone\u003e #Endpoints对象为空 Session Affinity: None Events: \u003cnone\u003e #endpoints状态 [root@node-1 demo]# kubectl describe endpoints nginx-service Name: nginx-service Namespace: default Labels: app=nginx Annotations: endpoints.kubernetes.io/last-change-trigger-time: 2019-09-30T14:27:37Z Subsets: Addresses: \u003cnone\u003e NotReadyAddresses: 10.244.2.22 #pod处于NotReady状态 Ports: Name Port Protocol ---- ---- -------- http 80 TCP Events: \u003cnone\u003e 进入到pod中手动创建网站文件，使readiness健康检查正常 [root@node-1 ~]# kubectl exec -it nginx-httpget-livess-readiness-probe /bin/bash root@nginx-httpget-livess-readiness-probe:/# echo \"readiness probe demo\" \u003e/usr/share/nginx/html/test.html 此时readiness健康检查正常，kubelet检测到pod就绪会将其加入到endpoints中 健康检查正常 [root@node-1 demo]# curl http://10.244.2.22/test.html 查看endpoints情况 readines[root@node-1 demo]# kubectl describe endpoints nginx-service Name: nginx-service Namespace: default Labels: app=nginx Annotations: endpoints.kubernetes.io/last-change-trigger-time: 2019-09-30T14:33:01Z Subsets: Addresses: 10.244.2.22 #就绪地址，已从NotReady中提出，加入到正常的Address列表中 NotReadyAddresses: \u003cnone\u003e Ports: Name Port Protocol ---- ---- -------- http 80 TCP 查看service状态 [root@node-1 demo]# kubectl describe services nginx-service Name: nginx-service Namespace: default L","date":"2019-08-04","objectID":"/08-pod%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E6%9C%BA%E5%88%B6/:5:0","tags":["kubernetes"],"title":"08 Pod健康检查机制","uri":"/08-pod%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E6%9C%BA%E5%88%B6/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.6 TKE设置健康检查 TKE中可以设定应用的健康检查机制，健康检查机制包含在不同的Workload中，可以通过模板生成健康监测机制，定义过程中可以选择高级选项，默认健康检查机制是关闭状态，包含前面介绍的两种探针：存活探针livenessProbe和就绪探针readinessProbe，根据需要分别开启 TKE健康检查 开启探针之后进入设置健康检查，支持上述介绍的三种方法：执行命令检查、TCP端口检查，HTTP请求检查 TKE健康检查方法 选择不同的检查方法填写不同的参数即可，如启动间隔，检查间隔，响应超时，等参数，以HTTP请求检查方法为例： TKE http健康检查方法 设置完成后创建workload时候会自动生成yaml文件，以刚创建的deployment为例，生成健康检查yaml文件内容如下： apiVersion: apps/v1beta2 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"1\" description: tke-health-check-demo creationTimestamp: \"2019-09-30T12:28:42Z\" generation: 1 labels: k8s-app: tke-health-check-demo qcloud-app: tke-health-check-demo name: tke-health-check-demo namespace: default resourceVersion: \"2060365354\" selfLink: /apis/apps/v1beta2/namespaces/default/deployments/tke-health-check-demo uid: d6cf1f25-e37d-11e9-87fd-567eb17a3218 spec: minReadySeconds: 10 progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: tke-health-check-demo qcloud-app: tke-health-check-demo strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: tke-health-check-demo qcloud-app: tke-health-check-demo spec: containers: - image: nginx:latest imagePullPolicy: Always livenessProbe: #通过模板生成的健康检查机制 failureThreshold: 1 httpGet: path: / port: 80 scheme: HTTP periodSeconds: 3 successThreshold: 1 timeoutSeconds: 2 name: tke-health-check-demo resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false procMount: Default terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey - name: tencenthubkey restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 写在最后 本章介绍kubernetes中健康检查两种Probe：livenessProbe和readinessProbe，livenessProbe主要用于存活检查，检查容器内部运行状态，readiness主要用于就绪检查，是否可以接受流量，通常需要和service的endpoints结合，当就绪准备妥当时加入到endpoints中，当就绪异常时从endpoints中删除，从而实现了services的健康检查和服务探测机制。对于Probe机制提供了三种检测的方法，分别适用于不同的场景：1. exec命令行，通过命令或shell实现健康检查，2. tcpSocket通过TCP协议探测端口，建立tcp连接，3. httpGet通过建立http请求探测，读者可多实操掌握其用法。 附录 健康检查：https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ TKE健康检查设置方法：https://cloud.tencent.com/document/product/457/32815 『 转载 』该文章来源于网络，侵删。 ","date":"2019-08-04","objectID":"/08-pod%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E6%9C%BA%E5%88%B6/:6:0","tags":["kubernetes"],"title":"08 Pod健康检查机制","uri":"/08-pod%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E6%9C%BA%E5%88%B6/"},{"categories":["转载","kubernetes","基础教程"],"content":"写在前面 上一篇文章中kubernetes系列教程（八）Pod健康检查机制介绍了kubernetes中Pod健康检查机制，通过实战介绍了kubernetes中两种健康检查探针：livenessProbe存活检查，readinessProbe就绪检查，存活检查用于检查应用的可用性，就绪检查用于检查容器是否准备接受流量，健康检查包含三种探测的方法：exec命令行探测，tcpSocket端口检测，httpGet请求检测，分别适用于不同场景下的健康检查。接下来介绍kubernetes系列教程pod的存储管理。 kubernetes存储管理按照发展的历程，涉及到有Volume，PV(Persistent Volume)和PVC(PersistentVolumeClaims),和StorageClass，Volume是最早提出的存储卷，主要解决容器和数据存储的依赖关系，抽象底层驱动以支持不同的存储类型；使用Volume需要了解底层存储细节，因此提出了PV，Persistent Volume是由k8s管理员定义的存储单元，应用端使用PersistentVolumeClaims声明去调用PV存储，进一步抽象了底层存储；随着PV数量的增加，管理员需要不停的定义PV的数量，衍生了通过StorageClass动态生成PV，StorageClass通过PVC中声明存储的容量，会调用底层的提供商生成PV。本文介绍Volume的使用，下篇文章介绍PV，PVC和StorageClass。 Volume 存储卷，独立于容器，后端和不同的存储驱动对接 PV Persistent Volume持久化存储卷，和node类似，是一种集群资源，由管理员定义，对接不同的存储 PVC PersistentVolumeClaims持久化存储声明，和pod类似，作为PV的使用者 StorageClass 动态存储类型，分为静态和动态两种类型，通过在PVC中定义存储类型，自动创建所需PV 1. kubernetes存储管理 ","date":"2019-08-04","objectID":"/09-%E5%88%9D%E8%AF%86pod%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/:0:0","tags":["kubernetes"],"title":"09 初识Pod存储管理","uri":"/09-%E5%88%9D%E8%AF%86pod%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.1 存储概述 kubernetes容器中的数据是临时的，即当重启重启或crash后容器的数据将会丢失，此外容器之间有共享存储的需求，所以kubernetes中提供了volume存储的抽象，volume后端能够支持多种不同的plugin驱动，通过.spec.volumes中定义一个存储，然后在容器中.spec.containers.volumeMounts调用，最终在容器内部以目录的形式呈现。 kubernetes内置能支持多种不同的驱动类型，大体上可以分为四种类型：1. 公/私有云驱动接口，如awsElasticBlockStore实现与aws EBS集成，2. 开源存储驱动接口，如ceph rbd，实现与ceph rb块存储对接，3. 本地临时存储，如hostPath，4. kubernetes对象API驱动接口，实现其他对象调用，如configmap，每种存储支持不同的驱动，如下介绍： 公/私有云驱动接口 awsElasticBlockStore AWS的EBS云盘 azureDisk 微软azure云盘 azureFile 微软NAS存储 gcePersistentDisk google云盘 cinder openstack cinder云盘 vsphereVolume VMware的VMFS存储 scaleIO EMC分布式存储 开源存储驱动接口 ceph rbd ceph块存储 cephfs ceph文件存储 glusterfs glusterfs存储 nfs nfs文件 iscsi flexvolume csi 社区标准化驱动 flocker 本地临时存储 hostpath 宿主机文件 emptyDir 临时目录 kubernetes对象API驱动接口 configMap 调用configmap对象，注入配置文件 secrets 调用secrets对象，注入秘文配置文件 persistentVolumeClaim 通过pvc调用存储 downloadAPI 下载URL projected ","date":"2019-08-04","objectID":"/09-%E5%88%9D%E8%AF%86pod%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/:1:0","tags":["kubernetes"],"title":"09 初识Pod存储管理","uri":"/09-%E5%88%9D%E8%AF%86pod%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2 emptyDir临时存储 emptyDir是一种临时存储，pod创建的时候会在node节点上为容器申请一个临时的目录，跟随容器的生命周期，如容器删除，emptyDir定义的临时存储空间也会随之删除，容器发生意外crash则不受影响，同时如果容器发生了迁移，其上的数据也会丢失，emptyDir一般用于测试，或者缓存场景。 定义一个emptyDir存储大小为1G，将其挂载到redis的/data目录中 [root@node-1 happylau]# cat emptydir-redis.yaml apiVersion: v1 kind: Pod metadata: name: emptydir-redis labels: volume: emptydir annotations: kubernetes.io/storage: emptyDir spec: containers: - name: emptydir-redis image: redis:latest imagePullPolicy: IfNotPresent ports: - name: redis-6379-port protocol: TCP containerPort: 6379 volumeMounts: #将定义的驱动emptydir-redis挂载到容器的/data目录，通过名字方式关联 - name: emptydir-redis mountPath: /data volumes: #定义一个存储，驱动类型为emptyDir，大小1G - name: emptydir-redis emptyDir: sizeLimit: 1Gi 生成redis pod,并查看describe pod的详情信息 [root@node-1 happylau]# kubectl apply -f emptydir-redis.yaml pod/emptydir-redis created 执行kubectl describe pods emptydir-redis查看容器的存储挂载信息 Containers: emptydir-redis: Container ID: docker://dddd9f3d0e395d784c08b712631d2b0c259bfdb30b0c655a0fc8021492f1ecf9 Image: redis:latest Image ID: docker-pullable://redis@sha256:cb379e1a076fcd3d3f09e10d7b47ca631fb98fb33149ab559fa02c1b11436345 Port: 6379/TCP Host Port: 0/TCP State: Running Started: Tue, 01 Oct 2019 11:04:30 +0800 Ready: True Restart Count: 0 Environment: \u003cnone\u003e Mounts: #挂载信息，将emptydir-redis挂载到/data目录，且是rw读写状态 /data from emptydir-redis (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-5qwmc (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: #定义了一个EmptyDir类型的存储，大小为1Gi emptydir-redis: Type: EmptyDir (a temporary directory that shares a pod's lifetime) Medium: SizeLimit: 1Gi default-token-5qwmc: Type: Secret (a volume populated by a Secret) SecretName: default-token-5qwmc Optional: false 向redis中写入数据 获取pod的ip地址 [root@node-1 happylau]# kubectl get pods emptydir-redis -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES emptydir-redis 1/1 Running 1 17m 10.244.1.27 node-2 \u003cnone\u003e \u003cnone\u003e 安装客户端redis-cli [root@node-1 ~]# yum install redis 向redis中写入两个key 10.244.1.27:6379\u003e set volume emptydir OK 10.244.1.27:6379\u003e set username happylauliu OK 10.244.1.27:6379\u003e get volume \"emptydir\" 10.244.1.27:6379\u003e get username \"happylauliu\" 登陆到pod中安装一个查看进程的工具procps，进程一般为1，如下图redis-server进程，可以直接kill，进程被kill后kubelet会自动将进程重启 登陆容器 [root@node-1 ~]# kubectl exec -it emptydir-redis /bin/bash 安装软件 root@emptydir-redis:/data# apt-get update ; apt-get install procps 可以通过top查看进程，进程号一般为1 root@emptydir-redis:/data# kill 1 pod异常重启后，再次登录redis并查看redis中的数据内容，发现数据没有丢失。 [root@node-1 ~]# redis-cli -h 10.244.1.27 10.244.1.27:6379\u003e get volume \"emptydir\" 10.244.1.27:6379\u003e get username \"happylauliu\" emptyDir实际是宿主机上创建的一个目录，将目录以bind mount的形势挂载到容器中，跟随容器的生命周期 [root@node-2 ~]# docker container list |grep redis e0e9a6b0ed77 01a52b3b5cd1 \"docker-entrypoint.s…\" 20 minutes ago Up 20 minutes k8s_emptydir-redis_emptydir-redis_default_4baadb25-1e62-4cf5-9724-821d04dcdd44_2 dfef32905fe5 k8s.gcr.io/pause:3.1 \"/pause\" 45 minutes ago Up 45 minutes k8s_POD_emptydir-redis_default_4baadb25-1e62-4cf5-9724-821d04dcdd44_0 docker container inspect e0e9a6b0ed77查看存储内容如下图： 查看目录的信息： [root@node-2 ~]# ls -l /var/lib/kubelet/pods/4baadb25-1e62-4cf5-9724-821d04dcdd44/volumes/kubernetes.io~empty-dir/emptydir-redis 总用量 4 -rw-r--r-- 1 polkitd input 156 10月 8 14:55 dump.rdb Pod删除后，volume的信息也随之删除 [root@node-1 ~]# kubectl delete pods emptydir-redis pod \"emptydir-redis\" deleted [root@node-1 ~]# ssh node-2 Last login: Tue Oct 8 15:15:41 2019 from 10.254.100.101 [root@node-2 ~]# ls -l /var/lib/kubelet/pods/4baadb25-1e62-4cf5-9724-821d04dcdd44/volumes/kubernetes.io~empty-dir/emptydir-redis ls: 无法访问/var/lib/kubelet/pods/4baadb25-1e62-4cf5-9724-821d04dcdd44/volumes/kubernetes.io~empty-dir/emptydir-redis: 没有那个文件或目录 小结：emptyDir是host上定义的一块临时存储，通过bind mount的形式挂载到容器中使用，容器重启数据会保留，容器删除则volume会随之删除。 ","date":"2019-08-04","objectID":"/09-%E5%88%9D%E8%AF%86pod%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/:2:0","tags":["kubernetes"],"title":"09 初识Pod存储管理","uri":"/09-%E5%88%9D%E8%AF%86pod%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.3 hostPath主机存储 与emptyDir类似，hostpath支持将node节点的目录或文件挂载到容器中使用，用于单机测试场景，此外适用于一些容器业务需要访问宿主机目录，如监控系统访问/proc和/sys目录，日志系统访问/var/lib/docker目录的一些场景。支持设置不同的type类型 Directory 本地存在的目录 DirectoryOrCreate 目录，如果不存在则创建，权限设置为755，属主和组设置和kubelet一致 File 本地存在文件 FileOrCreate 文件，如果不存在则创建，权限设置为644，属主和组设置和kubelet一致 Socket 本地已存在Socket文件 CharDevice 本地已存在的Char字符设备 BlockDevice 本地已存在的Block块设备 挂载本地/mnt目录到容器中 [root@node-1 happylau]# cat hostpath-demo.yaml apiVersion: v1 kind: Pod metadata: name: hostpath-demo labels: storage: hostpath annotations: kubernetes.io/storage: hostpath spec: containers: - name: nginx image: nginx:latest imagePullPolicy: IfNotPresent ports: - name: nginx-http-port protocol: TCP containerPort: 80 volumeMounts: #挂载到nginx的web站点目录下 - name: hostpath-demo mountPath: /usr/share/nginx/html volumes: #定一个hostPath本地的存储 - name: hostpath-demo hostPath: type: DirectoryOrCreate path: /mnt/data 生成nginx容器和web站点数据 [root@node-1 happylau]# kubectl apply -f hostpath-demo.yaml pod/hostpath-demo created 获取pod所在的node节点 [root@node-1 happylau]# kubectl get pods hostpath-demo -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES hostpath-demo 1/1 Running 0 31s 10.244.2.24 node-3 \u003cnone\u003e \u003cnone\u003e 生成web站点的数据 [root@node-1 happylau]# ssh node-3 Last login: Tue Oct 8 22:49:14 2019 from 10.254.100.101 [root@node-3 ~]# echo \"hostPath test page\" \u003e/mnt/data/index.html [root@node-3 ~]# curl http://10.244.2.24 hostPath test page 查看容器挂载存储的情况，以bind mount的形式挂载到容器中 模拟容器重启的的故障，容器重启后volume中的数据依保留 #docker层面kill掉进程 [root@node-3 ~]# docker container list |grep hostpath 39a7e21afebb f949e7d76d63 \"nginx -g 'daemon of…\" 11 minutes ago Up 11 minutes k8s_nginx_hostpath-demo_default_6da41e3d-8585-4997-bf90-255ca0948030_0 490f50108e41 k8s.gcr.io/pause:3.1 \"/pause\" 11 minutes ago Up 11 minutes k8s_POD_hostpath-demo_default_6da41e3d-8585-4997-bf90-255ca0948030_0 [root@node-3 ~]# docker container kill 39a7e21afebb 39a7e21afebb [root@node-3 ~]# exit 登出 #获取pod的地址，通过RESTART可知，容器重启过一次，测试数据依旧保留 [root@node-1 happylau]# kubectl get pods -o wide hostpath-demo NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES hostpath-demo 1/1 Running 1 12m 10.244.2.24 node-3 \u003cnone\u003e \u003cnone\u003e [root@node-1 happylau]# curl http://10.244.2.24 hostPath test page 小结：hostPath与emptyDir类似提供临时的存储，hostPath适用于一些容器需要访问宿主机目录或文件的场景，对于数据持久化而言都不是很好的实现方案。 ","date":"2019-08-04","objectID":"/09-%E5%88%9D%E8%AF%86pod%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/:3:0","tags":["kubernetes"],"title":"09 初识Pod存储管理","uri":"/09-%E5%88%9D%E8%AF%86pod%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.4 NFS存储对接 NFS是实现Network File System网络文件共享的NAS存储，kubernetes与NFS对接实现存储的共享，当容器删除不影响存储且可以实现跨机存储共享，本文以搭建一个NFS存储实现kubernetes对接。 准备一个nfs server共享，将node-1的/mnt/data目录共享 安装nfs服务 [root@node-1 ~]# yum install nfs-utils -y 配置nfs共享,提前创建好目录 [root@node-1 ~]# cat /etc/exports /mnt/data 10.254.100.0/24(rw) 重启并验证 [root@node-1 ~]# systemctl restart nfs [root@node-1 ~]# showmount -e node-1 Export list for node-1: /mnt/data 10.254.100.0/24 kubernets使用nfs的驱动对接 [root@node-1 happylau]# cat nfs-demo.yaml apiVersion: v1 kind: Pod metadata: name: nfs-demo labels: storage: nfs annotations: kubernetes.io/storage: nfs spec: containers: - name: nginx image: nginx:latest imagePullPolicy: IfNotPresent ports: - name: nginx-http-port protocol: TCP containerPort: 80 volumeMounts: #挂载到nfs的目录下 - name: nfs-demo mountPath: /usr/share/nginx/html volumes: #定义一个nfs驱动的存储 - name: nfs-demo nfs: server: 10.254.100.101 path: /mnt/data 生成pod，使用kubectl get pods的时候提示events中报错信息，挂载失败 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 40s default-scheduler Successfully assigned default/nfs-demo to node-2 Warning FailedMount 39s kubelet, node-2 MountVolume.SetUp failed for volume \"nfs-demo\" : mount failed: exit status 32 Mounting command: systemd-run Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/pods/78bf6a81-082d-4d6c-a163-75241bf21cde/volumes/kubernetes.io~nfs/nfs-demo --scope -- mount -t nfs 10.254.100.101:/mnt/data /var/lib/kubelet/pods/78bf6a81-082d-4d6c-a163-75241bf21cde/volumes/kubernetes.io~nfs/nfs-demo Output: Running scope as unit run-29843.scope. mount: wrong fs type, bad option, bad superblock on 10.254.100.101:/mnt/data, missing codepage or helper program, or other error (for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.\u003ctype\u003e helper program) 从上面的步骤中得知，宿主机挂载nfs的时候提示没有mount.nfs的命令，因此需要在所有的node节点上安装上nfs的客户端软件nfs-utils,以node-2为例，其他节点类似 [root@node-1 happylau]# ssh node-2 Last login: Tue Oct 8 15:22:04 2019 from 10.254.100.101 [root@node-2 ~]# yum install nfs-utils -y 测试站点数据 [root@node-1 happylau]# kubectl get pods nfs-demo -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nfs-demo 1/1 Running 0 4m41s 10.244.1.28 node-2 \u003cnone\u003e \u003cnone\u003e [root@node-1 happylau]# echo \"nfs test age\" \u003e/mnt/data/index.html [root@node-1 happylau]# curl http://10.244.1.28 nfs test age 删除pod后查看nfs共享的数据情况,原有数据依旧保留 [root@node-1 happylau]# kubectl delete pods nfs-demo pod \"nfs-demo\" deleted [root@node-1 happylau]# mount.nfs node-1:/mnt/data/ /media/ [root@node-1 happylau]# ls -l /media/ 总用量 4 -rw-r--r-- 1 root root 13 10月 8 23:26 index.html ","date":"2019-08-04","objectID":"/09-%E5%88%9D%E8%AF%86pod%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/:4:0","tags":["kubernetes"],"title":"09 初识Pod存储管理","uri":"/09-%E5%88%9D%E8%AF%86pod%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.5 TKE使用volume存储 TKE支持在创建Workload时如Deployments，DaemonSets，StatefulSets等指定存储卷，支持临时目录emptyDir，主机路径hostPath，nfs盘，pvc，云硬盘，configmap，secrets，此处以腾讯云CFS为例（提前在CFS中创建好存储，确保CFS和容器宿主机在同一个VPC网络内）。 创建存储卷，使用NFS挂载腾讯云CFS存储 Pod中使用存储，通过volume-nfs-demo名字调用存储卷 对应生成的yaml文件内容如下 apiVersion: apps/v1beta2 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"1\" description: demo creationTimestamp: \"2019-10-08T15:45:18Z\" generation: 1 labels: k8s-app: the-volume-demo qcloud-app: the-volume-demo name: the-volume-demo namespace: default resourceVersion: \"618380753\" selfLink: /apis/apps/v1beta2/namespaces/default/deployments/the-volume-demo uid: a0fc4600-e9e2-11e9-b3f4-decf0ef369cf spec: minReadySeconds: 10 progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: the-volume-demo qcloud-app: the-volume-demo strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: the-volume-demo qcloud-app: the-volume-demo spec: containers: - image: nginx:latest imagePullPolicy: Always name: nginx-demo resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: #挂载到pod中 - mountPath: /usr/share/nginx/html name: volume-nfs-demo dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey - name: tencenthubkey restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: #CFS存储 - name: volume-nfs-demo nfs: path: / server: 10.66.200.7 写在最后 本文介绍了kubernetes存储中最基本volume的使用，介绍了volume支持多种不同驱动，以实际案例介绍emptyDir，hostPath，nfs驱动的对接，并介绍了TKE下volume功能的使用。由于volume需要知道底层存储的细节，不便于广泛使用，后来衍生为PV，管理员定义PV实现和底层存储对接，用户通过PVC使用PV，下节我们将介绍PV/PVC和StorageClass的使用。 参考文献 volume管理：https://kubernetes.io/docs/concepts/storage/volumes/ pod中使用volume：https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/ 『 转载 』该文章来源于网络，侵删。 ","date":"2019-08-04","objectID":"/09-%E5%88%9D%E8%AF%86pod%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/:5:0","tags":["kubernetes"],"title":"09 初识Pod存储管理","uri":"/09-%E5%88%9D%E8%AF%86pod%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/"},{"categories":["转载","kubernetes","基础教程"],"content":"写在前面 上一篇文章中kubernetes系列教程（九）初识Pod存储管理介绍了kubernetes中存储Volume的使用，volume支持多种不同的内置驱动，使用volumes需要知道后端驱动的细节，使用起来不方便，因此社区提出了PV概念，即通过管理员定义好PV，通过PVC使用PV；随着PV数量的不断增加，管理员需要频繁定义PV，因此提出了动态存储StorageClass，通过PVC中调用StorageClass动态创建PV，接下来介绍kubernetes系列教程高级进阶PV/PVC。 1. PV与PVC存储 ","date":"2019-08-04","objectID":"/10-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8pv%E5%92%8Cpvc/:0:0","tags":["kubernetes"],"title":"10 深入学习持久化存储PV和PVC","uri":"/10-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8pv%E5%92%8Cpvc/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.1 PV概念介绍 PV即PersistentVolume持久化存储，是管理员定义的一块存储空间，能抽象化底层存储细节，和node类似，PV是集群级别的资源，生命周期独立于Pod，支持静态创建和动态创建，动态创建需通过StorageClass。 PVC即PersistentVolumeClaim持久化存储申明，作为PV资源的使用方，可以指定请求存储容量大小和访问模式 StorageClass，存储类型支持创建PV，通过在PVC中指定StorageClass可动态创建PV，且支持指定不同的存储 PV支持设置字段介绍： Capacity 存储的特性，当前只支持通过capacity指定存储大小，未来会支持IOPS，吞吐量等指标 VolumeMode 存储卷的类型，默认为filesystem，如果是块设备指定为block Class 通过storageClassName指定静态StorageClass的名称 Reclaim Policy 回收策略，支持Retain保留，Recycle回收，DELETE删除 Volume驱动类型，和上一篇文章介绍的类似，支持不同的plugin驱动如RBD，NFS Mount Options 挂载模式，支持管理员定义不同的挂载选项 AccessMode 访问模式，指定node的挂载方式，支持ReadWriteOnce读写挂载一次，ReadOnlyMany多个节点挂载只读模式，ReadWriteMany多个节点挂载读写模式，不同的volume驱动类型支持的模式有所不同，如下 ","date":"2019-08-04","objectID":"/10-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8pv%E5%92%8Cpvc/:1:0","tags":["kubernetes"],"title":"10 深入学习持久化存储PV和PVC","uri":"/10-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8pv%E5%92%8Cpvc/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2 定义PV存储 接下来我们开始学习PV的使用，使用阶段分为：1. 预先创建好PV，2. 用户通过PVC调用PV，3. Pod中应用PVC，创建流程参考下图： \\1. 定义一个PV，指定大小为10G，读写模式为单个node读写，回收模式为Retain，后端驱动plugin为NFS [root@node-1 happylau]# cat pv-nfs-storage.yaml apiVersion: v1 kind: PersistentVolume metadata: name: pv-nfs-storage labels: storage: nfs annotations: kubernetes.io.description: pv-storage spec: storageClassName: nfs #静态指定存储类StorageClass名称 capacity: #capacity指定存储容量大小 storage: 10Gi accessModes: #访问模式为单个节点读写模式 - ReadWriteOnce persistentVolumeReclaimPolicy: Retain #回收模式为保留 nfs: #后端plugin驱动类型为NFS，指定server和path路径 server: 10.254.100.101 path: /mnt/data \\2. 创建PersistentVolumes [root@node-1 happylau]# kubectl apply -f pv-nfs-storage.yaml persistentvolume/pv-nfs-storage unchanged \\3. 查看PersistentVolumes列表 [root@node-1 happylau]# kubectl get persistentvolumes NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv-nfs-storage 10Gi RWO Retain Available nfs 43m 关于PV输出解释说明： ACCESS MODES 指定的是读写模式，RWO代表ReadWriteOnce，ROM代表ReadOnlyMany，RWX代表ReadWriteMany STATUS代表PV状态，Available刚创建未绑定状态，Bound为与PVC绑定，Released为PVC删除PV未释放，Failed状态异常。 \\4. 查看PV详细信息,呈现的信息会更详细 [root@node-1 ~]# kubectl describe persistentvolumes pv-nfs-storage Name: pv-nfs-storage Labels: storage=nfs Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"v1\",\"kind\":\"PersistentVolume\",\"metadata\":{\"annotations\":{\"kubernetes.io.description\":\"pv-storage\"},\"labels\":{\"storage\":\"nfs... kubernetes.io.description: pv-storage Finalizers: [kubernetes.io/pv-protection] StorageClass: nfs Status: Available Claim: Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: \u003cnone\u003e Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: 10.254.100.101 Path: /mnt/data ReadOnly: false Events: \u003cnone\u003e ","date":"2019-08-04","objectID":"/10-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8pv%E5%92%8Cpvc/:2:0","tags":["kubernetes"],"title":"10 深入学习持久化存储PV和PVC","uri":"/10-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8pv%E5%92%8Cpvc/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.3. PVC引用PV \\1. 通过定义PVC，通过selector和PV实现关联，指定到相同的StorageClass [root@node-1 happylau]# cat pvc-nfs-storage.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-nfs-storage labels: storage: pvc annotations: kubernetes.io/description: \"PersistentVolumeClaim for PV\" spec: accessModes: - ReadWriteOnce volumeMode: Filesystem storageClassName: nfs resources: requests: storage: 1Gi limits: storage: 10Gi selector: matchLabels: storage: nfs \\2. 生成PersistentVolumeClaim [root@node-1 happylau]# kubectl apply -f pvc-nfs-storage.yaml persistentvolumeclaim/pvc-nfs-storage created \\3. 查看PersistentVolumeClaim列表,通过STATUS可以知道，当前PVC和PV已经Bond关联 [root@node-1 happylau]# kubectl get persistentvolumeclaims NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc-nfs-storage Bound pv-nfs-storage 10Gi RWO nfs 查看PVC详情： [root@node-1 happylau]# kubectl describe persistentvolumeclaims pvc-nfs-storage Name: pvc-nfs-storage Namespace: default StorageClass: nfs Status: Bound Volume: pv-nfs-storage Labels: storage=pvc Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"annotations\":{\"kubernetes.io/description\":\"PersistentVolumeClaim for PV\"},\"... kubernetes.io/description: PersistentVolumeClaim for PV pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 10Gi Access Modes: RWO VolumeMode: Filesystem Mounted By: \u003cnone\u003e Events: \u003cnone\u003e \\4. 再次查看PV的状态，此时状态为Bond，和default命名空间下的PVC pvc-nfs-storage关联,此时PVC已经定义好 ","date":"2019-08-04","objectID":"/10-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8pv%E5%92%8Cpvc/:3:0","tags":["kubernetes"],"title":"10 深入学习持久化存储PV和PVC","uri":"/10-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8pv%E5%92%8Cpvc/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.4 Pod引用PVC PV和PVC定义好后，需要在Pod中引用定义的存储，引用方式和之前定义的类似，spec.containers.volumeMounts在Pod中引用定义的存储，前面的文章中我们直接通过Pod调用，本文案例通过将Pod以Template的形式定义封装在Deployment的控制器中，下篇文章我们再深入介绍Deployment，ReplicaSet，StatefulSet等副本控制器。 \\1. 定义一个Deployments，通过deployment.spec.template.spec应用Pod,在volumes中调用PVC存储，volumeMounts将存储挂载到指定目录。 [root@node-1 happylau]# cat pvc-nfs-deployments.yaml apiVersion: apps/v1 kind: Deployment metadata: #deployment的元数据 name: pvc-nfs-deployment labels: app: pvc-nfs-deployment spec: #deployment的属性信息 replicas: 1 #副本控制数 selector: matchLabels: app: pvc-nfs-deployment template: #通过定义模板引用Pod，template中的信息和Pod定义的信息一致，包含metadata,spec信息 metadata: #定义Pod的labels labels: app: pvc-nfs-deployment spec: containers: - name: nginx-web image: nginx:latest imagePullPolicy: IfNotPresent ports: - name: nginx-http-80 protocol: TCP containerPort: 80 volumeMounts: #将PVC存储挂载到目录 - name: pvc-nfs-storage mountPath: /usr/share/nginx/html volumes: #通过volumes引用persistentVolumeClaim存储 - name: pvc-nfs-storage persistentVolumeClaim: claimName: pvc-nfs-storage \\2. 创建Deployments并查看创建情况和Pod情况 创建Deployments [root@node-1 happylau]# kubectl apply -f pvc-nfs-deployments.yaml deployment.apps/pvc-nfs-deployment created 查看Deployments列表 [root@node-1 happylau]# kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE pvc-nfs-deployment 1/1 1 1 16s 查看Deployments中的Pod [root@node-1 happylau]# kubectl get pods -l app=pvc-nfs-deployment -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pvc-nfs-deployment-7467b9fbfc-xwdpr 1/1 Running 0 106s 10.244.1.29 node-2 \u003cnone\u003e \u003cnone\u003e \\3. Pod中查看存储挂载信息，并做数据读写。 [root@node-1 happylau]# kubectl exec -it pvc-nfs-deployment-7467b9fbfc-xwdpr /bin/bash root@pvc-nfs-deployment-7467b9fbfc-xwdpr:/# df -h Filesystem Size Used Avail Use% Mounted on overlay 50G 4.3G 43G 10% / tmpfs 64M 0 64M 0% /dev tmpfs 920M 0 920M 0% /sys/fs/cgroup /dev/vda1 50G 4.3G 43G 10% /etc/hosts shm 64M 0 64M 0% /dev/shm 10.254.100.101:/mnt/data 50G 9.9G 37G 22% /usr/share/nginx/html #挂载成功 tmpfs 920M 12K 920M 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 920M 0 920M 0% /proc/acpi tmpfs 920M 0 920M 0% /proc/scsi tmpfs 920M 0 920M 0% /sys/firmware 写入站点数据内容 root@pvc-nfs-deployment-7467b9fbfc-xwdpr:~# echo \"pvc index by happylau\" \u003e/usr/share/nginx/html/index.html root@pvc-nfs-deployment-7467b9fbfc-xwdpr:~# ls -l /usr/share/nginx/html/ total 4 -rw-r--r-- 1 nobody nogroup 22 Oct 12 02:00 index.html \\4. 测试访问，直接访问Pod的IP，一般通过service来调用，后续再介绍service [root@node-1 ~]# kubectl get pods -l app=pvc-nfs-deployment -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pvc-nfs-deployment-7467b9fbfc-xwdpr 1/1 Running 0 14m 10.244.1.29 node-2 \u003cnone\u003e \u003cnone\u003e [root@node-1 ~]# curl http://10.244.1.29 pvc index by happylau 写在最后 本文通过介绍了持久化存储PV和持久化存储声明PVC的使用场景和相关概念，并通过实例演示PV和PVC的使用，由于PV需要管理员预先定义，对于大规模环境下使用不便利，因此有了动态PV，即通过StorageClass实现，下章节我们将介绍StorageClass的使用。 参考文献 PV和PVC介绍：https://kubernetes.io/docs/concepts/storage/persistent-volumes/ 有状态化应用：https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/ 『 转载 』该文章来源于网络，侵删。 ","date":"2019-08-04","objectID":"/10-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8pv%E5%92%8Cpvc/:4:0","tags":["kubernetes"],"title":"10 深入学习持久化存储PV和PVC","uri":"/10-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8pv%E5%92%8Cpvc/"},{"categories":["转载","kubernetes","基础教程"],"content":"写在前面 前面的文章我们深入介绍了Pod的使用，包括Pod定义，Pod资源管理和服务质量，Pod健康检查，Pod存储管理，Pod调度，当Pod所在的node异常时，Pod无法自动恢复，因此Pod很少单独使用，一般以template的形式嵌套在控制器中使用，下来介绍kubernetes系列教程副本控制器Deployment，ReplicaSet，ReplicationController的使用。 1. 深入学习控制器 ","date":"2019-08-04","objectID":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/:0:0","tags":["kubernetes"],"title":"11 深入学习Deployment控制器","uri":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.1 控制器概述 Pod是kubernetes所有运行应用或部署服务的基础，可以看作是k8s中运行的机器人，应用单独运行在Pod中不具备高级的特性，比如节点故障时Pod无法自动迁移，Pod多副本横向扩展，应用滚动升级RollingUpdate等，因此Pod一般不会单独使用，需要使用控制器来实现。 我们先看一个概念ReplicationController副本控制器，简称RC，副本控制是实现Pod高可用的基础，其通过定义副本的副本数replicas，当运行的Pod数量少于replicas时RC会自动创建Pod，当Pod数量多于replicas时RC会自动删除多余的Pod，确保当前运行的Pod和RC定义的副本数保持一致。 副本控制器包括Deployment，ReplicaSet，ReplicationController，StatefulSet等。其中常用有两个：Deployment和StatefulSet，Deployment用于无状态服务，StatefulSet用于有状态服务，ReplicaSet作为Deployment后端副本控制器，ReplicationController则是旧使用的副本控制器。 为了实现不同的功能，kubernetes中提供多种不同的控制器满足不同的业务场景，可以分为四类： Stateless application无状态化应用，如Deployment，ReplicaSet，RC等； Stateful application有状态化应用，需要保存数据状态，如数据库，数据库集群； Node daemon节点支撑守护，适用于在所有或部分节点运行Daemon，如日志，监控采集； Batch批处理任务，非长期运行服务，分为一次性运行Job和计划运行CronJob两种。 本文我们主要介绍无状态服务副本控制器的使用，包括Deployment，ReplicaSet和ReplicationController。 ","date":"2019-08-04","objectID":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/:1:0","tags":["kubernetes"],"title":"11 深入学习Deployment控制器","uri":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2 Deployment Deployment是实现无状态应用副本控制器，其通过declarative申明式的方式定义Pod的副本数，Deployment的副本机制是通过ReplicaSet实现，replicas副本的管理通过在ReplicaSet中添加和删除Pod，RollingUpdate通过新建ReplicaSet，然后逐步移除和添加ReplicaSet中的Pod数量，从而实现滚动更新，使用Deployment的场景如下： 滚动升级RollingUpdate，后台通过ReplicaSet实现 多副本replicas实现，增加副本(高负载)或减少副本(低负载) 应用回滚Rollout，版本更新支持回退 ","date":"2019-08-04","objectID":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/:2:0","tags":["kubernetes"],"title":"11 深入学习Deployment控制器","uri":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2.1 Deployment定义 \\1. 我们定义一个Deployment，副本数为3，Pod以模版Template的形式封装在Deployment中，为了结合之前Pod学习内容，我们增加了resource和健康检查的定义，具体实现参考前面介绍的文章。 [root@node-1 happylau]# cat deployment-demo.yaml apiVersion: apps/v1 kind: Deployment metadata: #Deployment的元数据信息，包含名字，标签 name: deployment-nginx-demo labels: app: nginx rc: deployment annotations: kubernetes.io/replicationcontroller: Deployment kubernetes.io/description: \"ReplicationController Deployment Demo\" spec: replicas: 3 #副本数量，包含有3个Pod副本 selector: #标签选择器，选择管理包含指定标签的Pod matchLabels: app: nginx rc: deployment template: #如下是Pod的模板定义，没有apiVersion，Kind属性，需包含metadata定义 metadata: #Pod的元数据信息，必须包含有labels labels: app: nginx rc: deployment spec: #spec指定容器的属性信息 containers: - name: nginx-deployment image: nginx:1.7.9 imagePullPolicy: IfNotPresent ports: #容器端口信息 - name: http-80-port protocol: TCP containerPort: 80 resources: #资源管理,requests请求资源，limits限制资源 requests: cpu: 100m memory: 128Mi limits: cpu: 200m memory: 256Mi livenessProbe: #健康检查器livenessProbe，存活检查 httpGet: path: /index.html port: 80 scheme: HTTP initialDelaySeconds: 3 periodSeconds: 5 timeoutSeconds: 2 readinessProbe: #健康检查器readinessProbe，就绪检查 httpGet: path: /index.html port: 80 scheme: HTTP initialDelaySeconds: 3 periodSeconds: 5 timeoutSeconds: 2 Deployment字段说明： deployment基本属性，包括apiVersion,Kind,metadata和spec，其中，deployment.metdata指定名称和标签内容，deployment.spec指定部署组的属性信息； deployment属性信息包含有replicas，Selector和template，其中replicas指定副本数目，Selector指定管理的Pod标签，template为定义Pod的模板，Deployment通过模板创建Pod； deployment.spec.template为Pod定义的模板，和Pod定义不太一样，template中不包含apiVersion和Kind属性，要求必须有metadata，deployment.spec.template.spec为容器的属性信息，其他定义内容和Pod一致。 \\2. 生成Deployment，创建时加一个–record参数,会在annotation中记录deployment.kubernetes.io/revision版本 [root@node-1 happylau]# kubectl apply -f deployment-demo.yaml --record deployment.apps/deployment-nginx-demo created \\3. 查看Deployment列表，运行时自动下载镜像，如下已运行了3个副本 [root@node-1 happylau]# kubectl get deployments deployment-nginx-demo NAME READY UP-TO-DATE AVAILABLE AGE deployment-nginx-demo 3/3 3 3 2m37s NAME代表名称,metadata.name字段定义 READY代表Pod的健康状态，前面值是readiness，后面是liveness UP-TO-DATE代表更新，用于滚动升级 AVAILABLE代表可用 AGE创建至今运行的时长 \\4. 查看Deployment的详情，可以看到Deployment通过一个deployment-nginx-demo-866bb6cf78 replicaset副本控制器控制Pod的副本数量 \\5. 查看replicaset的详情信息，通过Events可查看到deployment-nginx-demo-866bb6cf78创建了三个Pod \\6. 查看Pod详情，最终通过Pod定义的模版创建container，资源定义，健康检查等包含在Pod定义的模版中 通过上面的实战演练我们可得知Deployment的副本控制功能是由replicaset实现，replicaset生成Deployment中定义的replicas副本的数量，即创建多个副本，如下图所示： ","date":"2019-08-04","objectID":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/:2:1","tags":["kubernetes"],"title":"11 深入学习Deployment控制器","uri":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2.2 Deployment扩容 当业务比较繁忙时可以通过增加副本数，增加副本数是通过yaml文件中的replicas控制的，当设置了replias后，Deployment控制器会自动根据当前副本数目创建所需的Pod数，这些pod会自动加入到service中实现负载均衡，相反减少副本数，这些pod会自动从service中删除。 \\1. 将deployment的副本数扩容至4个，可通过修改yaml文件的replicas个数或者通过scale命令扩展。 [root@node-1 ~]# kubectl scale --replicas=4 deployment deployment-nginx-demo deployment.extensions/deployment-nginx-demo scaled \\2. 查看Deployment副本数量,已增加至4个副本 [root@node-1 ~]# kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE deployment-nginx-demo 4/4 4 4 77m \\3. 副本的扩容是如何实现的呢？我们查看replicaset的详情信息观察，增加副本的个数是通过replicaset来扩容，通过模版复制新的Pod \\4. 副本缩容 [root@node-1 ~]# kubectl scale --replicas=2 deployment deployment-nginx-demo deployment.extensions/deployment-nginx-demo scaled [root@node-1 ~]# kubectl get deployments deployment-nginx-demo NAME READY UP-TO-DATE AVAILABLE AGE deployment-nginx-demo 2/2 2 2 7h41m 通过上面的操作演练我们可以得知:Deployment的扩容是通过ReplicaSet的模版创建Pod或删除Pod实现，scale是手动扩展实现副本的机制，kubernetes还提供了另外一种副本自动扩容机制horizontalpodautoscalers(Horizontal Pod Autoscaling),即通过定义CPU的利用率实现自动的横向扩展，由于需要依赖于监控组件，后续我们再做介绍。 ","date":"2019-08-04","objectID":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/:2:2","tags":["kubernetes"],"title":"11 深入学习Deployment控制器","uri":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2.3 滚动更新 Deployment支持滚动更新，默认创建Deployment后会增加滚动更新的策略，通过逐步替代replicas中的pod实现更新无服务中断（需要结合service），如下图所示：将一个deployment副本数为3的应用更新，先更新10.0.0.6 pod，更新pod应用，替换新的ip，然后加入到service中，以此类推再继续更新其他pod，从而实现滚动更新，不影响服务的升级。 通过类型为：RollingUpdate，每次更新最大的数量maxSurge是replicas总数的25%,最大不可用的数量maxUnavailable为25%，如下是通过kubectl get deployments deployment-nginx-demo -o yaml查看滚动更新相关的策略。 spec: progressDeadlineSeconds: 600 replicas: 3 revisionHistoryLimit: 10 selector: matchLabels: app: nginx rc: deployment strategy: #strategy定义的是升级的策略，类型为RollingUpdate rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate \\1. 滚动更新是当pod.template中定义的相关属性变化，如下将镜像更新到1.9.1，通过–record会记录操作命令 [root@node-1 ~]# kubectl set image deployments deployment-nginx-demo nginx-deployment=nginx:1.9.1 --record deployment.extensions/deployment-nginx-demo image updated \\2. 查看滚动升级的状态（由于第一次nginx的镜像写错了，容器下载镜像失败，写成nignx，导致修改后有两次record） [root@node-1 happylau]# kubectl rollout status deployment deployment-nginx-demo deployment \"deployment-nginx-demo\" successfully rolled out 查看滚动升级版本，REVSISION代表版本号： [root@node-1 happylau]# kubectl rollout history deployment deployment-nginx-demo deployment.extensions/deployment-nginx-demo REVISION CHANGE-CAUSE 1 \u003cnone\u003e 2 kubectl set image deployments deployment-nginx-demo nginx-deployment=nignx:1.9.1 --record=true 3 kubectl set image deployments deployment-nginx-demo nginx-deployment=nignx:1.9.1 --record=true \\3. 观察Deployment的升级过程，新创建一个RS deployment-nginx-demo-65c8c98c7b，逐渐将旧RS中的pod替换，直至旧的RS deployment-nginx-demo-866bb6cf78上的副本数为0. \\4. 查看RS的列表，可以看到新的RS的副本数为2，其他RS副本数为0 [root@node-1 ~]# kubectl get replicasets NAME DESIRED CURRENT READY AGE deployment-nginx-demo-65c8c98c7b 2 2 2 21m #新的RS，REVSION为3 deployment-nginx-demo-6cb65f58c6 0 0 0 22m #镜像写错的RS，REVISON为2 deployment-nginx-demo-866bb6cf78 0 0 0 40m #旧的RS，对应REVSION为1 \\5. 测试版本升级是否成功 [root@node-1 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deployment-nginx-demo-65c8c98c7b-bzql9 1/1 Running 0 25m 10.244.1.58 node-2 \u003cnone\u003e \u003cnone\u003e deployment-nginx-demo-65c8c98c7b-vrjhp 1/1 Running 0 25m 10.244.2.72 node-3 \u003cnone\u003e \u003cnone\u003e [root@node-1 ~]# curl -I http://10.244.2.72 HTTP/1.1 200 OK Server: nginx/1.9.1 #镜像的版本成功更新 Date: Mon, 28 Oct 2019 15:28:49 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Tue, 26 May 2015 15:02:09 GMT Connection: keep-alive ETag: \"55648af1-264\" Accept-Ranges: bytes ","date":"2019-08-04","objectID":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/:2:3","tags":["kubernetes"],"title":"11 深入学习Deployment控制器","uri":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2.4 版本回退 如果版本不符合预期，kubernetes提供回退的功能，和滚动更新一样，回退的功能Deployment将替换到原始的RS上，即逐步将Pod的副本替换到旧的RS上. \\1. 执行回滚，回退到REVISON版本为1 [root@node-1 ~]# kubectl rollout undo deployment deployment-nginx-demo --to-revision=1 deployment.extensions/deployment-nginx-demo rolled back \\2. 查看Deployment的回退状态和历史版本 [root@node-1 ~]# kubectl rollout status deployment deployment-nginx-demo deployment \"deployment-nginx-demo\" successfully rolled out [root@node-1 ~]# kubectl rollout history deployment deployment-nginx-demo deployment.extensions/deployment-nginx-demo REVISION CHANGE-CAUSE 2 kubectl set image deployments deployment-nginx-demo nginx-deployment=nignx:1.9.1 --record=true 3 kubectl set image deployments deployment-nginx-demo nginx-deployment=nignx:1.9.1 --record=true 4 \u003cnone\u003e \\3. 查看Deployment的详情，可以看到RS已经会退到原始的RS了 \\4. 测试nginx的版本 [root@node-1 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deployment-nginx-demo-866bb6cf78-9thtn 1/1 Running 0 3m42s 10.244.1.59 node-2 \u003cnone\u003e \u003cnone\u003e deployment-nginx-demo-866bb6cf78-ws2hx 1/1 Running 0 3m48s 10.244.2.73 node-3 \u003cnone\u003e \u003cnone\u003e #测试版本 [root@node-1 ~]# curl -I http://10.244.1.59 HTTP/1.1 200 OK Server: nginx/1.7.9 #回退到1.7.9版本 Date: Mon, 28 Oct 2019 15:36:07 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Tue, 23 Dec 2014 16:25:09 GMT Connection: keep-alive ETag: \"54999765-264\" Accept-Ranges: bytes 小结：通过上面的操作演练可知，Deployment是通过ReplicaSet实现Pod副本数的管理（扩容或减少副本数），滚动更新是通过新建RS，将Pod从旧的RS逐步更新到新的RS上；相反，回滚版本将会退到指定版本的ReplicaSet上。 ","date":"2019-08-04","objectID":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/:2:4","tags":["kubernetes"],"title":"11 深入学习Deployment控制器","uri":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.3 ReplicaSet ReplicaSet副本集简称RS，用于实现副本数的控制，通过上面的学习我们可以知道Deployment实际是调用ReplicaSet实现副本的控制，RS不具备滚动升级和回滚的特性，一般推荐使用Deployment，ReplicaSet的定义和Deployment差不多，如下： \\1. 定义ReplicaSet [root@node-1 happylau]# cat replicaset-demo.yaml apiVersion: extensions/v1beta1 kind: ReplicaSet metadata: name: replicaset-demo labels: controller: replicaset annotations: kubernetes.io/description: \"Kubernetes Replication Controller Replication\" spec: replicas: 3 #副本数 selector: #Pod标签选择器 matchLabels: controller: replicaset template: #创建Pod的模板 metadata: labels: controller: replicaset spec: #容器信息 containers: - name: nginx-replicaset-demo image: nginx:1.7.9 imagePullPolicy: IfNotPresent ports: - name: http-80-port protocol: TCP containerPort: 80 \\2. 创建RS并查看RS列表 [root@node-1 happylau]# kubectl apply -f replicaset-demo.yaml replicaset.extensions/replicaset-demo created [root@node-1 happylau]# kubectl get replicasets replicaset-demo NAME DESIRED CURRENT READY AGE replicaset-demo 3 3 3 15s \\3. 扩展副本数至4个 [root@node-1 happylau]# kubectl scale --replicas=4 replicaset replicaset-demo replicaset.extensions/replicaset-demo scaled [root@node-1 happylau]# kubectl get replicasets replicaset-demo NAME DESIRED CURRENT READY AGE replicaset-demo 4 4 4 76s \\4. 减少副本数至2个 [root@node-1 happylau]# kubectl scale --replicas=2 replicaset replicaset-demo replicaset.extensions/replicaset-demo scaled [root@node-1 happylau]# kubectl get replicasets replicaset-demo NAME DESIRED CURRENT READY AGE replicaset-demo 2 2 2 114s \\5. 镜像版本升级，验证得知不具备版本升级的能力 [root@node-1 happylau]# kubectl set image replicasets replicaset-demo nginx-replicaset-demo=nginx:1.9.1 replicaset.extensions/replicaset-demo image updated #命令执行成功了 ReplicaSet小结：通过上面的演示可以知道，RS定义和Deployment类似，能实现副本的控制，扩展和缩减，Deployment是更高层次的副本控制器，ReplicaSet主要为Deployment的副本控制器和滚动更新机制，ReplicaSet本身无法提供滚动更新的能力。 ","date":"2019-08-04","objectID":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/:3:0","tags":["kubernetes"],"title":"11 深入学习Deployment控制器","uri":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.4 ReplicationController ReplicationController副本控制器简称RC，是kubernetes中最早的副本控制器，RC是ReplicaSet之前的版本，ReplicationController提供副本控制能力，其定义方式和Deployment，ReplicaSet相类似，如下： \\1. 定义ReplicationController [root@node-1 happylau]# cat rc-demo.yaml apiVersion: v1 kind: ReplicationController metadata: name: rc-demo labels: controller: replicationcontroller annotations: kubernetes.io/description: \"Kubernetes Replication Controller Replication\" spec: replicas: 3 selector: #不能使用matchLables字符集模式 controller: replicationcontroller template: metadata: labels: controller: replicationcontroller spec: containers: - name: nginx-rc-demo image: nginx:1.7.9 imagePullPolicy: IfNotPresent ports: - name: http-80-port protocol: TCP containerPort: 80 \\2. 生成RC并查看列表 [root@node-1 happylau]# kubectl apply -f rc-demo.yaml replicationcontroller/rc-demo created [root@node-1 happylau]# kubectl get replicationcontrollers NAME DESIRED CURRENT READY AGE rc-demo 3 3 3 103s #查看详情 [root@node-1 happylau]# kubectl describe replicationcontrollers rc-demo Name: rc-demo Namespace: default Selector: controller=replicationcontroller Labels: controller=replicationcontroller Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"v1\",\"kind\":\"ReplicationController\",\"metadata\":{\"annotations\":{\"kubernetes.io/description\":\"Kubernetes Replication Controlle... kubernetes.io/description: Kubernetes Replication Controller Replication Replicas: 3 current / 3 desired Pods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: controller=replicationcontroller Containers: nginx-rc-demo: Image: nginx:1.7.9 Port: 80/TCP Host Port: 0/TCP Environment: \u003cnone\u003e Mounts: \u003cnone\u003e Volumes: \u003cnone\u003e Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 113s replication-controller Created pod: rc-demo-hm8s9 Normal SuccessfulCreate 113s replication-controller Created pod: rc-demo-xnfht Normal SuccessfulCreate 113s replication-controller Created pod: rc-demo-lfhc9 3.副本扩容至4个 [root@node-1 happylau]# kubectl scale --replicas=4 replicationcontroller rc-demo replicationcontroller/rc-demo scaled [root@node-1 happylau]# kubectl get replicationcontrollers NAME DESIRED CURRENT READY AGE rc-demo 4 4 4 3m23s \\4. 副本缩容至2个 [root@node-1 happylau]# kubectl scale --replicas=2 replicationcontroller rc-demo replicationcontroller/rc-demo scaled [root@node-1 happylau]# kubectl get replicationcontrollers NAME DESIRED CURRENT READY AGE rc-demo 2 2 2 3m51s 写在最后 本文介绍了kubernetes中三个副本控制器：Deployment，ReplicaSet和ReplicationController，当前使用最广泛的是Deployment，ReplicaSet为Deployment提供滚动更新机制，RC当前是旧版的副本控制器，当前已废弃，推荐使用Deployment控制器，具备副本控制器，扩展副本，缩减副本，滚动升级和回滚等高级能力。 参考文献 Deployment：https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ ReplicaSet：https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/ ReplicationController：[https://kubernetes.io/docs/concepts/workloads/controllers/ 『 转载 』该文章来源于网络，侵删。 ","date":"2019-08-04","objectID":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/:4:0","tags":["kubernetes"],"title":"11 深入学习Deployment控制器","uri":"/11-%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0deployment%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"写在前面 上章节中介绍了Deployment，ReplicaSet，ReplicationController等副本控制器的使用和场景，接下来介绍kubernetes系列教程控制器DaemonSet使用。 1. DaemonSet控制器 ","date":"2019-08-04","objectID":"/12-%E8%AF%A6%E8%A7%A3daemonset%E6%8E%A7%E5%88%B6%E5%99%A8/:0:0","tags":["kubernetes"],"title":"12 详解DaemonSet控制器","uri":"/12-%E8%AF%A6%E8%A7%A3daemonset%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.1 DaemonSet简介 介绍DaemonSet时我们先来思考一个问题：相信大家都接触过监控系统比如zabbix，监控系统需要在被监控机安装一个agent，安装agent通常会涉及到以下几个场景： 所有节点都必须安装agent以便采集监控数据 新加入的节点需要配置agent，手动或者运行脚本 节点下线后需要手动在监控系统中删除 kubernetes中经常涉及到在node上安装部署应用，它是如何解决上述的问题的呢？答案是DaemonSet。DaemonSet守护进程简称DS，适用于在所有节点或部分节点运行一个daemon守护进程，如监控我们安装部署时网络插件kube-flannel和kube-proxy，DaemonSet具有如下特点： DaemonSet确保所有节点运行一个Pod副本 指定节点运行一个Pod副本，通过标签选择器或者节点亲和性 新增节点会自动在节点增加一个Pod 移除节点时垃圾回收机制会自动清理Pod DaemonSet适用于每个node节点均需要部署一个守护进程的场景，常见的场景例如： 日志采集agent，如fluentd或logstash 监控采集agent，如Prometheus Node Exporter,Sysdig Agent,Ganglia gmond 分布式集群组件，如Ceph MON，Ceph OSD，glusterd，Hadoop Yarn NodeManager等 k8s必要运行组件，如网络flannel，weave，calico，kube-proxy等 安装k8s时默认在kube-system命名空间已经安装了有两个DaemonSet，分别为kube-flannel-ds-amd64和kube-proxy，分别负责flannel overlay网络的互通和service代理的实现，可以通过如下命令查看： \\1. 查看kube-system命令空间的DaemonSet列表，当前集群有三个node节点，所以每个DS会运行三个Pod副本 [root@node-1 ~]# kubectl get ds -n kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-flannel-ds-amd64 3 3 3 3 3 beta.kubernetes.io/arch=amd64 46d kube-proxy 3 3 3 3 3 beta.kubernetes.io/os=linux 46d \\2. 查看Pod的副本情况，可以看到DaemonSet在每个节点都运行一个Pod ","date":"2019-08-04","objectID":"/12-%E8%AF%A6%E8%A7%A3daemonset%E6%8E%A7%E5%88%B6%E5%99%A8/:1:0","tags":["kubernetes"],"title":"12 详解DaemonSet控制器","uri":"/12-%E8%AF%A6%E8%A7%A3daemonset%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2 DaemonSet定义 DaemonSet的定义和Deployment定义使用相类似，需要定义apiVersion，Kind，metadata和spec属性信息，spec中不需要定义replicas个数，spec.template即定义DS生成容器的模版信息，如下是运行一个fluentd-elasticsearch镜像容器的daemon守护进程，运行在每个node上通过fluentd采集日志上报到ElasticSearch。 \\1. 通过yaml文件定义DaemonSet [root@node-1 happylau]# cat fluentd-es-daemonset.yaml apiVersion: apps/v1 #api版本信息 kind: DaemonSet #类型为DaemonSet metadata: #元数据信息 name: fluentd-elasticsearch namespace: kube-system #运行的命名空间 labels: k8s-app: fluentd-logging spec: #DS模版 selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: #容器信息 - name: fluentd-elasticsearch image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources: #resource资源 limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: #挂载存储，agent需要到这些目录采集日志 - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: #将主机的目录以hostPath的形式挂载到容器Pod中。 - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers DaemonSet定义注意事项： daemonset.spec.template定义Pod的模板信息，包含的metadata信息需要和selector保持一致 template必须定义RestartPolicy的策略，切策略值为Always，保障服务异常时能自动重启恢复 Pod运行在特定节点，支持指定调度策略，如nodeSelector，Node affinity，实现灵活调度 \\2. 生成DaemonSet [root@node-1 happylau]# kubectl apply -f fluentd-es-daemonset.yaml daemonset.apps/fluentd-elasticsearch created \\3. 查看DaemonSet列表 [root@node-1 happylau]# kubectl get daemonsets -n kube-system fluentd-elasticsearch NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE fluentd-elasticsearch 3 3 3 3 3 \u003cnone\u003e 16s \\4. 查看node上运行Pod的情况,在NODE列可以看到每个node都运行了一个Pod [root@node-1 happylau]# kubectl get pods -n kube-system -o wide |grep fluentd fluentd-elasticsearch-blpqb 1/1 Running 0 3m7s 10.244.2.79 node-3 \u003cnone\u003e \u003cnone\u003e fluentd-elasticsearch-ksdlt 1/1 Running 0 3m7s 10.244.0.11 node-1 \u003cnone\u003e \u003cnone\u003e fluentd-elasticsearch-shtkh 1/1 Running 0 3m7s 10.244.1.64 node-2 \u003cnone\u003e \u003cnone\u003e \\5. 查看DaemonSet详情，可以看到DaemonSet支持RollingUpdate滚动更新策略 [root@node-1 happylau]# kubectl get daemonsets -n kube-system fluentd-elasticsearch -o yaml apiVersion: extensions/v1beta1 kind: DaemonSet metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"apps/v1\",\"kind\":\"DaemonSet\",\"metadata\":{\"annotations\":{},\"labels\":{\"k8s-app\":\"fluentd-logging\"},\"name\":\"fluentd-elasticsearch\",\"namespace\":\"kube-system\"},\"spec\":{\"selector\":{\"matchLabels\":{\"name\":\"fluentd-elasticsearch\"}},\"template\":{\"metadata\":{\"labels\":{\"name\":\"fluentd-elasticsearch\"}},\"spec\":{\"containers\":[{\"image\":\"quay.io/fluentd_elasticsearch/fluentd:v2.5.2\",\"name\":\"fluentd-elasticsearch\",\"resources\":{\"limits\":{\"memory\":\"200Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"200Mi\"}},\"volumeMounts\":[{\"mountPath\":\"/var/log\",\"name\":\"varlog\"},{\"mountPath\":\"/var/lib/docker/containers\",\"name\":\"varlibdockercontainers\",\"readOnly\":true}]}],\"terminationGracePeriodSeconds\":30,\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/master\"}],\"volumes\":[{\"hostPath\":{\"path\":\"/var/log\"},\"name\":\"varlog\"},{\"hostPath\":{\"path\":\"/var/lib/docker/containers\"},\"name\":\"varlibdockercontainers\"}]}}}} creationTimestamp: \"2019-10-30T15:19:20Z\" generation: 1 labels: k8s-app: fluentd-logging name: fluentd-elasticsearch namespace: kube-system resourceVersion: \"6046222\" selfLink: /apis/extensions/v1beta1/namespaces/kube-system/daemonsets/fluentd-elasticsearch uid: c2c02c48-9f93-48f3-9d6c-32bfa671db0e spec: revisionHistoryLimit: 10 selector: matchLabels: name: fluentd-elasticsearch template: metadata: creationTimestamp: null labels: name: fluentd-elasticsearch spec: containers: - image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 imagePullPolicy: IfNotPresent name: fluentd-elasticsearch resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMoun","date":"2019-08-04","objectID":"/12-%E8%AF%A6%E8%A7%A3daemonset%E6%8E%A7%E5%88%B6%E5%99%A8/:2:0","tags":["kubernetes"],"title":"12 详解DaemonSet控制器","uri":"/12-%E8%AF%A6%E8%A7%A3daemonset%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.3 滚动更新与回滚 \\1. 更新镜像至最新版本 [root@node-1 ~]# kubectl set image daemonsets fluentd-elasticsearch fluentd-elasticsearch=quay.io/fluentd_elasticsearch/fluentd:latest -n kube-system daemonset.extensions/fluentd-elasticsearch image updated \\2. 查看滚动更新状态 [root@node-1 ~]# kubectl rollout status daemonset -n kube-system fluentd-elasticsearch Waiting for daemon set \"fluentd-elasticsearch\" rollout to finish: 1 out of 3 new pods have been updated... Waiting for daemon set \"fluentd-elasticsearch\" rollout to finish: 1 out of 3 new pods have been updated... Waiting for daemon set \"fluentd-elasticsearch\" rollout to finish: 1 out of 3 new pods have been updated... Waiting for daemon set \"fluentd-elasticsearch\" rollout to finish: 2 out of 3 new pods have been updated... Waiting for daemon set \"fluentd-elasticsearch\" rollout to finish: 2 out of 3 new pods have been updated... Waiting for daemon set \"fluentd-elasticsearch\" rollout to finish: 2 out of 3 new pods have been updated... Waiting for daemon set \"fluentd-elasticsearch\" rollout to finish: 2 of 3 updated pods are available... daemon set \"fluentd-elasticsearch\" successfully rolled out \\3. 查看DaemonSet详情，可以看到DS滚动更新的过程：DaemonSet先将node上的pod删除然后再创建 \\4. 查看DaemonSet滚动更新版本，REVSION 1为初始的版本 [root@node-1 ~]# kubectl rollout history daemonset -n kube-system fluentd-elasticsearch daemonset.extensions/fluentd-elasticsearch REVISION CHANGE-CAUSE 1 \u003cnone\u003e 2 \u003cnone\u003e \\5. 更新回退，如果配置没有符合到预期可以回滚到原始的版本 [root@node-1 ~]# kubectl rollout undo daemonset -n kube-system fluentd-elasticsearch --to-revision=1 daemonset.extensions/fluentd-elasticsearch rolled back \\6. 确认版本回退情况 \\7. 观察版本回退的过程，回退的过程和和滚动更新过程类似，先删除Pod再创建 \\8. 删除DaemonSet [root@node-1 ~]# kubectl delete daemonsets -n kube-system fluentd-elasticsearch daemonset.extensions \"fluentd-elasticsearch\" deleted [root@node-1 ~]# kubectl get pods -n kube-system |grep fluentd fluentd-elasticsearch-d6f6f 0/1 Terminating 0 110m ","date":"2019-08-04","objectID":"/12-%E8%AF%A6%E8%A7%A3daemonset%E6%8E%A7%E5%88%B6%E5%99%A8/:3:0","tags":["kubernetes"],"title":"12 详解DaemonSet控制器","uri":"/12-%E8%AF%A6%E8%A7%A3daemonset%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.4 DaemonSet调度 前面kubernetes系列教程（七）深入玩转pod调度文章介绍了Pod的调度机制，DaemonSet通过kubernetes默认的调度器scheduler会在所有的node节点上运行一个Pod副本，可以通过如下三种方式将Pod运行在部分节点上： 指定nodeName节点运行 通过标签运行nodeSelector 通过亲和力调度node Affinity和node Anti-affinity DaemonSet调度算法用于实现将Pod运行在特定的node节点上，如下以通过node affinity亲和力将Pod调度到部分的节点上node-2上为例。 \\1. 为node添加一个app=web的labels [root@node-1 happylau]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS node-1 Ready master 47d v1.15.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node-1,kubernetes.io/os=linux,node-role.kubernetes.io/master= node-2 Ready \u003cnone\u003e 47d v1.15.3 app=web,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node-2,kubernetes.io/os=linux node-3 Ready \u003cnone\u003e 47d v1.15.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node-3,kubernetes.io/os=linux \\2. 添加node affinity亲和力调度算法，requiredDuringSchedulingIgnoredDuringExecution设置基本需要满足条件，preferredDuringSchedulingIgnoredDuringExecution设置优选满足条件 [root@node-1 happylau]# cat fluentd-es-daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: #优先满足条件 - weight: 1 preference: matchExpressions: - key: app operator: In values: - web requiredDuringSchedulingIgnoredDuringExecution: #要求满足条件 nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node-2 - node-3 terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers \\3. 生成DS，并查看列表 [root@node-1 happylau]# kubectl delete ds -n kube-system fluentd-elasticsearch daemonset.extensions \"fluentd-elasticsearch\" deleted [root@node-1 happylau]# kubectl get daemonsets -n kube-system fluentd-elasticsearch NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE fluentd-elasticsearch 1 1 1 1 1 \u003cnone\u003e 112s \\4. 校验Pod运行的情况，DaemonSet的Pod调度到node-2节点上 [root@node-1 happylau]# kubectl get pods -n kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES \u003cnone\u003e fluentd-elasticsearch-9kngs 1/1 Running 0 2m39s 10.244.1.82 node-2 \u003cnone\u003e \u003cnone\u003e 写在最后 本文介绍了kubernetes中DaemonSet控制器，DS控制器能确保所有的节点运行一个特定的daemon守护进程，此外通过nodeSelector或node Affinity能够实现将Pod调度到特定的node节点。 参考文档 DaemonSet**：**https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/ 『 转载 』该文章来源于网络，侵删。 ","date":"2019-08-04","objectID":"/12-%E8%AF%A6%E8%A7%A3daemonset%E6%8E%A7%E5%88%B6%E5%99%A8/:4:0","tags":["kubernetes"],"title":"12 详解DaemonSet控制器","uri":"/12-%E8%AF%A6%E8%A7%A3daemonset%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1. Jobs让单次任务跑起来 ","date":"2019-08-04","objectID":"/13-%E4%B8%80%E6%AC%A1%E6%80%A7%E4%BB%BB%E5%8A%A1job%E5%92%8C%E5%91%A8%E6%9C%9F%E4%BB%BB%E5%8A%A1/:0:0","tags":["kubernetes"],"title":"13 一次性任务Job和周期任务","uri":"/13-%E4%B8%80%E6%AC%A1%E6%80%A7%E4%BB%BB%E5%8A%A1job%E5%92%8C%E5%91%A8%E6%9C%9F%E4%BB%BB%E5%8A%A1/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.1 Jobs简介 Windows下可以通过批处理脚本完成批处理任务，脚本运行完毕后任务即可终止，从而实现批处理任务运行工作，类似的任务如何在kubernetes中运行呢？答案是Jobs，Jobs是kubernetes中实现一次性计划任务的Pod控制器—JobController，通过控制Pod来执行任务，其特点为： 创建Pod运行特定任务，确保任务运行完成 任务运行期间节点异常时会自动重新创建Pod 支持并发创建Pod任务数和指定任务数 Jobs任务运行方式有如下三种： 运行单个Jobs任务，一般运行一个pod，pod运行结束任务运行完成； 运行特定数量的任务，通过completion指定总计运行任务； 并发运行任务，通过parallelism指定并发数 ","date":"2019-08-04","objectID":"/13-%E4%B8%80%E6%AC%A1%E6%80%A7%E4%BB%BB%E5%8A%A1job%E5%92%8C%E5%91%A8%E6%9C%9F%E4%BB%BB%E5%8A%A1/:1:0","tags":["kubernetes"],"title":"13 一次性任务Job和周期任务","uri":"/13-%E4%B8%80%E6%AC%A1%E6%80%A7%E4%BB%BB%E5%8A%A1job%E5%92%8C%E5%91%A8%E6%9C%9F%E4%BB%BB%E5%8A%A1/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2 运行单个Jobs任务 1、 定义一个jobs任务，通过在command中运行特定一个脚本，将当前的时间打印100次 apiVersion: batch/v1 kind: Job metadata: name: jobs-demo labels: controller: jobs spec: parallelism: 1 #并发数，默认为1，即创建pod副本的数量 template: metadata: name: jobs-demo labels: controller: jobs spec: containers: - name: echo-time image: centos:latest imagePullPolicy: IfNotPresent command: - /bin/sh - -c - \"for i in `seq 0 100`;do echo ${date} \u0026\u0026 sleep 1;done\" restartPolicy: Never #设置为Never，jobs任务运行完毕即可完成 2、 运行Jobs任务 [root@node-1 happylau]# kubectl apply -f job-demo.yaml job.batch/job-demo created [root@node-1 happylau]# kubectl get jobs job-demo NAME COMPLETIONS DURATION AGE job-demo 0/1 41s 41s 3、 此时jobs控制器创建了一个pod容器运行任务,此时处于Running状态，任务处在运行过程中，如果运行完毕则会变为completed状态 [root@node-1 happylau]# kubectl get pods |grep job job-demo-ssrk7 1/1 Running 0 97s 4、查看jobs日志日志数据，可以看到当前jobs创建的任务是持续在终端中打印数字，且每次打印暂停1s钟 5、再次查看jobs的任务，可以看到任务已经completions，运行时长为103s,对应的pod状态处于completed状态 [root@node-1 ~]# kubectl get jobs NAME COMPLETIONS DURATION AGE job-demo 1/1 103s 5m12s ","date":"2019-08-04","objectID":"/13-%E4%B8%80%E6%AC%A1%E6%80%A7%E4%BB%BB%E5%8A%A1job%E5%92%8C%E5%91%A8%E6%9C%9F%E4%BB%BB%E5%8A%A1/:2:0","tags":["kubernetes"],"title":"13 一次性任务Job和周期任务","uri":"/13-%E4%B8%80%E6%AC%A1%E6%80%A7%E4%BB%BB%E5%8A%A1job%E5%92%8C%E5%91%A8%E6%9C%9F%E4%BB%BB%E5%8A%A1/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.3 Jobs运行多个任务 Jobs控制器提供了两个控制并发数的参数：completions和parallelism，completions表示需要运行任务数的总数，parallelism表示并发运行的个数，如设置为1则会依次运行任务，前面任务运行再运行后面的任务，如下以创建5个任务数为例演示Jobs控制器实现并发数的机制。 1、 定义计算圆周率的Jobs任务 apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: perl command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(100)\"] restartPolicy: Never parallelism: 1 completions: 5 2、运行jobs任务，并用kubectl get jobs –watch查看jobs创建过程，可以看到pod任务是依次运行，直至达到completions所定义的数量 3、Jobs任务都已运行完毕，查看Jobs列表可以看到任务都处于Completed状态，查看pod日志可以看到Pi圆周率计算的结果 ","date":"2019-08-04","objectID":"/13-%E4%B8%80%E6%AC%A1%E6%80%A7%E4%BB%BB%E5%8A%A1job%E5%92%8C%E5%91%A8%E6%9C%9F%E4%BB%BB%E5%8A%A1/:3:0","tags":["kubernetes"],"title":"13 一次性任务Job和周期任务","uri":"/13-%E4%B8%80%E6%AC%A1%E6%80%A7%E4%BB%BB%E5%8A%A1job%E5%92%8C%E5%91%A8%E6%9C%9F%E4%BB%BB%E5%8A%A1/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.4 Jobs运行并发任务 Jobs控制器支持运行并发任务，并发任务即Jobs控制器一次运行多个Pod执行任务处理，如下以一次性运行3个Pod并发数为例演示通过Jobs控制器实现并发任务 1、定义Jobs任务，设置3个并发数任务 apiVersion: batch/v1 kind: Job metadata: name: jobs-demo labels: controller: jobs spec: parallelism: 3 #运行并发数为3，一次性创建3个pod template: metadata: name: jobs-demo labels: controller: jobs spec: containers: - name: echo-time image: centos:latest imagePullPolicy: IfNotPresent command: - /bin/sh - -c - \"for i in `seq 0 10`;do echo `date` \u0026\u0026 sleep 1;done\" restartPolicy: Never 2、运行Jobs任务并查看,Jobs控制器同时创建了3个并发任务 3、通过上面的演示可知，通过parallelism指定并发数量，Jobs控制器会创建出多个Pod副本并运行直至任务completed，同时parallelism可以配合completions一起使用，通过并发创建特定数量的任务，如下以单次运行3个并发任务实现9个任务的Jobs任务 apiVersion: batch/v1 kind: Job metadata: name: jobs-demo labels: controller: jobs spec: parallelism: 3 #并发任务为3 completions: 9 #任务数为9 template: metadata: name: jobs-demo labels: controller: jobs spec: containers: - name: echo-time image: centos:latest imagePullPolicy: IfNotPresent command: - /bin/sh - -c - \"for i in `seq 0 10`;do echo `date` \u0026\u0026 sleep 1;done\" restartPolicy: Never 4、运行Jobs任务并观察创建过程,在describe jobs的详情events日志中可以看到一共创建了9个任务，每3个任务创建时间一样，即并发创建的任务 总结：通过前面的例子解析可得知，Jobs能在kubernetes中实现类似Windows下批处理或Linux下shell任务的功能，通过运行特定任务数+并发数控制创建Pod任务。需要注意一点的是，Jobs控制器和Deployments副本控制器不一样，其不支持修改Jobs的yaml文件，如果有需要修改则需要提前将Jobs任务删除，然后再将修改后的yaml提交任务。 2. CronJobs周期性运转 ","date":"2019-08-04","objectID":"/13-%E4%B8%80%E6%AC%A1%E6%80%A7%E4%BB%BB%E5%8A%A1job%E5%92%8C%E5%91%A8%E6%9C%9F%E4%BB%BB%E5%8A%A1/:4:0","tags":["kubernetes"],"title":"13 一次性任务Job和周期任务","uri":"/13-%E4%B8%80%E6%AC%A1%E6%80%A7%E4%BB%BB%E5%8A%A1job%E5%92%8C%E5%91%A8%E6%9C%9F%E4%BB%BB%E5%8A%A1/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.1 CronJobs简介 CronJobs用于实现类似Linux下的cronjob周期性计划任务，CronJobs控制器通过时间线创建Jobs任务，从而完成任务的执行处理，其具有如下特点： 实现周期性计划任务 调用Jobs控制器创建任务 CronJobs任务名称小于52个字符 应用场景如：定期备份，周期性发送邮件 CronJobs可通过schedule指定任务运行的周期，其使用参数和cronjob类似，分别使用：分时日月星5个参数表示周期性，其中*表示任意时间点，/表示每隔多久，-表示范围 分钟 范围为0-59 小时 范围为0-23 日期 范围为1-31 月份 范围为1-12 星期 范围为0-7，其中0和7表示星期日 举例子说明： 1、 /1 * * * 表示每隔1分钟运行任务 2、 1 0 * * 6-7 表示每周六日的0点01分运行任务 ","date":"2019-08-04","objectID":"/13-%E4%B8%80%E6%AC%A1%E6%80%A7%E4%BB%BB%E5%8A%A1job%E5%92%8C%E5%91%A8%E6%9C%9F%E4%BB%BB%E5%8A%A1/:5:0","tags":["kubernetes"],"title":"13 一次性任务Job和周期任务","uri":"/13-%E4%B8%80%E6%AC%A1%E6%80%A7%E4%BB%BB%E5%8A%A1job%E5%92%8C%E5%91%A8%E6%9C%9F%E4%BB%BB%E5%8A%A1/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.2 运行Cronjobs任务 CronJobs任务是编写和Deployments类似，需啊哟一个schedule定期任务调度周期，通过jobTemplate定义生成Jobs任务的模版，定义一个任务为例： 1、 定义一个CronJobs任务，每隔5分钟运行一个任务 apiVersion: batch/v1beta1 kind: CronJob metadata: name: cronjob-demo labels: jobgroup: cronjob-demo spec: schedule: \"*/5 * * * *\" #调度任务周期 jobTemplate: #创建Jobs任务模版 spec: template: spec: containers: - name: cronjob-demo image: busybox:latest imagePullPolicy: IfNotPresent command: - /bin/sh - -c - \"for i in `seq 0 100`;do echo ${i} \u0026\u0026 sleep 1;done\" restartPolicy: Never 2、 运行CronJobs并查看任务列表 3、校验CronJobs任务运行的情况，可以看到CronJobs任务调用Jobs控制器创建Pod，Pod创建周期和schedule中定义的周期一致 当然，CronJobs中通过Jobs的模版也可以定义运行任务的数量和并发数，实现计划时间范围内并发运行多个任务的需求。 写在最后 文章总结了在kubernetes集群中运行Jobs批处理任务和CronJobs两种控制器的功能使用，适用于特定场景下任务，Jobs任务执行完毕即completed，CronJobs周期性调用Jobs控制器完成任务的创建执行。 参考文章 不错的博客：https://draveness.me/kubernetes-job-cronjob 运行Jobs任务：https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/ 计划任务ConJobs:https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/ 自动运行任务：https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/ TKE创建Jobs任务：https://cloud.tencent.com/document/product/457/31708 TKE创建CronJobs：[https://cloud.tencent.com/document/product/457/31709 『 转载 』该文章来源于网络，侵删。 ","date":"2019-08-04","objectID":"/13-%E4%B8%80%E6%AC%A1%E6%80%A7%E4%BB%BB%E5%8A%A1job%E5%92%8C%E5%91%A8%E6%9C%9F%E4%BB%BB%E5%8A%A1/:6:0","tags":["kubernetes"],"title":"13 一次性任务Job和周期任务","uri":"/13-%E4%B8%80%E6%AC%A1%E6%80%A7%E4%BB%BB%E5%8A%A1job%E5%92%8C%E5%91%A8%E6%9C%9F%E4%BB%BB%E5%8A%A1/"},{"categories":["转载","kubernetes","基础教程"],"content":"写在前面 本章介绍kubernetes系列教程的ingress概念，在kubernetes中对外暴露服务的方式有两种：service（NodePort或者外部LoadBalancer）和ingress，其中service是提供四层的负载均衡，通过iptables DNAT或lvs nat模式实现后端Pod的代理请求。如需实现http，域名，URI，证书等请求方式，service是无法实现的，需要借助于ingress来来实现，本文将来介绍ingress相关的内容。 1. Ingress简介 An API object that manages external access to the services in a cluster, typically HTTP. Ingress can provide load balancing, SSL termination and name-based virtual hosting. 引用官方关于ingress的介绍我们可以得知，ingress是一种通过http协议暴露kubernetes内部服务的api对象，即充当Edge Router边界路由器的角色对外基于七层的负载均衡调度机制，能够提供以下几个功能： 负载均衡，将请求自动负载均衡到后端的Pod上； SSL加密，客户端到Ingress Controller为https加密，到后端Pod为明文的http； 基于名称的虚拟主机，提供基于域名或URI更灵活的路由方式 实现Ingress包含的组件有： Ingress，客户端，负责定义ingress配置，将请求转发给Ingress Controller； Ingress Controller，Ingress控制器，实现七层转发的Edge Router，通过调用k8s的api动态感知集群中Pod的变化而动态更新配置文件并重载， Controller需要部署在k8s集群中以实现和集群中的pod通信，通常以DaemonSets或Deployments的形式部署，并对外暴露80和443端口，对于DaemonSets来说，一般是以hostNetwork或者hostPort的形式暴露，Deployments则以NodePort的方式暴露，控制器的多个节点则借助外部负载均衡ExternalLB以实现统一接入； Ingress配置规则，Controller控制器通过service服务发现机制动态实现后端Pod路由转发规则的实现； Service，kuberntes中四层的负载均衡调度机制，Ingress借助service的服务发现机制实现集群中Pod资源的动态感知； Pod，后端实际负责响应请求容器，由控制器如Deployment创建，通过标签Labels和service关联，服务发现。 简而言之，ingress控制器借助service的服务发现机制实现配置的动态更新以实现Pod的负载均衡机制实现，由于涉及到Ingress Controller的动态更新，目前社区Ingress Controller大体包含两种类型的控制器： 传统的七层负载均衡如Nginx，HAproxy，开发了适应微服务应用的插件，具有成熟，高性能等优点； 新型微服务负载均衡如Traefik，Envoy，Istio，专门适用于微服务+容器化应用场景，具有动态更新特点； 类型 常见类型 优点 缺点 传统负载均衡 nginx，haproxy 成熟，稳定，高性能 动态更新需reload配置文件 微服务负载均衡 Traefik，Envoy，Istio 天生为微服务而生，动态更新 性能还有待提升 2. Nginx Ingress ","date":"2019-08-04","objectID":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:0:0","tags":["kubernetes"],"title":"14 基于nginx Ingress实现服务暴露","uri":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.1 Nginx ingress介绍 By default, pods of Kubernetes services are not accessible from the external network, but only by other pods within the Kubernetes cluster. Kubernetes has a built‑in configuration for HTTP load balancing, called Ingress, that defines rules for external connectivity to Kubernetes services. Users who need to provide external access to their Kubernetes services create an Ingress resource that defines rules, including the URI path, backing service name, and other information. The Ingress controller can then automatically program a frontend load balancer to enable Ingress configuration. The NGINX Ingress Controller for Kubernetes is what enables Kubernetes to configure NGINX and NGINX Plus for load balancing Kubernetes services. Nginx Ingress Controller是实现ingress的具体实现，包含有两个版本：Ngnix OSS和Nginx Plus版，后者是商业化增强版，支持更多的功能，详情参考官方文档介绍https://www.nginx.com/products/nginx/kubernetes-ingress-controller#compare-versions ","date":"2019-08-04","objectID":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:1:0","tags":["kubernetes"],"title":"14 基于nginx Ingress实现服务暴露","uri":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.2 Nginx ingress安装 首先需要安装Nginx Ingress Controller控制器，控制器安装方式包含两种：DaemonSets和Deployments。 DaemonSets通过hostPort的方式暴露80和443端口，可通过Node的调度由专门的节点实现部署 Deployments则通过NodePort的方式实现控制器端口的暴露，借助外部负载均衡实现高可用负载均衡 除此之外，还需要部署Namespace，ServiceAccount，RBAC，Secrets，Custom Resource Definitions等资源，如下开始部署。 ","date":"2019-08-04","objectID":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:2:0","tags":["kubernetes"],"title":"14 基于nginx Ingress实现服务暴露","uri":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.2.1 基础依赖环境准备 1、github中下载源码包,安装部署文件在kubernetes-ingress/deployments/目录下 [root@node-1 ~]# git clone https://github.com/nginxinc/kubernetes-ingress.git [root@node-1 ~]# tree kubernetes-ingress/deployments/ kubernetes-ingress/deployments/ ├── common │ ├── custom-resource-definitions.yaml 自定义资源 │ ├── default-server-secret.yaml Secrets │ ├── nginx-config.yaml │ └── ns-and-sa.yaml Namspace+ServiceAccount ├── daemon-set │ ├── nginx-ingress.yaml DaemonSets控制器 │ └── nginx-plus-ingress.yaml ├── deployment │ ├── nginx-ingress.yaml Deployments控制器 │ └── nginx-plus-ingress.yaml ├── helm-chart Helm安装包 │ ├── chart-icon.png │ ├── Chart.yaml │ ├── README.md │ ├── templates │ │ ├── controller-configmap.yaml │ │ ├── controller-custom-resources.yaml │ │ ├── controller-daemonset.yaml │ │ ├── controller-deployment.yaml │ │ ├── controller-leader-election-configmap.yaml │ │ ├── controller-secret.yaml │ │ ├── controller-serviceaccount.yaml │ │ ├── controller-service.yaml │ │ ├── controller-wildcard-secret.yaml │ │ ├── _helpers.tpl │ │ ├── NOTES.txt │ │ └── rbac.yaml │ ├── values-icp.yaml │ ├── values-plus.yaml │ └── values.yaml ├── rbac RBAC认证授权 │ └── rbac.yaml ├── README.md └── service Service定义 ├── loadbalancer-aws-elb.yaml ├── loadbalancer.yaml DaemonSets暴露服务方式 └── nodeport.yaml Deployments暴露服务方式 2、创建Namespace和ServiceAccount, kubectl apply -f common/default-server-secret.yaml apiVersion: v1 kind: Namespace metadata: name: nginx-ingress --- apiVersion: v1 kind: ServiceAccount metadata: name: nginx-ingress namespace: nginx-ingress 3、创建Secrets自签名证书，kubectl apply -f common/default-server-secret.yaml apiVersion: v1 kind: Secret metadata: name: default-server-secret namespace: nginx-ingress type: Opaque data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2akNDQWFZQ0NRREFPRjl0THNhWFhEQU5CZ2txaGtpRzl3MEJBUXNGQURBaE1SOHdIUVlEVlFRRERCWk8KUjBsT1dFbHVaM0psYzNORGIyNTBjbTlzYkdWeU1CNFhEVEU0TURreE1qRTRNRE16TlZvWERUSXpNRGt4TVRFNApNRE16TlZvd0lURWZNQjBHQTFVRUF3d1dUa2RKVGxoSmJtZHlaWE56UTI5dWRISnZiR3hsY2pDQ0FTSXdEUVlKCktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUwvN2hIUEtFWGRMdjNyaUM3QlBrMTNpWkt5eTlyQ08KR2xZUXYyK2EzUDF0azIrS3YwVGF5aGRCbDRrcnNUcTZzZm8vWUk1Y2Vhbkw4WGM3U1pyQkVRYm9EN2REbWs1Qgo4eDZLS2xHWU5IWlg0Rm5UZ0VPaStlM2ptTFFxRlBSY1kzVnNPazFFeUZBL0JnWlJVbkNHZUtGeERSN0tQdGhyCmtqSXVuektURXUyaDU4Tlp0S21ScUJHdDEwcTNRYzhZT3ExM2FnbmovUWRjc0ZYYTJnMjB1K1lYZDdoZ3krZksKWk4vVUkxQUQ0YzZyM1lma1ZWUmVHd1lxQVp1WXN2V0RKbW1GNWRwdEMzN011cDBPRUxVTExSakZJOTZXNXIwSAo1TmdPc25NWFJNV1hYVlpiNWRxT3R0SmRtS3FhZ25TZ1JQQVpQN2MwQjFQU2FqYzZjNGZRVXpNQ0F3RUFBVEFOCkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWpLb2tRdGRPcEsrTzhibWVPc3lySmdJSXJycVFVY2ZOUitjb0hZVUoKdGhrYnhITFMzR3VBTWI5dm15VExPY2xxeC9aYzJPblEwMEJCLzlTb0swcitFZ1U2UlVrRWtWcitTTFA3NTdUWgozZWI4dmdPdEduMS9ienM3bzNBaS9kclkrcUI5Q2k1S3lPc3FHTG1US2xFaUtOYkcyR1ZyTWxjS0ZYQU80YTY3Cklnc1hzYktNbTQwV1U3cG9mcGltU1ZmaXFSdkV5YmN3N0NYODF6cFErUyt1eHRYK2VBZ3V0NHh3VlI5d2IyVXYKelhuZk9HbWhWNThDd1dIQnNKa0kxNXhaa2VUWXdSN0diaEFMSkZUUkk3dkhvQXprTWIzbjAxQjQyWjNrN3RXNQpJUDFmTlpIOFUvOWxiUHNoT21FRFZkdjF5ZytVRVJxbStGSis2R0oxeFJGcGZnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= tls.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcEFJQkFBS0NBUUVBdi91RWM4b1JkMHUvZXVJTHNFK1RYZUprckxMMnNJNGFWaEMvYjVyYy9XMlRiNHEvClJOcktGMEdYaVN1eE9ycXgrajlnamx4NXFjdnhkenRKbXNFUkJ1Z1B0ME9hVGtIekhvb3FVWmcwZGxmZ1dkT0EKUTZMNTdlT1l0Q29VOUZ4amRXdzZUVVRJVUQ4R0JsRlNjSVo0b1hFTkhzbysyR3VTTWk2Zk1wTVM3YUhudzFtMApxWkdvRWEzWFNyZEJ6eGc2clhkcUNlUDlCMXl3VmRyYURiUzc1aGQzdUdETDU4cGszOVFqVUFQaHpxdmRoK1JWClZGNGJCaW9CbTVpeTlZTW1hWVhsMm0wTGZzeTZuUTRRdFFzdEdNVWozcGJtdlFmazJBNnljeGRFeFpkZFZsdmwKMm82MjBsMllxcHFDZEtCRThCay90elFIVTlKcU56cHpoOUJUTXdJREFRQUJBb0lCQVFDZklHbXowOHhRVmorNwpLZnZJUXQwQ0YzR2MxNld6eDhVNml4MHg4Mm15d1kxUUNlL3BzWE9LZlRxT1h1SENyUlp5TnUvZ2IvUUQ4bUFOCmxOMjRZTWl0TWRJODg5TEZoTkp3QU5OODJDeTczckM5bzVvUDlkazAvYzRIbjAzSkVYNzZ5QjgzQm9rR1FvYksKMjhMNk0rdHUzUmFqNjd6Vmc2d2szaEhrU0pXSzBwV1YrSjdrUkRWYmhDYUZhNk5nMUZNRWxhTlozVDhhUUtyQgpDUDNDeEFTdjYxWTk5TEI4KzNXWVFIK3NYaTVGM01pYVNBZ1BkQUk3WEh1dXFET1lvMU5PL0JoSGt1aVg2QnRtCnorNTZud2p","date":"2019-08-04","objectID":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:2:1","tags":["kubernetes"],"title":"14 基于nginx Ingress实现服务暴露","uri":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.2.2 部署Ingress控制器 1、 部署控制器，控制器可以DaemonSets和Deployment的形式部署，如下是DaemonSets的配置文件 apiVersion: apps/v1 kind: DaemonSet metadata: name: nginx-ingress namespace: nginx-ingress spec: selector: matchLabels: app: nginx-ingress template: metadata: labels: app: nginx-ingress #annotations: #prometheus.io/scrape: \"true\" #prometheus.io/port: \"9113\" spec: serviceAccountName: nginx-ingress containers: - image: nginx/nginx-ingress:edge imagePullPolicy: Always name: nginx-ingress ports: - name: http containerPort: 80 hostPort: 80 #通过hostPort的方式暴露端口 - name: https containerPort: 443 hostPort: 443 #- name: prometheus #containerPort: 9113 securityContext: allowPrivilegeEscalation: true runAsUser: 101 #nginx capabilities: drop: - ALL add: - NET_BIND_SERVICE env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name args: - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret #- -v=3 # Enables extensive logging. Useful for troubleshooting. #- -report-ingress-status #- -external-service=nginx-ingress #- -enable-leader-election #- -enable-prometheus-metrics Deployments的配置文件 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-ingress namespace: nginx-ingress spec: replicas: 1 #副本的个数 selector: matchLabels: app: nginx-ingress template: metadata: labels: app: nginx-ingress #annotations: #prometheus.io/scrape: \"true\" #prometheus.io/port: \"9113\" spec: serviceAccountName: nginx-ingress containers: - image: nginx/nginx-ingress:edge imagePullPolicy: Always name: nginx-ingress ports: #内部暴露的服务端口，需要通过NodePort的方式暴露给外部 - name: http containerPort: 80 - name: https containerPort: 443 #- name: prometheus #containerPort: 9113 securityContext: allowPrivilegeEscalation: true runAsUser: 101 #nginx capabilities: drop: - ALL add: - NET_BIND_SERVICE env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name args: - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret #- -v=3 # Enables extensive logging. Useful for troubleshooting. #- -report-ingress-status #- -external-service=nginx-ingress #- -enable-leader-election #- -enable-prometheus-metrics 2、我们以DaemonSets的方式部署，DaemonSet部署集群中各个节点都是对等，如果有外部LoadBalancer则通过外部负载均衡路由至Ingress中 [root@node-1 deployments]# kubectl apply -f daemon-set/nginx-ingress.yaml daemonset.apps/nginx-ingress created [root@node-1 deployments]# kubectl get daemonsets -n nginx-ingress NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE nginx-ingress 3 3 3 3 3 \u003cnone\u003e 15s [root@node-1 ~]# kubectl get pods -n nginx-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-ingress-7mpfc 1/1 Running 0 2m44s 10.244.0.50 node-1 \u003cnone\u003e \u003cnone\u003e nginx-ingress-l2rtj 1/1 Running 0 2m44s 10.244.1.144 node-2 \u003cnone\u003e \u003cnone\u003e nginx-ingress-tgf6r 1/1 Running 0 2m44s 10.244.2.160 node-3 \u003cnone\u003e \u003cnone\u003e 3、校验Nginx Ingress安装情况，此时三个节点均是对等，即访问任意一个节点均能实现相同的效果，统一入口则通过外部负载均衡，如果在云环境下执行kubectl apply -f service/loadbalancer.yaml创建外部负载均衡实现入口调度，自建的可以通过lvs或nginx等负载均衡实现接入，本文不再赘述，读者可以自行研究。 备注说明：如果以Deployments的方式部署，则需要执行service/nodeport.yaml创建NodePort类型的Service，实现的效果和DaemonSets类似。 3. Ingress资源定义 上面的章节已安装了一个Nginx Ingress Controller控制器，有了Ingress控制器后，我们就可以定义Ingress资源来实现七层负载转发了，大体上Ingress支持三种使用方式：1. 基于虚拟主机转发，2. 基于虚拟机主机URI转发，3. 支持TLS加密转发。 ","date":"2019-08-04","objectID":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:2:2","tags":["kubernetes"],"title":"14 基于nginx Ingress实现服务暴露","uri":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"3.1 Ingress定义 1、环境准备，先创建一个nginx的Deployment应用，包含2个副本 [root@node-1 ~]# kubectl run ingress-demo --image=nginx:1.7.9 --port=80 --replicas=2 [root@node-1 ~]# kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE ingress-demo 2/2 2 2 116s 2、以service方式暴露服务端口 [root@node-1 ~]# kubectl expose deployment ingress-demo --port=80 --protocol=TCP --target-port=80 service/ingress-demo exposed [root@node-1 ~]# kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-demo ClusterIP 10.109.33.91 \u003cnone\u003e 80/TCP 2m15s 3、上述两个步骤已创建了一个service，如下我们定义一个ingress对象将起转发至ingress-demo这个service，通过ingress.class指定控制器的类型为nginx [root@node-1 nginx-ingress]# cat nginx-ingress-demo.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress-demo labels: ingres-controller: nginx annotations: kubernets.io/ingress.class: nginx spec: rules: - host: www.happylau.cn http: paths: - path: / backend: serviceName: ingress-demo servicePort: 80 4、创建ingress对象 [root@node-1 nginx-ingress]# kubectl apply -f nginx-ingress-demo.yaml ingress.extensions/nginx-ingress-demo created 查看ingress资源列表 [root@node-1 nginx-ingress]# kubectl get ingresses NAME HOSTS ADDRESS PORTS AGE nginx-ingress-demo www.happylau.cn 80 4m4s 5、查看ingress详情，可以在Rules规则中看到后端Pod的列表，自动发现和关联相关Pod [root@node-1 ~]# kubectl describe ingresses nginx-ingress-demo Name: nginx-ingress-demo Namespace: default Address: Default backend: default-http-backend:80 (\u003cnone\u003e) Rules: Host Path Backends ---- ---- -------- www.happylau.cn / ingress-demo:80 (10.244.1.146:80,10.244.2.162:80) Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"extensions/v1beta1\",\"kind\":\"Ingress\",\"metadata\":{\"annotations\":{\"kubernets.io/ingress.class\":\"nginx\"},\"labels\":{\"ingres-controller\":\"nginx\"},\"name\":\"nginx-ingress-demo\",\"namespace\":\"default\"},\"spec\":{\"rules\":[{\"host\":\"www.happylaulab.cn\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"ingress-demo\",\"servicePort\":80},\"path\":\"/\"}]}}]}} kubernets.io/ingress.class: nginx Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal AddedOrUpdated 9m7s nginx-ingress-controller Configuration for default/nginx-ingress-demo was added or updated Normal AddedOrUpdated 9m7s nginx-ingress-controller Configuration for default/nginx-ingress-demo was added or updated Normal AddedOrUpdated 9m7s nginx-ingress-controller Configuration for default/nginx-ingress-demo was added or updated 6、测试验证，ingress规则的配置信息已注入到Ingress Controller中，环境中Ingress Controller是以DaemonSets的方式部署在集群中，如果有外部的负载均衡，则将www.happylau.cn域名的地址解析为负载均衡VIP。由于测试环境没有搭建负载均衡，将hosts解析执行node-1，node-2或者node-3任意一个IP都能实现相同的功能。 上述测试解析正常，当然也可以解析为node-1和node-2的IP，如下： [root@node-1 ~]# curl -I http://www.happylau.cn --resolve www.happylau.cn:80:10.254.100.101 HTTP/1.1 200 OK Server: nginx/1.17.6 Date: Tue, 24 Dec 2019 10:32:22 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Last-Modified: Tue, 23 Dec 2014 16:25:09 GMT ETag: \"54999765-264\" Accept-Ranges: bytes [root@node-1 ~]# curl -I http://www.happylau.cn --resolve www.happylau.cn:80:10.254.100.102 HTTP/1.1 200 OK Server: nginx/1.17.6 Date: Tue, 24 Dec 2019 10:32:24 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Last-Modified: Tue, 23 Dec 2014 16:25:09 GMT ETag: \"54999765-264\" Accept-Ranges: bytes ","date":"2019-08-04","objectID":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:3:0","tags":["kubernetes"],"title":"14 基于nginx Ingress实现服务暴露","uri":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"3.2 Ingress动态配置 上面的章节介绍了ingress资源对象的申明配置，在这个章节中我们探究一下Nginx Ingress Controller的实现机制和动态配置更新机制，以方便了解Ingress控制器的工作机制。 1、 查看Nginx Controller控制器的配置文件，在nginx-ingress pod中存储着ingress的配置文件 [root@node-1 ~]# kubectl get pods -n nginx-ingress NAME READY STATUS RESTARTS AGE nginx-ingress-7mpfc 1/1 Running 0 6h15m nginx-ingress-l2rtj 1/1 Running 0 6h15m nginx-ingress-tgf6r 1/1 Running 0 6h15m #查看配置文件，每个ingress生成一个配置文件，文件名为：命名空间-ingres名称.conf [root@node-1 ~]# kubectl exec -it nginx-ingress-7mpfc -n nginx-ingress -- ls -l /etc/nginx/conf.d total 4 -rw-r--r-- 1 nginx nginx 1005 Dec 24 10:06 default-nginx-ingress-demo.conf #查看配置文件 [root@node-1 ~]# kubectl exec -it nginx-ingress-7mpfc -n nginx-ingress -- cat /etc/nginx/conf.d/default-nginx-ingress-demo.conf # configuration for default/nginx-ingress-demo #upstream的配置，会用least_conn算法，通过service服务发现机制动态识别到后端的Pod upstream default-nginx-ingress-demo-www.happylau.cn-ingress-demo-80 { zone default-nginx-ingress-demo-www.happylau.cn-ingress-demo-80 256k; random two least_conn; server 10.244.1.146:80 max_fails=1 fail_timeout=10s max_conns=0; server 10.244.2.162:80 max_fails=1 fail_timeout=10s max_conns=0; } server { listen 80; server_tokens on; server_name www.happylau.cn; location / { proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; client_max_body_size 1m; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering on; proxy_pass http://default-nginx-ingress-demo-www.happylau.cn-ingress-demo-80; #调用upstream实现代理 } } 通过上述查看配置文件可得知，Nginx Ingress Controller实际是根据ingress规则生成对应的nginx配置文件，以实现代理转发的功能，加入Deployments的副本数变更后nginx的配置文件会发生什么改变呢？ 2、更新控制器的副本数，由2个Pod副本扩容至3个 [root@node-1 ~]# kubectl scale --replicas=3 deployment ingress-demo deployment.extensions/ingress-demo scaled [root@node-1 ~]# kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE ingress-demo 3/3 3 3 123m 3、再次查看nginx的配置文件，ingress借助于service的服务发现机制，将加入的Pod自动加入到nginx upstream中 4、查看nginx pod的日志（kubectl logs nginx-ingress-7mpfc -n nginx-ingress），有reload优雅重启的记录，即通过更新配置文件+reload实现配置动态更新。 通过上述的配置可知，ingress调用kubernetes api去感知kubernetes集群中的变化情况，Pod的增加或减少这些变化，然后动态更新nginx ingress controller的配置文件，并重新载入配置。当集群规模越大时，会频繁涉及到配置文件的变动和重载，因此nginx这方面会存在先天的劣势，专门为微服务负载均衡应运而生，如Traefik，Envoy，Istio，这些负载均衡工具能够提供大规模，频繁动态更新的场景，但性能相比Nginx，HAproxy还存在一定的劣势。往后的章节中，我们再对其他的Ingress控制器做介绍。 ","date":"2019-08-04","objectID":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:4:0","tags":["kubernetes"],"title":"14 基于nginx Ingress实现服务暴露","uri":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"3.3 Ingress路径转发 Ingress支持URI格式的转发方式，同时支持URL重写，如下以两个service为例演示，service-1安装nginx，service-2安装httpd，分别用http://demo.happylau.cn/news和http://demo.happylau.cn/sports转发到两个不同的service 1、环境准备，创建两个应用并实现service暴露，创建deployments时指定–explose创建service [root@node-1 ~]# kubectl run service-1 --image=nginx:1.7.9 --port=80 --replicas=1 --expose=true service/service-1 created deployment.apps/service-1 created [root@node-1 ~]# kubectl run service-2 --image=httpd --port=80 --replicas=1 --expose=true service/service-2 created deployment.apps/service-2 created 查看deployment状态 [root@node-1 ~]# kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE ingress-demo 4/4 4 4 4h36m service-1 1/1 1 1 65s service-2 1/1 1 1 52s 查看service状态，服务已经正常 [root@node-1 ~]# kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-demo ClusterIP 10.109.33.91 \u003cnone\u003e 80/TCP 4h36m kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 101d service-1 ClusterIP 10.106.245.71 \u003cnone\u003e 80/TCP 68s service-2 ClusterIP 10.104.204.158 \u003cnone\u003e 80/TCP 55s 2、创建ingress对象，通过一个域名将请求转发至后端两个service [root@node-1 nginx-ingress]# cat nginx-ingress-uri-demo.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress-uri-demo labels: ingres-controller: nginx annotations: kubernets.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: demo.happylau.cn http: paths: - path: /news backend: serviceName: service-1 servicePort: 80 - path: /sports backend: serviceName: service-2 servicePort: 80 3、创建ingress规则，查看详情 [root@node-1 nginx-ingress]# kubectl apply -f nginx-ingress-uri-demo.yaml ingress.extensions/nginx-ingress-uri-demo created #查看详情 [root@node-1 nginx-ingress]# kubectl get ingresses. NAME HOSTS ADDRESS PORTS AGE nginx-ingress-demo www.happylau.cn 80 4h35m nginx-ingress-uri-demo demo.happylau.cn 80 4s [root@node-1 nginx-ingress]# kubectl describe ingresses nginx-ingress-uri-demo Name: nginx-ingress-uri-demo Namespace: default Address: Default backend: default-http-backend:80 (\u003cnone\u003e) Rules: #对应的转发url规则 Host Path Backends ---- ---- -------- demo.happylau.cn /news service-1:80 (10.244.2.163:80) /sports service-2:80 (10.244.1.148:80) Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"extensions/v1beta1\",\"kind\":\"Ingress\",\"metadata\":{\"annotations\":{\"kubernets.io/ingress.class\":\"nginx\",\"nginx.ingress.kubernetes.io/rewrite-target\":\"/\"},\"labels\":{\"ingres-controller\":\"nginx\"},\"name\":\"nginx-ingress-uri-demo\",\"namespace\":\"default\"},\"spec\":{\"rules\":[{\"host\":\"demo.happylau.cn\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"service-1\",\"servicePort\":80},\"path\":\"/news\"},{\"backend\":{\"serviceName\":\"service-2\",\"servicePort\":80},\"path\":\"/sports\"}]}}]}} kubernets.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: / Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal AddedOrUpdated 11s nginx-ingress-controller Configuration for default/nginx-ingress-uri-demo was added or updated Normal AddedOrUpdated 11s nginx-ingress-controller Configuration for default/nginx-ingress-uri-demo was added or updated Normal AddedOrUpdated 11s nginx-ingress-controller Configuration for default/nginx-ingress-uri-demo was added or updated 4、准备测试，站点中创建对应的路径 [root@node-1 ~]# kubectl exec -it service-1-7b66bf758f-xj9jh /bin/bash root@service-1-7b66bf758f-xj9jh:/# echo \"service-1 website page\" \u003e/usr/share/nginx/html/news [root@node-1 ~]# kubectl exec -it service-2-7c7444684d-w9cv9 /bin/bash root@service-2-7c7444684d-w9cv9:/usr/local/apache2# echo \"service-2 website page\" \u003e/usr/local/apache2/htdocs/sports 5、测试验证 [root@node-1 ~]# curl http://demo.happylau.cn/news --resolve demo.happylau.cn:80:10.254.100.101 service-1 website page [root@node-1 ~]# curl http://demo.happylau.cn/sports --resolve demo.happylau.cn:80:10.254.100.101 service-2 website page 6、通过上述的验证测试可以得知，ingress支持URI的路由方式转发，其对应在ingress中的配置文件内容是怎样的呢，我们看下ingress controller生成对应的nginx配置文件内容，实际是通过ingress的location来实现，将不同的localtion转发至不同的upstream以实现service的关联，配置文件如下： [roo","date":"2019-08-04","objectID":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:5:0","tags":["kubernetes"],"title":"14 基于nginx Ingress实现服务暴露","uri":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"3.4 Ingress虚拟主机 ingress支持基于名称的虚拟主机，实现单个IP多个域名转发的需求，通过请求头部携带主机名方式区分开，将上个章节的ingress删除，使用service-1和service-2两个service来做演示。 1、创建ingress规则，通过主机名实现转发规则 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress-virtualname-demo labels: ingres-controller: nginx annotations: kubernets.io/ingress.class: nginx spec: rules: - host: news.happylau.cn http: paths: - path: / backend: serviceName: service-1 servicePort: 80 - host: sports.happylau.cn http: paths: - path: / backend: serviceName: service-2 servicePort: 80 2、生成ingress规则并查看详情，一个ingress对应两个HOSTS [root@node-1 nginx-ingress]# kubectl apply -f nginx-ingress-virtualname.yaml ingress.extensions/nginx-ingress-virtualname-demo created #查看列表 [root@node-1 nginx-ingress]# kubectl get ingresses nginx-ingress-virtualname-demo NAME HOSTS ADDRESS PORTS AGE nginx-ingress-virtualname-demo news.happylau.cn,sports.happylau.cn 80 12s #查看详情 [root@node-1 nginx-ingress]# kubectl describe ingresses nginx-ingress-virtualname-demo Name: nginx-ingress-virtualname-demo Namespace: default Address: Default backend: default-http-backend:80 (\u003cnone\u003e) Rules: Host Path Backends ---- ---- -------- news.happylau.cn / service-1:80 (10.244.2.163:80) sports.happylau.cn / service-2:80 (10.244.1.148:80) Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"extensions/v1beta1\",\"kind\":\"Ingress\",\"metadata\":{\"annotations\":{\"kubernets.io/ingress.class\":\"nginx\"},\"labels\":{\"ingres-controller\":\"nginx\"},\"name\":\"nginx-ingress-virtualname-demo\",\"namespace\":\"default\"},\"spec\":{\"rules\":[{\"host\":\"news.happylau.cn\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"service-1\",\"servicePort\":80},\"path\":\"/\"}]}},{\"host\":\"sports.happylau.cn\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"service-2\",\"servicePort\":80},\"path\":\"/\"}]}}]}} kubernets.io/ingress.class: nginx Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal AddedOrUpdated 28s nginx-ingress-controller Configuration for default/nginx-ingress-virtualname-demo was added or updated Normal AddedOrUpdated 28s nginx-ingress-controller Configuration for default/nginx-ingress-virtualname-demo was added or updated Normal AddedOrUpdated 28s nginx-ingress-controller Configuration for default/nginx-ingress-virtualname-demo was added or updated 3、准备测试数据并测试 [root@node-1 ~]# kubectl exec -it service-1-7b66bf758f-xj9jh /bin/bash root@service-1-7b66bf758f-xj9jh:/# echo \"news demo\" \u003e/usr/share/nginx/html/index.html [root@node-1 ~]# kubectl exec -it service-2-7c7444684d-w9cv9 /bin/bash root@service-2-7c7444684d-w9cv9:/usr/local/apache2# echo \"sports demo\" \u003e/usr/local/apache2/htdocs/index.html 测试： [root@node-1 ~]# curl http://news.happylau.cn --resolve news.happylau.cn:80:10.254.100.102 news demo [root@node-1 ~]# curl http://sports.happylau.cn --resolve sports.happylau.cn:80:10.254.100.102 sports demo 4、查看nginx的配置文件内容，通过在server中定义不同的server_name以区分，代理到不同的upstream以实现service的代理。 # configuration for default/nginx-ingress-virtualname-demo upstream default-nginx-ingress-virtualname-demo-news.happylau.cn-service-1-80 { zone default-nginx-ingress-virtualname-demo-news.happylau.cn-service-1-80 256k; random two least_conn; server 10.244.2.163:80 max_fails=1 fail_timeout=10s max_conns=0; } upstream default-nginx-ingress-virtualname-demo-sports.happylau.cn-service-2-80 { zone default-nginx-ingress-virtualname-demo-sports.happylau.cn-service-2-80 256k; random two least_conn; server 10.244.1.148:80 max_fails=1 fail_timeout=10s max_conns=0; } server { listen 80; server_tokens on; server_name news.happylau.cn; location / { proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; client_max_body_size 1m; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering on; proxy_pass http://default-nginx-ingress-virtu","date":"2019-08-04","objectID":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:6:0","tags":["kubernetes"],"title":"14 基于nginx Ingress实现服务暴露","uri":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"3.5 Ingress TLS加密 四层的负载均衡无法支持https请求，当前大部分业务都要求以https方式接入，Ingress能支持https的方式接入，通过Secrets存储证书+私钥，实现https接入，同时还能支持http跳转功能。对于用户的请求流量来说，客户端到ingress controller是https流量，ingress controller到后端service则是http，提高用户访问性能，如下介绍ingress TLS功能实现步骤。 1、生成自签名证书和私钥 [root@node-1 ~]# openssl req -x509 -newkey rsa:2048 -nodes -days 365 -keyout tls.key -out tls.crt Generating a 2048 bit RSA private key ....................................................+++ ........................................+++ writing new private key to 'tls.key' ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [XX]:CN #国家 State or Province Name (full name) []:GD #省份 Locality Name (eg, city) [Default City]:ShenZhen #城市 Organization Name (eg, company) [Default Company Ltd]:Tencent #公司 Organizational Unit Name (eg, section) []:HappyLau #组织 Common Name (eg, your name or your server's hostname) []:www.happylau.cn #域名 Email Address []:573302346@qq.com #邮箱地址 #tls.crt为证书，tls.key为私钥 [root@node-1 ~]# ls tls.* -l -rw-r--r-- 1 root root 1428 12月 26 13:21 tls.crt -rw-r--r-- 1 root root 1708 12月 26 13:21 tls.key 2、配置Secrets，将证书和私钥配置到Secrets中 [root@node-1 ~]# kubectl create secret tls happylau-sslkey --cert=tls.crt --key=tls.key secret/happylau-sslkey created 查看Secrets详情,证书和私要包含在data中，文件名为两个不同的key：tls.crt和tls.key [root@node-1 ~]# kubectl describe secrets happylau-sslkey Name: happylau-sslkey Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Type: kubernetes.io/tls Data ==== tls.crt: 1428 bytes tls.key: 1708 bytes 3、配置ingress调用Secrets实现SSL证书加密 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress-tls-demo labels: ingres-controller: nginx annotations: kubernets.io/ingress.class: nginx spec: tls: - hosts: - news.happylau.cn - sports.happylau.cn secretName: happylau-sslkey rules: - host: news.happylau.cn http: paths: - path: / backend: serviceName: service-1 servicePort: 80 - host: sports.happylau.cn http: paths: - path: / backend: serviceName: service-2 servicePort: 80 4、创建ingress并查看ingress详情 [root@node-1 nginx-ingress]# kubectl describe ingresses nginx-ingress-tls-demo Name: nginx-ingress-tls-demo Namespace: default Address: Default backend: default-http-backend:80 (\u003cnone\u003e) TLS: happylau-sslkey terminates news.happylau.cn,sports.happylau.cn Rules: Host Path Backends ---- ---- -------- news.happylau.cn / service-1:80 (10.244.2.163:80) sports.happylau.cn / service-2:80 (10.244.1.148:80) Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"extensions/v1beta1\",\"kind\":\"Ingress\",\"metadata\":{\"annotations\":{\"kubernets.io/ingress.class\":\"nginx\"},\"labels\":{\"ingres-controller\":\"nginx\"},\"name\":\"nginx-ingress-tls-demo\",\"namespace\":\"default\"},\"spec\":{\"rules\":[{\"host\":\"news.happylau.cn\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"service-1\",\"servicePort\":80},\"path\":\"/\"}]}},{\"host\":\"sports.happylau.cn\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"service-2\",\"servicePort\":80},\"path\":\"/\"}]}}],\"tls\":[{\"hosts\":[\"news.happylau.cn\",\"sports.happylau.cn\"],\"secretName\":\"happylau-sslkey\"}]}} kubernets.io/ingress.class: nginx Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal AddedOrUpdated 22s nginx-ingress-controller Configuration for default/nginx-ingress-tls-demo was added or updated Normal AddedOrUpdated 22s nginx-ingress-controller Configuration for default/nginx-ingress-tls-demo was added or updated Normal AddedOrUpdated 22s nginx-ingress-controller Configuration for default/nginx-ingress-tls-demo was added or updated 5、 将news.happylau.cn和sports.happylau.cn写入到hosts文件中，并通过https://news.happylau.cn 的方式访问，浏览器访问内容提示证书如下，信任证书即可访问到站点内容。 查看证书详情，正是我们制作的自签名证书，生产实际使用时，推荐使用CA机构颁发签名证书。 6、接下来查看一下tls配置https的nginx配置文件内容，可以看到在server块启用了https","date":"2019-08-04","objectID":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:7:0","tags":["kubernetes"],"title":"14 基于nginx Ingress实现服务暴露","uri":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"4.1 定制化参数 ingress controller提供了基础反向代理的功能，如果需要定制化nginx的特性或参数，需要通过ConfigMap和Annotations来实现，两者实现的方式有所不同，ConfigMap用于指定整个ingress集群资源的基本参数，修改后会被所有的ingress对象所继承；Annotations则被某个具体的ingress对象所使用，修改只会影响某个具体的ingress资源，冲突时其优先级高于ConfigMap。 ","date":"2019-08-04","objectID":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:8:0","tags":["kubernetes"],"title":"14 基于nginx Ingress实现服务暴露","uri":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"4.1.1 ConfigMap自定义参数 安装nginx ingress controller时默认会包含一个空的ConfigMap，可以通过ConfigMap来自定义nginx controller的默认参数，如下以修改一些参数为例： 1、 定义ConfigMap参数 kind: ConfigMap apiVersion: v1 metadata: name: nginx-config namespace: nginx-ingress data: proxy-connect-timeout: \"10s\" proxy-read-timeout: \"10s\" proxy-send-timeout: \"10\" client-max-body-size: \"3m\" 2、 应用配置并查看ConfigMap配置 [root@node-1 ~]# kubectl get configmaps -n nginx-ingress nginx-config -o yaml apiVersion: v1 data: client-max-body-size: 3m proxy-connect-timeout: 10s proxy-read-timeout: 10s proxy-send-timeout: 10s kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"v1\",\"data\":{\"client-max-body-size\":\"3m\",\"proxy-connect-timeout\":\"10s\",\"proxy-read-timeout\":\"10s\",\"proxy-send-timeout\":\"10\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"name\":\"nginx-config\",\"namespace\":\"nginx-ingress\"}} creationTimestamp: \"2019-12-24T04:39:23Z\" name: nginx-config namespace: nginx-ingress resourceVersion: \"13845543\" selfLink: /api/v1/namespaces/nginx-ingress/configmaps/nginx-config uid: 9313ae47-a0f0-463e-a25a-1658f1ca0d57 3 、此时，ConfigMap定义的配置参数会被集群中所有的Ingress资源继承（除了annotations定义之外） 有很多参数可以定义，详情配置可参考方文档说明：https://github.com/nginxinc/kubernetes-ingress/blob/master/docs/configmap-and-annotations.md#Summary-of-ConfigMap-and-Annotations ","date":"2019-08-04","objectID":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:8:1","tags":["kubernetes"],"title":"14 基于nginx Ingress实现服务暴露","uri":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"4.1.2 Annotations自定义参数 ConfigMap定义的是全局的配置参数，修改后所有的配置都会受影响，如果想针对某个具体的ingress资源自定义参数，则可以通过Annotations来实现，下面开始以实际的例子演示Annotations的使用。 1、修改ingress资源，添加annotations的定义,通过nginx.org组修改了一些参数，如proxy-connect-timeout，调度算法为round_robin（默认为least _conn） apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress-demo labels: ingres-controller: nginx annotations: kubernets.io/ingress.class: nginx nginx.org/proxy-connect-timeout: \"30s\" nginx.org/proxy-send-timeout: \"20s\" nginx.org/proxy-read-timeout: \"20s\" nginx.org/client-max-body-size: \"2m\" nginx.org/fail-timeout: \"5s\" nginx.org/lb-method: \"round_robin\" spec: rules: - host: www.happylau.cn http: paths: - path: / backend: serviceName: ingress-demo servicePort: 80 2、 重新应用ingress对象并查看参数配置情况 由上面的演示可得知，Annotations的优先级高于ConfigMapMap，Annotations修改参数只会影响到某一个具体的ingress资源，其定义的方法和ConfigMap相相近似，但又有差别，部分ConfigMap的参数Annotations无法支持，反过来Annotations定义的参数ConfigMap也不一定支持，下图列举一下常规支持参数情况： ConfigMap和Annotations详细支持说明：链接说明 ","date":"2019-08-04","objectID":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:8:2","tags":["kubernetes"],"title":"14 基于nginx Ingress实现服务暴露","uri":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"4.2 虚拟主机和路由 安装nginx ingress时我们安装了一个customresourcedefinitions自定义资源，其能够提供除了默认ingress功能之外的一些高级特性如 虚拟主机VirtualServer 虚拟路由VirtualServerRoute 健康检查Healthcheck 流量切割Split 会话保持SessionCookie 重定向Redirect 这些功能大部分依赖于Nginx Plus高级版本的支持，社区版本仅支持部分，对于企业级开发而言，丰富更多的功能可以购买企业级Nginx Plus版本。如下以通过VirtualServer和VirtualServerRoute定义upstream配置为例演示功能使用。 1、定义VirtualServer资源,其配置和ingress资源对象类似，能支持的功能会更丰富一点 apiVersion: k8s.nginx.org/v1 kind: VirtualServer metadata: name: cafe spec: host: cafe.example.com tls: secret: cafe-secret upstreams: - name: tea service: tea-svc port: 80 name: tea service: ingress-demo subselector: version: canary lb-method: round_robin fail-timeout: 10s max-fails: 1 max-conns: 32 keepalive: 32 connect-timeout: 30s read-timeout: 30s send-timeout: 30s next-upstream: \"error timeout non_idempotent\" next-upstream-timeout: 5s next-upstream-tries: 10 client-max-body-size: 2m tls: enable: true routes: - path: /tea action: pass: tea 2、 应用资源并查看VirtualServer资源列表 [root@node-1 ~]# kubectl apply -f vs.yaml virtualserver.k8s.nginx.org/cafe unchanged [root@node-1 ~]# kubectl get virtualserver NAME AGE cafe 2m52s 3、检查ingress控制器的配置文件情况,生成的配置和upstream定义一致 nginx@nginx-ingress-7mpfc:/etc/nginx/conf.d$ cat vs_default_cafe.conf upstream vs_default_cafe_tea { zone vs_default_cafe_tea 256k; server 10.244.0.51:80 max_fails=1 fail_timeout=10s max_conns=32; server 10.244.1.146:80 max_fails=1 fail_timeout=10s max_conns=32; server 10.244.1.147:80 max_fails=1 fail_timeout=10s max_conns=32; server 10.244.2.162:80 max_fails=1 fail_timeout=10s max_conns=32; keepalive 32; } server { listen 80; server_name cafe.example.com; listen 443 ssl; ssl_certificate /etc/nginx/secrets/default; ssl_certificate_key /etc/nginx/secrets/default; ssl_ciphers NULL; server_tokens \"on\"; location /tea { proxy_connect_timeout 30s; proxy_read_timeout 30s; proxy_send_timeout 30s; client_max_body_size 2m; proxy_max_temp_file_size 1024m; proxy_buffering on; proxy_http_version 1.1; set $default_connection_header \"\"; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $vs_connection_header; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass https://vs_default_cafe_tea; proxy_next_upstream error timeout non_idempotent; proxy_next_upstream_timeout 5s; proxy_next_upstream_tries 10; } } 写在最后 本文详细介绍了基于nginx的ingress实现，通过实际的案例演示ingress的安装部署，基于虚拟主机的配置，基于TLS加密实现https，高级章节中介绍了负载均衡参数定制，自定义资源虚拟主机和虚拟路由的实现，通过该章节相信能加深对ingress服务暴露机制的理解。实现ingress controller的方式有多种，下一个章节我们将介绍基于HAproxy和TKE Ingress控制器的实现。 参考文献 Ingress配置：https://kubernetes.io/docs/concepts/services-networking/ingress/ Ingress控制器：https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/ Nginx ingress安装文档：https://github.com/nginxinc/kubernetes-ingress/blob/master/docs/installation.md Nginx ingress文档说明：https://github.com/nginxinc/kubernetes-ingress/tree/master/docs 虚拟主机和路由：https://github.com/nginxinc/kubernetes-ingress/blob/master/docs/virtualserver-and-virtualserverroute.md 『 转载 』该文章来源于网络，侵删。 ","date":"2019-08-04","objectID":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:9:0","tags":["kubernetes"],"title":"14 基于nginx Ingress实现服务暴露","uri":"/14-%E5%9F%BA%E4%BA%8Enginx-ingress%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"写在前面 上一篇文章中介绍了基于Nginx实现Ingress Controller的实现，介绍了Nginx Ingress Controller安装、相关功能，TLS，高级特性等介绍，本章开始介绍基于腾讯云TKE实现ingress服务暴露。 1. TKE ingress ","date":"2019-08-04","objectID":"/15-tke%E4%B8%AD%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:0:0","tags":["kubernetes"],"title":"15 TKE中实现ingress服务暴露","uri":"/15-tke%E4%B8%AD%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.1 TKE ingress架构 TKE是Tencent Kubernetes Engine即腾讯云基于kubernetes提供的公有云上容器云服务，TKE提供了两种暴露服务的方式：service和ingress。 内网CLB，四层负载均衡，提供VPC内访问，通过node节点的NodePort转发至service； 外网CLB，四层负载均衡，提供公网访问，需要node节点具有访问公网的能力； ingress， 七层负载均衡，提供http和https接入，提供ingress控制器的功能，借助NodePort转发 要使用TKE的ingress功能，需要了解一下相关的组件内容： l7-lb-controller ingress客户端，安装在kube-system，用于解析ingress配置并更新CLB的规则 CLB 七层负载均衡，提供ingress controller的功能，根据ingress规则创建http/https监听器，配置转发规则，以NodePort端口绑定后端RS Service 用于ingress服务发现，通过NodePort方式接入CLB 证书 用于提供https接入，配置在CLB负载均衡上，提供CA签名证书，通过Secrets封装给CLB使用 由于nginx ingress controller是直接以Pod的形势部署在kubernetes集群中，借助于service的服务发现可直接实现和pod通讯，而TKE中ingress controller未直接部署在k8s集群中，网络的接入需借助于service的NodePort实现接入，其数据流如下图： ","date":"2019-08-04","objectID":"/15-tke%E4%B8%AD%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:1:0","tags":["kubernetes"],"title":"15 TKE中实现ingress服务暴露","uri":"/15-tke%E4%B8%AD%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2 ingress虚拟主机 环境说明: 创建两个Deployment并以NodePort方式暴露服务，www1.happylau.cn对应tke-app-1服务，同理www2.happylau.cn对应tke-app-2服务，如下演示操作过程： 1、创建两个Deployments [root@VM_10_2_centos ingress]# kubectl create deployment tke-app-1 --image=nginx:1.7.9 [root@VM_10_2_centos ingress]# kubectl create deployment tke-app-2 --image=nginx:1.7.9 2、 将两个Deployment以NodePort的方式暴露服务 [root@VM_10_2_centos ~]# kubectl expose deployment tke-app-1 --port=80 --type=NodePort [root@VM_10_2_centos ~]# kubectl expose deployment tke-app-2 --port=80 --type=NodePort 查看服务列表 [root@VM_10_2_centos ~]# kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 172.16.255.1 \u003cnone\u003e 443/TCP 83d tke-app-1 NodePort 172.16.255.91 \u003cnone\u003e 80:30597/TCP 2s tke-app-2 NodePort 172.16.255.236 \u003cnone\u003e 80:31674/TCP 73s 3、定义ingress规则，定义两个host将不同主机转发至backend不同的service apiVersion: extensions/v1beta1 kind: Ingress metadata: name: tke-ingress-demo annotations: kubernetes.io/ingress.class: qcloud spec: rules: - host: www1.happylau.cn http: paths: - path: / backend: serviceName: tke-app-1 servicePort: 80 - host: www2.happylau.cn http: paths: - path: / backend: serviceName: tke-app-2 servicePort: 80 4、 应用ingress规则，并查看ingress详情，可以看到ingress创建了一个公网CLB实例 #应用ingress规则 [root@VM_10_2_centos ingress]# kubectl apply -f tke-ingress-demo.yaml ingress.extensions/tke-ingress-demo created #查看ingress列表 [root@VM_10_2_centos ingress]# kubectl get ingresses NAME HOSTS ADDRESS PORTS AGE tke-ingress-demo www1.happylau.cn,www2.happylau.cn 140.143.84.xxx 80 67s #查看 ingress详情 [root@VM_10_2_centos ingress]# kubectl describe ingresses tke-ingress-demo Name: tke-ingress-demo Namespace: default Address: 140.143.84.xxx Default backend: default-http-backend:80 (\u003cnone\u003e) Rules: Host Path Backends ---- ---- -------- www1.happylau.cn / tke-app-1:80 (172.16.1.15:80) www2.happylau.cn / tke-app-2:80 (172.16.2.17:80) Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"extensions/v1beta1\",\"kind\":\"Ingress\",\"metadata\":{\"annotations\":{\"kubernetes.io/ingress.class\":\"qcloud\"},\"name\":\"tke-ingress-demo\",\"namespace\":\"default\"},\"spec\":{\"rules\":[{\"host\":\"www1.happylau.cn\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"tke-app-1\",\"servicePort\":80},\"path\":\"/\"}]}},{\"host\":\"www2.happylau.cn\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"tke-app-2\",\"servicePort\":80},\"path\":\"/\"}]}}]}} kubernetes.io/ingress.class: qcloud kubernetes.io/ingress.qcloud-loadbalance-id: lb-a0xwhcx3 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal EnsuringIngress 69s (x3 over 89s) loadbalancer-controller Ensuring ingress Normal CREATE 69s (x2 over 70s) loadbalancer-controller create loadbalancer succ Normal EnsuredIngress 68s (x3 over 70s) loadbalancer-controller Ensured ingress 5、测试验证，将IP和域名写入到hosts文件中，访问域名测试验证，如下通过curl解析的方式测试验证 6、ingress会创建一个CLB，并在CLB中创建监听器、设置转发规则、绑定后端RS，下图是CLB上自动生成的规则 通过上面演示可知： 自动创建CLB实例 CLB实例上配置监听器 配置转发规则 绑定Node节点 绑定端口为service创建的NodePort ","date":"2019-08-04","objectID":"/15-tke%E4%B8%AD%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:2:0","tags":["kubernetes"],"title":"15 TKE中实现ingress服务暴露","uri":"/15-tke%E4%B8%AD%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.3 ingress证书加密 TKE支持将在CLB中加载证书实现https加密传输，证书是经过第三方认证的CA签名过的证书，需要先购买好证书，通过Secrets对象在kubernetes集群中定义，如下演示https的实现。 1、 通过Secrets创建证书，先获取到证书的id，如果没有则先创建证书，证书管理，本文以证书id TKPmsWb3 为例，通过stringData能实现base64自动加密 apiVersion: v1 kind: Secret metadata: name: ingress-ssl-key stringData: qcloud_cert_id: TKPmsWb3 type: Opaque #生成Secrets对象 [root@VM_10_2_centos ingress]# kubectl apply -f ingress-secret.yaml secret/ingress-ssl-key created [root@VM_10_2_centos ingress]# kubectl get secrets ingress-ssl-key NAME TYPE DATA AGE ingress-ssl-key Opaque 1 7s #查看secrets详情，可得知VEtQbXNXYjM= 已自动通过base64加密 [root@VM_10_2_centos ingress]# kubectl get secrets ingress-ssl-key -o yaml apiVersion: v1 data: qcloud_cert_id: VEtQbXNXYjM= kind: Secret metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"v1\",\"kind\":\"Secret\",\"metadata\":{\"annotations\":{},\"name\":\"ingress-ssl-key\",\"namespace\":\"default\"},\"stringData\":{\"qcloud_cert_id\":\"TKPmsWb3\"},\"type\":\"Opaque\"} creationTimestamp: \"2020-01-03T11:53:33Z\" name: ingress-ssl-key namespace: default resourceVersion: \"7083702418\" selfLink: /api/v1/namespaces/default/secrets/ingress-ssl-key uid: aaea4a86-2e1f-11ea-a618-ae9224ffad1a type: Opaque #可以通过base64查看解密后的内容，和配置文件中定义的id一致 [root@VM_10_2_centos ingress]# echo VEtQbXNXYjM= | base64 -d TKPmsWb3 2、准备环境，创建一个nginx的Deployment [root@VM_10_2_centos ~]# kubectl create deployment tke-ingress-ssl-demo --image=nginx:1.7.9 deployment.apps/tke-ingress-ssl-demo created [root@VM_10_2_centos ~]# kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE tke-ingress-ssl-demo 1/1 1 1 6s 3、将Deployment暴露以NodePort类型暴露service [root@VM_10_2_centos ~]# kubectl expose deployment tke-ingress-ssl-demo --port=80 --type=NodePort service/tke-ingress-ssl-demo exposed [root@VM_10_2_centos ~]# kubectl get service tke-ingress-ssl-demo -o yaml apiVersion: v1 kind: Service metadata: creationTimestamp: \"2020-01-03T12:00:05Z\" labels: app: tke-ingress-ssl-demo name: tke-ingress-ssl-demo namespace: default resourceVersion: \"7083890283\" selfLink: /api/v1/namespaces/default/services/tke-ingress-ssl-demo uid: 94659f42-2e20-11ea-a618-ae9224ffad1a spec: clusterIP: 172.16.255.64 externalTrafficPolicy: Cluster ports: - nodePort: 30324 port: 80 protocol: TCP targetPort: 80 selector: app: tke-ingress-ssl-demo sessionAffinity: None type: NodePort #类型为NodePort status: loadBalancer: {} 4、定义ingress规则，加载证书实现https转发 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: tke-ingress-ssl annotations: kubernetes.io/ingress.class: qcloud qcloud_cert_id: TKPmsWb3 spec: rules: - host: www.happylauliu.cn http: paths: - path: / backend: serviceName: tke-ingress-ssl-demo servicePort: 80 tls: - hosts: - www.happylauliu.cn secretName: ingress-ssl-key 5、应用ingress规则，并查看详情，此时已正常创建CLB并配置规则 [root@VM_10_2_centos ingress]# kubectl apply -f ingress-demo.yaml ingress.extensions/tke-ingress-ssl created #查看ingress详情 [root@VM_10_2_centos ingress]# kubectl describe ingresses tke-ingress-ssl Name: tke-ingress-ssl Namespace: default Address: 140.143.83.xxx #CLB的外网IP Default backend: default-http-backend:80 (\u003cnone\u003e) TLS: ingress-ssl-key terminates www.happylauliu.cn Rules: Host Path Backends ---- ---- -------- www.happylauliu.cn / tke-ingress-ssl-demo:80 (172.16.0.25:80) Annotations: qcloud_cert_id: TKPmsWb3 kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"extensions/v1beta1\",\"kind\":\"Ingress\",\"metadata\":{\"annotations\":{\"kubernetes.io/ingress.class\":\"qcloud\",\"qcloud_cert_id\":\"TKPmsWb3\"},\"name\":\"tke-ingress-ssl\",\"namespace\":\"default\"},\"spec\":{\"rules\":[{\"host\":\"www.happylauliu.cn\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"tke-ingress-ssl-demo\",\"servicePort\":80},\"path\":\"/\"}]}}],\"tls\":[{\"hosts\":[\"www.happylauliu.cn\"],\"secretName\":\"ingress-ssl-key\"}]}} kubernetes.io/ingress.class: qcloud kubernetes.io/ingress.qcloud-loadbalance-id: lb-2kcrtwbn #CLB的实例id Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal EnsuringIngress 51s (x3 over 73s) loadbalancer-controller Ensuring ingress Nor","date":"2019-08-04","objectID":"/15-tke%E4%B8%AD%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:3:0","tags":["kubernetes"],"title":"15 TKE中实现ingress服务暴露","uri":"/15-tke%E4%B8%AD%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"写在前面 前面文章介绍了基于nginx实现ingress controller的功能，本章节接续介绍kubernetes系列教程中另外一个姐妹开源负载均衡的控制器：haproxy ingress controller。 1. HAproxy Ingress控制器 ","date":"2019-08-04","objectID":"/16-%E5%9F%BA%E4%BA%8Ehaproxy%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:0:0","tags":["kubernetes"],"title":"16 基于haproxy实现ingress服务暴露","uri":"/16-%E5%9F%BA%E4%BA%8Ehaproxy%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.1 HAproxy Ingress简介 HAProxy Ingress watches in the k8s cluster and how it builds HAProxy configuration 和Nginx相类似，HAproxy通过监视kubernetes api获取到service后端pod的状态，动态更新haproxy配置文件，以实现七层的负载均衡。 HAproxy Ingress控制器具备的特性如下： Fast，Carefully built on top of the battle-tested HAProxy load balancer. 基于haproxy性能有保障 Reliable，Trusted by sysadmins on clusters as big as 1,000 namespaces, 2,000 domains and 3,000 ingress objects. 可靠，支持1000最多1000个命名空间和2000多个域名 Highly customizable，100+ configuration options and growing. 可定制化强，支持100多个配置选项 HAproxy ingress控制器版本 社区版，基于haproxy社区高度定制符合ingress的控制器，功能相对有限 企业版，haproxy企业版本，支持很多高级特性和功能，大部分高级功能在企业版本中实现 ","date":"2019-08-04","objectID":"/16-%E5%9F%BA%E4%BA%8Ehaproxy%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:1:0","tags":["kubernetes"],"title":"16 基于haproxy实现ingress服务暴露","uri":"/16-%E5%9F%BA%E4%BA%8Ehaproxy%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2 HAproxy控制器安装 haproxy ingress安装相对简单，官方提供了安装的yaml文件，先将文件下载查看一下kubernetes资源配置，包含的资源类型有： ServiceAccount 和RBAC认证授权关联 RBAC认证 Role、ClusterRole、 ClusterRoleBinding Deployment 默认包含的一个后端backend应用服务器，与之关联一个Service Service 后端的一个service DaemonSet HAproxy最核心的一个控制器，关联认证ServiceAccount和配置ConfigMap，定义了一个nodeSelector，label为role: ingress-controller，将运行在特定的节点上 ConfigMap 实现haproxy ingress自定义配置 安装文件路径https://haproxy-ingress.github.io/resources/haproxy-ingress.yaml 1、创建命名空间，haproxy ingress部署在ingress-controller这个命名空间，先创建ns [root@node-1 ~]# kubectl create namespace ingress-controller namespace/ingress-controller created [root@node-1 ~]# kubectl get namespaces ingress-controller -o yaml apiVersion: v1 kind: Namespace metadata: creationTimestamp: \"2019-12-27T09:56:04Z\" name: ingress-controller resourceVersion: \"13946553\" selfLink: /api/v1/namespaces/ingress-controller uid: ea70b2f7-efe4-43fd-8ce9-3b917b09b533 spec: finalizers: - kubernetes status: phase: Active 2、安装haproxy ingress控制器 [root@node-1 ~]# wget https://haproxy-ingress.github.io/resources/haproxy-ingress.yaml [root@node-1 ~]# kubectl apply -f haproxy-ingress.yaml serviceaccount/ingress-controller created clusterrole.rbac.authorization.k8s.io/ingress-controller created role.rbac.authorization.k8s.io/ingress-controller created clusterrolebinding.rbac.authorization.k8s.io/ingress-controller created rolebinding.rbac.authorization.k8s.io/ingress-controller created deployment.apps/ingress-default-backend created service/ingress-default-backend created configmap/haproxy-ingress created daemonset.apps/haproxy-ingress created 3、 检查haproxy ingress安装情况，检查haproxy ingress核心的DaemonSets，发现DS并未部署Pod，原因是配置文件中定义了nodeSelector节点标签选择器，因此需要给node设置合理的标签 [root@node-1 ~]# kubectl get daemonsets -n ingress-controller NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE haproxy-ingress 0 0 0 0 0 role=ingress-controller 5m51s 4、 给node设置标签，让DaemonSets管理的Pod能调度到node节点上，生产环境中根据情况定义，将实现haproxy ingress功能的节点定义到特定的节点，对个node节点的访问，需要借助于负载均衡实现统一接入，本文主要以探究haproxy ingress功能，因此未部署负载均衡调度器，读者可根据实际的情况部署。以node-1和node-2为例： [root@node-1 ~]# kubectl label node node-1 role=ingress-controller node/node-1 labeled [root@node-1 ~]# kubectl label node node-2 role=ingress-controller node/node-2 labeled #查看labels的情况 [root@node-1 ~]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS node-1 Ready master 104d v1.15.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node-1,kubernetes.io/os=linux,node-role.kubernetes.io/master=,role=ingress-controller node-2 Ready \u003cnone\u003e 104d v1.15.3 app=web,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node-2,kubernetes.io/os=linux,label=test,role=ingress-controller node-3 Ready \u003cnone\u003e 104d v1.15.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node-3,kubernetes.io/os=linux 5、再次查看ingress部署情况，已完成部署，并调度至node-1和node-2节点上 [root@node-1 ~]# kubectl get daemonsets -n ingress-controller NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE haproxy-ingress 2 2 2 2 2 role=ingress-controller 15m [root@node-1 ~]# kubectl get pods -n ingress-controller -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES haproxy-ingress-bdns8 1/1 Running 0 2m27s 10.254.100.102 node-2 \u003cnone\u003e \u003cnone\u003e haproxy-ingress-d5rnl 1/1 Running 0 2m31s 10.254.100.101 node-1 \u003cnone\u003e \u003cnone\u003e haproxy ingress部署时候也通过deployments部署了一个后端backend服务，这是部署haproxy ingress必须部署服务，否则ingress controller无法启动，可以通过查看Deployments列表确认 [root@node-1 ~]# kubectl get deployments -n ingress-controller NAME READY UP-TO-DATE AVAILABLE AGE ingress-default-backend 1/1 1 1 18m 6、 查看haproxy ingress的日志，通过查询日志可知，多个haproxy ingress是通过选举实现高可用HA机制。 其他资源包括ServiceAccount，ClusterRole，ConfigMaps请单独确认，至此HAproxy ingress controller部署完毕。另外两种部署方式： Deployment部署方式 Helm部署方式 2. haproxy ingress使用 ","date":"2019-08-04","objectID":"/16-%E5%9F%BA%E4%BA%8Ehaproxy%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:2:0","tags":["kubernetes"],"title":"16 基于haproxy实现ingress服务暴露","uri":"/16-%E5%9F%BA%E4%BA%8Ehaproxy%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.1 haproxy ingress基础 Ingress控制器部署完毕后需要定义Ingress规则，以方便Ingress控制器能够识别到service后端Pod的资源，这个章节我们将来介绍在HAproxy Ingress Controller环境下Ingress的使用。 1、环境准备，创建一个deployments并暴露其端口 #创建应用并暴露端口 [root@node-1 haproxy-ingress]# kubectl run haproxy-ingress-demo --image=nginx:1.7.9 --port=80 --replicas=1 --expose kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead. service/haproxy-ingress-demo created deployment.apps/haproxy-ingress-demo created #查看应用 [root@node-1 haproxy-ingress]# kubectl get deployments haproxy-ingress-demo NAME READY UP-TO-DATE AVAILABLE AGE haproxy-ingress-demo 1/1 1 1 10s #查看service情况 [root@node-1 haproxy-ingress]# kubectl get services haproxy-ingress-demo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE haproxy-ingress-demo ClusterIP 10.106.199.102 \u003cnone\u003e 80/TCP 17s 2、创建ingress规则,如果有多个ingress控制器，可以通过ingress.class指定类型为haproxy apiVersion: extensions/v1beta1 kind: Ingress metadata: name: haproxy-ingress-demo labels: ingresscontroller: haproxy annotations: kubernetes.io/ingress.class: haproxy spec: rules: - host: www.happylau.cn http: paths: - path: / backend: serviceName: haproxy-ingress-demo servicePort: 80 3、应用ingress规则，并查看ingress详情，查看Events日志发现控制器已正常更新 [root@node-1 haproxy-ingress]# kubectl apply -f ingress-demo.yaml ingress.extensions/haproxy-ingress-demo created #查看详情 [root@node-1 haproxy-ingress]# kubectl describe ingresses haproxy-ingress-demo Name: haproxy-ingress-demo Namespace: default Address: Default backend: default-http-backend:80 (\u003cnone\u003e) Rules: Host Path Backends ---- ---- -------- www.happylau.cn / haproxy-ingress-demo:80 (10.244.2.166:80) Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"extensions/v1beta1\",\"kind\":\"Ingress\",\"metadata\":{\"annotations\":{\"kubernetes.io/ingress.class\":\"haproxy\"},\"labels\":{\"ingresscontroller\":\"haproxy\"},\"name\":\"haproxy-ingress-demo\",\"namespace\":\"default\"},\"spec\":{\"rules\":[{\"host\":\"www.happylau.cn\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"haproxy-ingress-demo\",\"servicePort\":80},\"path\":\"/\"}]}}]}} kubernetes.io/ingress.class: haproxy Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 27s ingress-controller Ingress default/haproxy-ingress-demo Normal CREATE 27s ingress-controller Ingress default/haproxy-ingress-demo Normal UPDATE 20s ingress-controller Ingress default/haproxy-ingress-demo Normal UPDATE 20s ingress-controller Ingress default/haproxy-ingress-demo 4、测试验证ingress规则，可以将域名写入到hosts文件中，我们直接使用gcurl测试，地址指向node-1或node-2均可 [root@node-1 haproxy-ingress]# curl http://www.happylau.cn --resolve www.happylau.cn:80:10.254.100.101 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e 5、测试正常，接下来到haproxy ingress controller中刚查看对应生成规则配置文件 [root@node-1 ~]# kubectl exec -it haproxy-ingress-bdns8 -n ingress-controller /bin/sh #查看配置文件 /etc/haproxy # cat /etc/haproxy/haproxy.cfg # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # HAProxy Ingress Controller # # -------------------------- # # This file is automatically updated, do not edit # # # 全局配置文件内容 global daemon nbthread 2 cpu-map auto:1/1-2 0-1 stats socket /var/run/haproxy-stats.sock level admin expose-fd listeners maxconn 2000 hard-stop-after 10m lua-load /usr/local/etc/haproxy/lua/send-response.lua lua-load /usr/local/etc/haproxy/lua/auth-request.lua tu","date":"2019-08-04","objectID":"/16-%E5%9F%BA%E4%BA%8Ehaproxy%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:3:0","tags":["kubernetes"],"title":"16 基于haproxy实现ingress服务暴露","uri":"/16-%E5%9F%BA%E4%BA%8Ehaproxy%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.2 动态更新和负载均衡 后端Pod是实时动态变化的，haproxy ingress通过service的服务发现机制，动态识别到后端Pod的变化情况，并动态更新haproxy.cfg配置文件，并重载配置（实际不需要重启haproxy服务），本章节将演示haproxy ingress动态更新和负载均衡功能。 1、动态更新，我们以扩容pod的副本为例，将副本数从replicas=1扩容至3个 [root@node-1 ~]# kubectl scale --replicas=3 deployment haproxy-ingress-demo deployment.extensions/haproxy-ingress-demo scaled [root@node-1 ~]# kubectl get deployments haproxy-ingress-demo NAME READY UP-TO-DATE AVAILABLE AGE haproxy-ingress-demo 3/3 3 3 43m #查看扩容后Pod的IP地址 [root@node-1 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES haproxy-ingress-demo-5d487d4fc-5pgjt 1/1 Running 0 43m 10.244.2.166 node-3 \u003cnone\u003e \u003cnone\u003e haproxy-ingress-demo-5d487d4fc-pst2q 1/1 Running 0 18s 10.244.0.52 node-1 \u003cnone\u003e \u003cnone\u003e haproxy-ingress-demo-5d487d4fc-sr8tm 1/1 Running 0 18s 10.244.1.149 node-2 \u003cnone\u003e \u003cnone\u003e 2、查看haproxy配置文件内容,可以看到backend后端主机列表已动态发现新增的pod地址 backend default_haproxy-ingress-demo_80 mode http balance roundrobin acl https-request ssl_fc http-request set-header X-Original-Forwarded-For %[hdr(x-forwarded-for)] if { hdr(x-forwarded-for) -m found } http-request del-header x-forwarded-for option forwardfor http-response set-header Strict-Transport-Security \"max-age=15768000\" server srv001 10.244.2.166:80 weight 1 check inter 2s #新增的pod地址 server srv002 10.244.0.52:80 weight 1 check inter 2s server srv003 10.244.1.149:80 weight 1 check inter 2s server srv004 127.0.0.1:1023 disabled weight 1 check inter 2s server srv005 127.0.0.1:1023 disabled weight 1 check inter 2s server srv006 127.0.0.1:1023 disabled weight 1 check inter 2s server srv007 127.0.0.1:1023 disabled weight 1 check inter 2s 4、查看haproxy ingress日志，日志中提示HAProxy updated without needing to reload，即配置动态识别，不需要重启haproxy服务就能够识别，自从1.8后haproxy能支持动态配置更新的能力，以适应微服务的场景，详情查看文章说明 [root@node-1 ~]# kubectl logs haproxy-ingress-bdns8 -n ingress-controller -f I1227 12:21:11.523066 6 controller.go:274] Starting HAProxy update id=20 I1227 12:21:11.561001 6 instance.go:162] HAProxy updated without needing to reload. Commands sent: 3 I1227 12:21:11.561057 6 controller.go:325] Finish HAProxy update id=20: ingress=0.149764ms writeTmpl=37.738947ms total=37.888711ms 5、接下来测试负载均衡的功能，为了验证测试效果，往pod中写入不同的内容，以测试负载均衡的效果 [root@node-1 ~]# kubectl exec -it haproxy-ingress-demo-5d487d4fc-5pgjt /bin/bash root@haproxy-ingress-demo-5d487d4fc-5pgjt:/# echo \"web-1\" \u003e /usr/share/nginx/html/index.html [root@node-1 ~]# kubectl exec -it haproxy-ingress-demo-5d487d4fc-pst2q /bin/bash root@haproxy-ingress-demo-5d487d4fc-pst2q:/# echo \"web-2\" \u003e /usr/share/nginx/html/index.html [root@node-1 ~]# kubectl exec -it haproxy-ingress-demo-5d487d4fc-sr8tm /bin/bash root@haproxy-ingress-demo-5d487d4fc-sr8tm:/# echo \"web-3\" \u003e /usr/share/nginx/html/index.html 6、测试验证负载均衡效果,haproxy采用轮询的调度算法，因此可以明显看到轮询效果 [root@node-1 ~]# curl http://www.happylau.cn --resolve www.happylau.cn:80:10.254.100.102 web-1 [root@node-1 ~]# curl http://www.happylau.cn --resolve www.happylau.cn:80:10.254.100.102 web-2 [root@node-1 ~]# curl http://www.happylau.cn --resolve www.happylau.cn:80:10.254.100.102 web-3 这个章节验证了haproxy ingress控制器动态配置更新的能力，相比于nginx ingress控制器而言，haproxy ingress控制器不需要重载服务进程就能够动态识别到配置，在微服务场景下将具有非常大的优势；并通过一个实例验证了ingress负载均衡调度能力。 ","date":"2019-08-04","objectID":"/16-%E5%9F%BA%E4%BA%8Ehaproxy%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:4:0","tags":["kubernetes"],"title":"16 基于haproxy实现ingress服务暴露","uri":"/16-%E5%9F%BA%E4%BA%8Ehaproxy%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.3 基于名称虚拟主机 这个小节将演示haproxy ingress基于虚拟云主机功能的实现，定义两个虚拟主机news.happylau.cn和sports.happylau.cn，将请求各自转发至haproxy-1和haproxy-2 1、 准备环境测试环境，创建两个应用haproxy-1和haproxy并暴露服务端口 [root@node-1 ~]# kubectl run haproxy-1 --image=nginx:1.7.9 --port=80 --replicas=1 --expose=true [root@node-1 ~]# kubectl run haproxy-2 --image=nginx:1.7.9 --port=80 --replicas=1 --expose=true 查看应用 [root@node-1 ~]# kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE haproxy-1 1/1 1 1 39s haproxy-2 1/1 1 1 36s 查看service [root@node-1 ~]# kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE haproxy-1 ClusterIP 10.100.239.114 \u003cnone\u003e 80/TCP 55s haproxy-2 ClusterIP 10.100.245.28 \u003cnone\u003e 80/TCP 52s 3、定义ingress规则，定义不同的主机并将请求转发至不同的service中 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: haproxy-ingress-virtualhost annotations: kubernetes.io/ingress.class: haproxy spec: rules: - host: news.happylau.cn http: paths: - path: / backend: serviceName: haproxy-1 servicePort: 80 - host: sports.happylau.cn http: paths: - path: / backend: serviceName: haproxy-2 servicePort: 80 #应用ingress规则并查看列表 [root@node-1 haproxy-ingress]# kubectl apply -f ingress-virtualhost.yaml ingress.extensions/haproxy-ingress-virtualhost created [root@node-1 haproxy-ingress]# kubectl get ingresses haproxy-ingress-virtualhost NAME HOSTS ADDRESS PORTS AGE haproxy-ingress-virtualhost news.happylau.cn,sports.happylau.cn 80 8s 查看ingress规则详情 [root@node-1 haproxy-ingress]# kubectl describe ingresses haproxy-ingress-virtualhost Name: haproxy-ingress-virtualhost Namespace: default Address: Default backend: default-http-backend:80 (\u003cnone\u003e) Rules: Host Path Backends ---- ---- -------- news.happylau.cn / haproxy-1:80 (10.244.2.168:80) sports.happylau.cn / haproxy-2:80 (10.244.2.169:80) Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"extensions/v1beta1\",\"kind\":\"Ingress\",\"metadata\":{\"annotations\":{\"kubernetes.io/ingress.class\":\"haproxy\"},\"name\":\"haproxy-ingress-virtualhost\",\"namespace\":\"default\"},\"spec\":{\"rules\":[{\"host\":\"news.happylau.cn\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"haproxy-1\",\"servicePort\":80},\"path\":\"/\"}]}},{\"host\":\"sports.happylau.cn\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"haproxy-2\",\"servicePort\":80},\"path\":\"/\"}]}}]}} kubernetes.io/ingress.class: haproxy Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 37s ingress-controller Ingress default/haproxy-ingress-virtualhost Normal CREATE 37s ingress-controller Ingress default/haproxy-ingress-virtualhost Normal UPDATE 20s ingress-controller Ingress default/haproxy-ingress-virtualhost Normal UPDATE 20s ingress-controller Ingress default/haproxy-ingress-virtualhost 4、测试验证虚拟机主机配置，通过curl直接解析的方式，或者通过写hosts文件 5、查看配置配置文件内容，配置中更新了haproxy.cfg的front段和backend段的内容 /etc/haproxy/haproxy.cfg 配置文件内容 backend default_haproxy-1_80 #haproxy-1后端 mode http balance roundrobin acl https-request ssl_fc http-request set-header X-Original-Forwarded-For %[hdr(x-forwarded-for)] if { hdr(x-forwarded-for) -m found } http-request del-header x-forwarded-for option forwardfor http-response set-header Strict-Transport-Security \"max-age=15768000\" server srv001 10.244.2.168:80 weight 1 check inter 2s server srv002 127.0.0.1:1023 disabled weight 1 check inter 2s server srv003 127.0.0.1:1023 disabled weight 1 check inter 2s server srv004 127.0.0.1:1023 disabled weight 1 check inter 2s server srv005 127.0.0.1:1023 disabled weight 1 check inter 2s server srv006 127.0.0.1:1023 disabled weight 1 check inter 2s server srv007 127.0.0.1:1023 disabled weight 1 check inter 2s #haproxy-2后端 backend default_haproxy-2_80 mode http balance roundrobin acl https-request ssl_fc http-request set-header X-Original-Forwarded-For %[hdr(x-forwarded-for)] if { hdr(x-forwarded-for) -m found } http-request del-header x-forwarded-for option forwardfor http-response set-header Strict-Transport-Security \"max-age=15768000\" server srv001 10.244.2.169:80 weight 1 check inter 2s server srv002 127.0.0.1:1023 disabled weight 1 check int","date":"2019-08-04","objectID":"/16-%E5%9F%BA%E4%BA%8Ehaproxy%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:5:0","tags":["kubernetes"],"title":"16 基于haproxy实现ingress服务暴露","uri":"/16-%E5%9F%BA%E4%BA%8Ehaproxy%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.4 URL自动跳转 haproxy ingress支持自动跳转的能力，需要通过annotations定义，通过ingress.kubernetes.io/ssl-redirect设置即可，默认为false，设置为true即可实现http往https跳转的能力，当然可以将配置写入到ConfigMap中实现默认跳转的能力，本文以编写annotations为例，实现访问http跳转https的能力。 1、定义ingress规则，设置ingress.kubernetes.io/ssl-redirect实现跳转功能 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: haproxy-ingress-virtualhost annotations: kubernetes.io/ingress.class: haproxy ingress.kubernetes.io/ssl-redirect: true #实现跳转功能 spec: rules: - host: news.happylau.cn http: paths: - path: / backend: serviceName: haproxy-1 servicePort: 80 - host: sports.happylau.cn http: paths: - path: / backend: serviceName: haproxy-2 servicePort: 80 按照上图测试了一下功能，未能实现跳转实现跳转的功能，开源版本中未能找到更多文档说明，企业版由于镜像需要认证授权下载，未能进一步做测试验证。 ","date":"2019-08-04","objectID":"/16-%E5%9F%BA%E4%BA%8Ehaproxy%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:6:0","tags":["kubernetes"],"title":"16 基于haproxy实现ingress服务暴露","uri":"/16-%E5%9F%BA%E4%BA%8Ehaproxy%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.4 基于TLS加密 haproxy ingress默认集成了一个 1、生成自签名证书和私钥 [root@node-1 haproxy-ingress]# openssl req -x509 -newkey rsa:2048 -nodes -days 365 -keyout tls.key -out tls.crt Generating a 2048 bit RSA private key ...........+++ .......+++ writing new private key to 'tls.key' ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [XX]:CN State or Province Name (full name) []:GD Locality Name (eg, city) [Default City]:ShenZhen Organization Name (eg, company) [Default Company Ltd]:Tencent Organizational Unit Name (eg, section) []:HappyLau Common Name (eg, your name or your server's hostname) []:www.happylau.cn Email Address []:573302346@qq.com 2、创建Secrets，关联证书和私钥 [root@node-1 haproxy-ingress]# kubectl create secret tls haproxy-tls --cert=tls.crt --key=tls.key secret/haproxy-tls created [root@node-1 haproxy-ingress]# kubectl describe secrets haproxy-tls Name: haproxy-tls Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Type: kubernetes.io/tls Data ==== tls.crt: 1424 bytes tls.key: 1704 bytes 3、编写ingress规则，通过tls关联Secrets apiVersion: extensions/v1beta1 kind: Ingress metadata: name: haproxy-ingress-virtualhost annotations: kubernetes.io/ingress.class: haproxy spec: tls: - hosts: - news.happylau.cn - sports.happylau.cn secretName: haproxy-tls rules: - host: news.happylau.cn http: paths: - path: / backend: serviceName: haproxy-1 servicePort: 80 - host: sports.happylau.cn http: paths: - path: / backend: serviceName: haproxy-2 servicePort: 80 4、应用配置并查看详情,在TLS中可以看到TLS关联的证书 [root@node-1 haproxy-ingress]# kubectl apply -f ingress-virtualhost.yaml ingress.extensions/haproxy-ingress-virtualhost configured [root@node-1 haproxy-ingress]# kubectl describe ingresses haproxy-ingress-virtualhost Name: haproxy-ingress-virtualhost Namespace: default Address: Default backend: default-http-backend:80 (\u003cnone\u003e) TLS: haproxy-tls terminates news.happylau.cn,sports.happylau.cn Rules: Host Path Backends ---- ---- -------- news.happylau.cn / haproxy-1:80 (10.244.2.168:80) sports.happylau.cn / haproxy-2:80 (10.244.2.169:80) Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"extensions/v1beta1\",\"kind\":\"Ingress\",\"metadata\":{\"annotations\":{\"kubernetes.io/ingress.class\":\"haproxy\"},\"name\":\"haproxy-ingress-virtualhost\",\"namespace\":\"default\"},\"spec\":{\"rules\":[{\"host\":\"news.happylau.cn\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"haproxy-1\",\"servicePort\":80},\"path\":\"/\"}]}},{\"host\":\"sports.happylau.cn\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"haproxy-2\",\"servicePort\":80},\"path\":\"/\"}]}}],\"tls\":[{\"hosts\":[\"news.happylau.cn\",\"sports.happylau.cn\"],\"secretName\":\"haproxy-tls\"}]}} kubernetes.io/ingress.class: haproxy Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 37m ingress-controller Ingress default/haproxy-ingress-virtualhost Normal CREATE 37m ingress-controller Ingress default/haproxy-ingress-virtualhost Normal UPDATE 7s (x2 over 37m) ingress-controller Ingress default/haproxy-ingress-virtualhost Normal UPDATE 7s (x2 over 37m) ingress-controller Ingress default/haproxy-ingress-virtualhost 5、测试https站点访问，可以看到安全的https访问 写在最后 haproxy实现ingress实际是通过配置更新haproxy.cfg配置，结合service的服务发现机制动态完成ingress接入，相比于nginx来说，haproxy不需要重载实现配置变更。在测试haproxy ingress过程中，有部分功能配置验证没有达到预期，更丰富的功能支持在haproxy ingress企业版中支持，社区版能支持蓝绿发布和WAF安全扫描功能，详情可以参考社区文档haproxy蓝绿发布和WAF安全支持。 haproxy ingress控制器目前在社区活跃度一般，相比于nginx，traefik，istio还有一定的差距，实际环境中不建议使用社区版的haproxy ingress。 参考文档 官方安装文档：https://haproxy-ingress.github.io/docs/getting-started/ haproxy ingress官方配置：https://www.haproxy.com/documentation/hapee/1-7r2/traffic-management/k8s-image-controller/ 当你的才华撑不起你的野心时，你就应该静下心来学习 返回kubernetes系列教程目录 如果觉得文章对您有帮助，请订阅专栏，分享给有需要的朋友吧😊 关于作者 刘海平（HappyLau ）云计算高级顾问 目前在腾讯","date":"2019-08-04","objectID":"/16-%E5%9F%BA%E4%BA%8Ehaproxy%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/:7:0","tags":["kubernetes"],"title":"16 基于haproxy实现ingress服务暴露","uri":"/16-%E5%9F%BA%E4%BA%8Ehaproxy%E5%AE%9E%E7%8E%B0ingress%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"},{"categories":["转载","kubernetes","基础教程"],"content":"1. 监控架构概述 kubernetes监控指标大体可以分为两类：核心监控指标和自定义指标，核心监控指标是kubernetes内置稳定可靠监控指标，早期由heapster完成，现由metric-server实现；自定义指标用于实现核心指标的扩展，能够提供更丰富的指标支持，如应用状态指标，自定义指标需要通过Aggregator和k8s api集成，当前主流通过promethues实现。 监控指标用途： kubectl top 查看node和pod的cpu+内存使用情况 kubernetes-dashbaord 控制台查看节点和pod资源监控 Horizontal Pod Autoscaler 水平横向动态扩展 Scheduler 调度器调度选择条件 2. metric-server架构和安装 ","date":"2019-08-04","objectID":"/17-%E4%BD%BF%E7%94%A8metric-server%E8%AE%A9hpa%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E6%84%89%E5%BF%AB%E8%BF%90%E8%A1%8C/:0:0","tags":["kubernetes"],"title":"17 使用metric Server让HPA弹性伸缩愉快运行","uri":"/17-%E4%BD%BF%E7%94%A8metric-server%E8%AE%A9hpa%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E6%84%89%E5%BF%AB%E8%BF%90%E8%A1%8C/"},{"categories":["转载","kubernetes","基础教程"],"content":"2. 1 metric-server简介 Metrics Server is a cluster-wide aggregator of resource usage data. Resource metrics are used by components like kubectl top and the Horizontal Pod Autoscaler to scale workloads. To autoscale based upon a custom metric, you need to use the Prometheus Adapter Metric-server是一个集群级别的资源指标收集器，用于收集资源指标数据 提供基础资源如CPU、内存监控接口查询； 接口通过 Kubernetes aggregator注册到kube-apiserver中； 对外通过Metric API暴露给外部访问； 自定义指标使用需要借助Prometheus实现。 The Metrics API /node 获取所有节点的指标，指标名称为NodeMetrics /node/\u003cnode_name\u003e 特定节点指标 /namespaces/{namespace}/pods 获取命名空间下的所有pod指标 /namespaces/{namespace}/pods/{pod} 特定pod的指标，指标名称为PodMetrics 未来将能够支持指标聚合，如max最大值，min最小值，95th峰值，以及自定义时间窗口，如1h，1d，1w等。 ","date":"2019-08-04","objectID":"/17-%E4%BD%BF%E7%94%A8metric-server%E8%AE%A9hpa%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E6%84%89%E5%BF%AB%E8%BF%90%E8%A1%8C/:1:0","tags":["kubernetes"],"title":"17 使用metric Server让HPA弹性伸缩愉快运行","uri":"/17-%E4%BD%BF%E7%94%A8metric-server%E8%AE%A9hpa%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E6%84%89%E5%BF%AB%E8%BF%90%E8%A1%8C/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.2 metric-server架构 监控架构分两部分内容：核心监控(图白色部分)和自定义监控（图蓝色部分） 1、 核心监控实现 通过kubelet收集资源估算+使用估算 metric-server负责数据收集，不负责数据存储 metric-server对外暴露Metric API接口 核心监控指标客用户HPA，kubectl top，scheduler和dashboard 2、 自定义监控实现 自定义监控指标包括监控指标和服务指标 需要在每个node上部署一个agent上报至集群监控agent，如prometheus 集群监控agent收集数据后需要将监控指标+服务指标通过API adaptor转换为apiserver能够处理的接口 HPA通过自定义指标实现更丰富的弹性扩展能力，需要通过HPA adaptor API做次转换。 ","date":"2019-08-04","objectID":"/17-%E4%BD%BF%E7%94%A8metric-server%E8%AE%A9hpa%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E6%84%89%E5%BF%AB%E8%BF%90%E8%A1%8C/:2:0","tags":["kubernetes"],"title":"17 使用metric Server让HPA弹性伸缩愉快运行","uri":"/17-%E4%BD%BF%E7%94%A8metric-server%E8%AE%A9hpa%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E6%84%89%E5%BF%AB%E8%BF%90%E8%A1%8C/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.3 metric-server部署 1、获取metric-server安装文件，当前具有两个版本：1.7和1.8+，kubernetes1.7版本安装1.7的metric-server版本，kubernetes 1.8后版本安装metric server 1.8+版本 [root@node-1 ~]# git clone https://github.com/kubernetes-sigs/metrics-server.git 2、部署metric-server，部署1.8+版本 [root@node-1 metrics-server]# kubectl apply -f deploy/1.8+/ clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created serviceaccount/metrics-server created deployment.apps/metrics-server created service/metrics-server created clusterrole.rbac.authorization.k8s.io/system:metrics-server created clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created 核心的配置文件是metrics-server-deployment.yaml，metric-server以Deployment的方式部署在集群中，j镜像k8s.gcr.io/metrics-server-amd64:v0.3.6需要提前下载好，其对应的安装文件内容如下： --- apiVersion: v1 kind: ServiceAccount metadata: name: metrics-server namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: metrics-server namespace: kube-system labels: k8s-app: metrics-server spec: selector: matchLabels: k8s-app: metrics-server template: metadata: name: metrics-server labels: k8s-app: metrics-server spec: serviceAccountName: metrics-server volumes: # mount in tmp so we can safely use from-scratch images and/or read-only containers - name: tmp-dir emptyDir: {} containers: - name: metrics-server image: k8s.gcr.io/metrics-server-amd64:v0.3.6 args: - --cert-dir=/tmp - --secure-port=4443 ExternalIP ports: - name: main-port containerPort: 4443 protocol: TCP securityContext: readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 1000 imagePullPolicy: Always volumeMounts: - name: tmp-dir mountPath: /tmp nodeSelector: beta.kubernetes.io/os: linux 3、检查metric-server部署的情况,查看metric-server的Pod已部署成功 [root@node-1 1.8+]# kubectl get deployments metrics-server -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE metrics-server 1/1 1 1 2m49s [root@node-1 1.8+]# kubectl get pods -n kube-system metrics-server-67db467b7b-5xf8x NAME READY STATUS RESTARTS AGE metrics-server-67db467b7b-5xf8x 1/1 Running 0 3m 实际此时metric-server并不能使用，使用kubectl top node 查看会提示Error from server (NotFound): nodemetrics.metrics.k8s.io “node-1” not found类似的报错，查看metric-server的pod的日志信息，显示如下： [root@node-1 1.8+]# kubectl logs metrics-server-67db467b7b-5xf8x -n kube-system -f I1230 11:34:10.905500 1 serving.go:312] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key) I1230 11:34:11.527346 1 secure_serving.go:116] Serving securely on [::]:4443 E1230 11:35:11.552067 1 manager.go:111] unable to fully collect metrics: [unable to fully scrape metrics from source kubelet_summary:node-1: unable to fetch metrics from Kubelet node-1 (node-1): Get https://node-1:10250/stats/summary?only_cpu_and_memory=true: dial tcp: lookup node-1 on 10.96.0.10:53: no such host, unable to fully scrape metrics from source kubelet_summary:node-3: unable to fetch metrics from Kubelet node-3 (node-3): Get https://node-3:10250/stats/summary?only_cpu_and_memory=true: dial tcp: lookup node-3 on 10.96.0.10:53: no such host, unable to fully scrape metrics from source kubelet_summary:node-2: unable to fetch metrics from Kubelet node-2 (node-2): Get https://node-2:10250/stats/summary?only_cpu_and_memory=true: dial tcp: lookup node-2 on 10.96.0.10:53: no such host] 4、上述的报错信息提示pod中通过DNS无法解析主机名，可以通过在pod中定义hosts文件或告知metric-server优先使用IP的方式通讯，修改metric-server的deployment配置文件，修改如下并重新应用配置 5、应用metric-server部署文件后重新生成一个pod，日志中再次查看提示另外一个报错信息 [root@node-1 1.8+]# kubectl logs metrics-server-f54f5d6bf-s42rc -n kube-system -f I1230 11:45:26.615547 1 serving.go:312] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key) I1230 11:45:27.043723 1 secure_serving.go:116] Serving securely on [::]:4443 E1230 11:46:27.065274 1 manager.go:111] unable to fully collect metrics: [unable to fully","date":"2019-08-04","objectID":"/17-%E4%BD%BF%E7%94%A8metric-server%E8%AE%A9hpa%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E6%84%89%E5%BF%AB%E8%BF%90%E8%A1%8C/:3:0","tags":["kubernetes"],"title":"17 使用metric Server让HPA弹性伸缩愉快运行","uri":"/17-%E4%BD%BF%E7%94%A8metric-server%E8%AE%A9hpa%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E6%84%89%E5%BF%AB%E8%BF%90%E8%A1%8C/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.4 metric-server api测试 1、安装完metric-server后会增加一个metrics.k8s.io/v1beta1的API组，该API组通过Aggregator接入apiserver中 2、使用命令行查看kubectl top node的监控信息,可以看到CPU和内存的利用率 [root@node-1 1.8+]# kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% node-1 110m 5% 4127Mi 53% node-2 53m 5% 1066Mi 61% node-3 34m 3% 1002Mi 57% 3、查看pod监控信息,可以看到pod中CPU和内存的使用情况 [root@node-1 1.8+]# kubectl top pods NAME CPU(cores) MEMORY(bytes) haproxy-1-686c67b997-kw8pp 0m 1Mi haproxy-2-689b4f897-7cwmf 0m 1Mi haproxy-ingress-demo-5d487d4fc-5pgjt 0m 1Mi haproxy-ingress-demo-5d487d4fc-pst2q 0m 1Mi haproxy-ingress-demo-5d487d4fc-sr8tm 0m 1Mi ingress-demo-d77bdf4df-7kwbj 0m 1Mi ingress-demo-d77bdf4df-7x6jn 0m 1Mi ingress-demo-d77bdf4df-hr88b 0m 1Mi ingress-demo-d77bdf4df-wc22k 0m 1Mi service-1-7b66bf758f-xj9jh 0m 2Mi service-2-7c7444684d-w9cv9 1m 3Mi 4、除了用命令行连接metricc-server获取监控资源，还可以通过API方式链接方式获取，可用API有 http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes/ http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/pods http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/namespace//pods/\u003cpod-name 如下测试API接口的使用： a、创建一个kube proxy代理，用于链接apiserver，默认将监听在127的8001端口 [root@node-1 ~]# kubectl proxy Starting to serve on 127.0.0.1:8001 b、查看node列表的监控数据，可以获取到所有node的资源监控数据，usage中包含cpu和memory [root@node-1 ~]# curl http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1167 100 1167 0 0 393k 0 --:--:-- --:--:-- --:--:-- 569k { \"kind\": \"NodeMetricsList\", \"apiVersion\": \"metrics.k8s.io/v1beta1\", \"metadata\": { \"selfLink\": \"/apis/metrics.k8s.io/v1beta1/nodes\" }, \"items\": [ { \"metadata\": { \"name\": \"node-3\", \"selfLink\": \"/apis/metrics.k8s.io/v1beta1/nodes/node-3\", \"creationTimestamp\": \"2019-12-30T14:23:00Z\" }, \"timestamp\": \"2019-12-30T14:22:07Z\", \"window\": \"30s\", \"usage\": { \"cpu\": \"32868032n\", \"memory\": \"1027108Ki\" } }, { \"metadata\": { \"name\": \"node-1\", \"selfLink\": \"/apis/metrics.k8s.io/v1beta1/nodes/node-1\", \"creationTimestamp\": \"2019-12-30T14:23:00Z\" }, \"timestamp\": \"2019-12-30T14:22:07Z\", \"window\": \"30s\", \"usage\": { \"cpu\": \"108639556n\", \"memory\": \"4305356Ki\" } }, { \"metadata\": { \"name\": \"node-2\", \"selfLink\": \"/apis/metrics.k8s.io/v1beta1/nodes/node-2\", \"creationTimestamp\": \"2019-12-30T14:23:00Z\" }, \"timestamp\": \"2019-12-30T14:22:12Z\", \"window\": \"30s\", \"usage\": { \"cpu\": \"47607386n\", \"memory\": \"1119960Ki\" } } ] } c、指定某个具体的node访问到具体node的资源监控数据 [root@node-1 ~]# curl http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes/node-2 { \"kind\": \"NodeMetrics\", \"apiVersion\": \"metrics.k8s.io/v1beta1\", \"metadata\": { \"name\": \"node-2\", \"selfLink\": \"/apis/metrics.k8s.io/v1beta1/nodes/node-2\", \"creationTimestamp\": \"2019-12-30T14:24:39Z\" }, \"timestamp\": \"2019-12-30T14:24:12Z\", \"window\": \"30s\", \"usage\": { \"cpu\": \"43027609n\", \"memory\": \"1120168Ki\" } } d、查看所有pod的列表信息 curl http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/pods e、查看某个具体pod的监控数据 [root@node-1 ~]# curl http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/namespaces/default/pods/haproxy-ingress-demo-5d487d4fc-sr8tm { \"kind\": \"PodMetrics\", \"apiVersion\": \"metrics.k8s.io/v1beta1\", \"metadata\": { \"name\": \"haproxy-ingress-demo-5d487d4fc-sr8tm\", \"namespace\": \"default\", \"selfLink\": \"/apis/metrics.k8s.io/v1beta1/namespaces/default/pods/haproxy-ingress-demo-5d487d4fc-sr8tm\", \"creationTimestamp\": \"2019-12-30T14:36:30Z\" }, \"timestamp\": \"2019-12-30T14:36:13Z\", \"window\": \"30s\", \"containers\": [ { \"name\": \"haproxy-ingress-demo\", \"usage\": { \"cpu\": \"0\", \"memory\": \"1428Ki\" } } ] } 5、当然也可以通过kubectl -raw的方式访问接口,如调用node-3的数据 [root@node-1 ~]# kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/node-3 | jq . { \"kind\": \"NodeMetrics\", \"apiVersion\": \"metrics.k8s.io/v1beta1\", \"metadata\": { \"name\": \"node-3\", \"selfLink\": \"/apis/metrics.k8s.io/v1beta1/nodes/node-3\", \"creationTimestamp\": \"2019-12-30T14:44:46Z\" }, \"timestamp\": \"2019-12-30T14:44:09Z\", \"window\": \"30s\", \"usage\": { \"cpu\": \"35650151n\", \"memory\": \"1026820Ki\" } } 其他近似的接口有： kub","date":"2019-08-04","objectID":"/17-%E4%BD%BF%E7%94%A8metric-server%E8%AE%A9hpa%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E6%84%89%E5%BF%AB%E8%BF%90%E8%A1%8C/:4:0","tags":["kubernetes"],"title":"17 使用metric Server让HPA弹性伸缩愉快运行","uri":"/17-%E4%BD%BF%E7%94%A8metric-server%E8%AE%A9hpa%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E6%84%89%E5%BF%AB%E8%BF%90%E8%A1%8C/"},{"categories":["转载","kubernetes","基础教程"],"content":"3.1 HPA概述 The Horizontal Pod Autoscaler automatically scales the number of pods in a replication controller, deployment, replica set or stateful set based on observed CPU utilization (or, with custom metrics support, on some other application-provided metrics). Note that Horizontal Pod Autoscaling does not apply to objects that can’t be scaled, for example, DaemonSets. HPA即Horizontal Pod Autoscaler,Pod水平横向动态扩展，即根据应用分配资源使用情况，动态增加或者减少Pod副本数量，以实现集群资源的扩容，其实现机制为： HPA需要依赖于监控组件，调用监控数据实现动态伸缩，如调用Metrics API接口 HPA是二级的副本控制器，建立在Deployments，ReplicaSet，StatefulSets等副本控制器基础之上 HPA根据获取资源指标不同支持两个版本：v1和v2alpha1 HPA V1获取核心资源指标，如CPU和内存利用率，通过调用Metric-server API接口实现 HPA V2获取自定义监控指标，通过Prometheus获取监控数据实现 HPA根据资源API周期性调整副本数，检测周期horizontal-pod-autoscaler-sync-period定义的值，默认15s ","date":"2019-08-04","objectID":"/17-%E4%BD%BF%E7%94%A8metric-server%E8%AE%A9hpa%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E6%84%89%E5%BF%AB%E8%BF%90%E8%A1%8C/:5:0","tags":["kubernetes"],"title":"17 使用metric Server让HPA弹性伸缩愉快运行","uri":"/17-%E4%BD%BF%E7%94%A8metric-server%E8%AE%A9hpa%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E6%84%89%E5%BF%AB%E8%BF%90%E8%A1%8C/"},{"categories":["转载","kubernetes","基础教程"],"content":"3.2 HPA实现 如下开始延时HPA功能的实现，先创建一个Deployment副本控制器，然后再通过HPA定义资源度量策略，当CPU利用率超过requests分配的80%时即扩容。 1、创建Deployment副本控制器 [root@node-1 ~]# kubectl run hpa-demo --image=nginx:1.7.9 --port=80 --replicas=1 --expose=true --requests=\"'cpu=200m,memory=64Mi\" [root@node-1 ~]# kubectl get deployments hpa-demo -o yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"1\" creationTimestamp: \"2019-12-31T01:43:24Z\" generation: 1 labels: run: hpa-demo name: hpa-demo namespace: default resourceVersion: \"14451208\" selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/hpa-demo uid: 3b0f29e8-8606-4e52-8f5b-6c960d396136 spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: run: hpa-demo strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: run: hpa-demo spec: containers: - image: nginx:1.7.9 imagePullPolicy: IfNotPresent name: hpa-demo ports: - containerPort: 80 protocol: TCP resources: requests: cpu: 200m memory: 64Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: availableReplicas: 1 conditions: - lastTransitionTime: \"2019-12-31T01:43:25Z\" lastUpdateTime: \"2019-12-31T01:43:25Z\" message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \"True\" type: Available - lastTransitionTime: \"2019-12-31T01:43:24Z\" lastUpdateTime: \"2019-12-31T01:43:25Z\" message: ReplicaSet \"hpa-demo-755bdd875c\" has successfully progressed. reason: NewReplicaSetAvailable status: \"True\" type: Progressing observedGeneration: 1 readyReplicas: 1 replicas: 1 updatedReplicas: 1 2、创建HPA控制器，基于CPU实现横向扩展,策略为至少2个Pod，最大5个，targetCPUUtilizationPercentage表示CPU实际使用率占requests百分比 apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: hpa-demo spec: maxReplicas: 5 minReplicas: 2 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: hpa-demo targetCPUUtilizationPercentage: 80 3、应用HPA规则并查看详情，由于策略需确保最小2个副本，Deployment默认不是2个副本，因此需要扩容，在详情日志中看到副本扩展至2个 [root@node-1 ~]# kubectl apply -f hpa-demo.yaml horizontalpodautoscaler.autoscaling/hpa-demo created #查看HPA列表 [root@node-1 ~]# kubectl get horizontalpodautoscalers.autoscaling NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-demo Deployment/hpa-demo \u003cunknown\u003e/80% 2 5 0 7s #查看HPA详情 [root@node-1 ~]# kubectl describe horizontalpodautoscalers.autoscaling hpa-demo Name: hpa-demo Namespace: default Labels: \u003cnone\u003e Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"autoscaling/v1\",\"kind\":\"HorizontalPodAutoscaler\",\"metadata\":{\"annotations\":{},\"name\":\"hpa-demo\",\"namespace\":\"default\"},\"spe... CreationTimestamp: Tue, 31 Dec 2019 09:52:51 +0800 Reference: Deployment/hpa-demo Metrics: ( current / target ) resource cpu on pods (as a percentage of request): \u003cunknown\u003e / 80% Min replicas: 2 Max replicas: 5 Deployment pods: 1 current / 2 desired Conditions: Type Status Reason Message ---- ------ ------ ------- AbleToScale True SucceededRescale the HPA controller was able to update the target scale to 2 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulRescale 1s horizontal-pod-autoscaler New size: 2; reason: Current number of replicas below Spec.MinReplicas #副本扩容至2个，根据MinReplica的策略 4、查看Deployment列表校验确认扩容情况，已达到HPA基础最小化策略 [root@node-1 ~]# kubectl get deployments hpa-demo --show-labels NAME READY UP-TO-DATE AVAILABLE AGE LABELS hpa-demo 2/2 2 2 94m run=hpa-demo [root@node-1 ~]# kubectl get pods -l run=hpa-demo NAME READY STATUS RESTARTS AGE hpa-demo-5fcd9c757d-7q4td 1/1 Running 0 5m10s hpa-demo-5fcd9c757d-cq6k6 1/1 Running 0 10m 5、假如业务增长期间，CPU利用率增高，会自动横向增加Pod来实现，下面开始通过CPU压测来演示Deployment的扩展 [root@node-1 ~]# kubectl exec -it hpa-demo-5fcd9c757d-cq6k6 /bin/bash root@hpa-demo-5fcd9c757d-cq6k6:/# dd if=/dev/zero of=/dev/nu","date":"2019-08-04","objectID":"/17-%E4%BD%BF%E7%94%A8metric-server%E8%AE%A9hpa%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E6%84%89%E5%BF%AB%E8%BF%90%E8%A1%8C/:6:0","tags":["kubernetes"],"title":"17 使用metric Server让HPA弹性伸缩愉快运行","uri":"/17-%E4%BD%BF%E7%94%A8metric-server%E8%AE%A9hpa%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E6%84%89%E5%BF%AB%E8%BF%90%E8%A1%8C/"},{"categories":["转载","kubernetes","基础教程"],"content":"写在前面 上一个章节中kubernetes系列教程(十九)使用metric-server让HPA弹性伸缩愉快运行介绍了在kubernetes中的监控架构，通过安装和使用metric-server提供kubernetes中的核心监控指标：提供node节点和pod容器CPU和内存的监控能力，核心监控指标提供的监控维度和指标相对有限，需要更好的扩展监控能力，需要使用自定义监控来实现，本文介绍prometheus提供更更加丰富的自定义监控能力。 1. 初识prometheus ","date":"2019-08-04","objectID":"/18-prometheus%E6%8F%90%E4%BE%9B%E5%AE%8C%E5%A4%87%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/:0:0","tags":["kubernetes"],"title":"18 Prometheus提供完备监控系统","uri":"/18-prometheus%E6%8F%90%E4%BE%9B%E5%AE%8C%E5%A4%87%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.1 prometheus简介 Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud. It is now a standalone open source project and maintained independently of any company. To emphasize this, and to clarify the project’s governance structure, Prometheus joined theCloud Native Computing Foundationin 2016 as the second hosted project, afterKubernetes. Prometheus是一个开源的监控系统+告警系统工具集，最早由SoudCloud开发，目前已被很多公司广泛使用，于2016年加入CNCF组织，成为继kubernetes之后第二个管理的项目。得益于kubernetes的火热，prometheus被越来越多的企业应用，已成为新一代的监控系统，成为CNCF第二个毕业的项目。 prometheus特点： 一个指标和键值对标识的时间序列化多维度数据模型 PromQL提供一个便捷查询语言实现多维度数据查询 不依赖于分布式存储，单个节点能提供自治功能 通过HTTP协议拉取时间系列数据模型 支持通过gateway主动推送时间序列 支持服务发现或者静态配置发现节点 内置有多维度数据画图和集成grafana数据展示 ","date":"2019-08-04","objectID":"/18-prometheus%E6%8F%90%E4%BE%9B%E5%AE%8C%E5%A4%87%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/:1:0","tags":["kubernetes"],"title":"18 Prometheus提供完备监控系统","uri":"/18-prometheus%E6%8F%90%E4%BE%9B%E5%AE%8C%E5%A4%87%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2 prometheus架构 prometheus架构： prometheus-server，prometheus主服务端，从exporters端采集和存储数据，并提供PromQL数据查询语言 Retrieval 采集模块，从exporters和pushgateway中采集数据，采集数据经过一定规则处理 TSDB 数据存储，TSDB是时序化数据库，将Retrieval采集数据存储，默认存储在本地 http server 提供http接口查询和数据展板，默认端口是9090，可以登陆查询监控指标和绘图 PromQL 提供边界的PromQL语言，用于数据统计，数据输出和数据展示接口集成 数据采集，数据采集模块，包含两种数据采集方式：拉去pull和推送push Jobs exporters 采集宿主机和container的性能指标，通过http方式拉取，支持多种不同数据类型采集 Short-lived jobs 瞬时在线任务，适用于实时监控指标，server端拉去时可能消失了，采用主动上报机制 Pushgateway 推动网关，Short-lived jobs将数据主动push到过gateway，server再从gateway拉取 数据展示，借助于PromQL语言实现实现数据的展示，包含还有prometheus UI，Gafana和API clients Prometheus Web UI，prometheus默认提一个数据查询和画图展示的UI，通过http 9090端口 Grafana，一个开源非常优秀绚烂的数据展示框架，从Prometheus中获取数据，采用模版绘图 API Clients，支持多种不同的客户端SDK语言，包括Go，python，Java等，便于编写开发监控系统 告警系统，从server接受告警，推送给AlertManager告警系统，告警系统接受告警信息去重，分组。通知包含 pageduty Email，邮件告警，结合smtp 其他，如webhook等 服务发现，借助于第三方接口实现服务机制，如DNS，Consul，Kubernetes等，如和kubernetes apiserver结合，获取目标target的是列表，并定期轮训获取到监控数据。 2. prometheus和kubernetes结合 ","date":"2019-08-04","objectID":"/18-prometheus%E6%8F%90%E4%BE%9B%E5%AE%8C%E5%A4%87%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/:2:0","tags":["kubernetes"],"title":"18 Prometheus提供完备监控系统","uri":"/18-prometheus%E6%8F%90%E4%BE%9B%E5%AE%8C%E5%A4%87%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.1 prometheus安装简介 prometheus安装涉及较多的组件，因此给安装带来较大的困难，kube-prometheus是coreos公司提供在kubernets中自动安装prometheus的组件，为集成kuberntes提供的安装，包含如下组件： The Prometheus Operator prometheus核心组件 Highly available Prometheus 提供高可用能力 Highly available Alertmanager 告警管理器 Prometheus node-exporter 数据采集组件 Prometheus Adapter for Kubernetes Metrics APIs 和kubernetes集成的适配器 kube-state-metrics 指标监控转换，使之适配kubernetes风格的接口 Grafana 数据展示 安装环境： 1、kubernetes版本,1.15.3 [root@node-1 ~]# kubectl version Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:13:54Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"} Server Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:05:50Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"} 2、prometheus版本 prometheus v2.12.0 prometheus-operator v0.34.0 node-exporter v0.18.1 alertmanager v0.20.0 grafana 6.4.3 ","date":"2019-08-04","objectID":"/18-prometheus%E6%8F%90%E4%BE%9B%E5%AE%8C%E5%A4%87%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/:3:0","tags":["kubernetes"],"title":"18 Prometheus提供完备监控系统","uri":"/18-prometheus%E6%8F%90%E4%BE%9B%E5%AE%8C%E5%A4%87%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.2 prometheus安装 1、获取kube-prometheus安装源 [root@node-1 ~]# git clone https://github.com/coreos/kube-prometheus.git 2、快速安装prometheus组件，相关的setup包的yaml文件在setup目录下,包含有很多自定义的CRD资源 [root@node-1 ~]# kubectl apply -f kube-prometheus/manifests/setup/ namespace/monitoring created customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com unchanged customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com unchanged customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com unchanged customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created clusterrole.rbac.authorization.k8s.io/prometheus-operator created clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created deployment.apps/prometheus-operator created service/prometheus-operator created serviceaccount/prometheus-operator created 校验CRD资源安装情况,prometheus,alertmanagers,rules,servicemonitor均以CRM资源的方式部署 [root@node-1 ~]# kubectl get customresourcedefinitions.apiextensions.k8s.io |grep monitoring alertmanagers.monitoring.coreos.com 2020-01-30T05:36:58Z podmonitors.monitoring.coreos.com 2020-01-30T05:37:06Z prometheuses.monitoring.coreos.com 2020-01-30T05:37:18Z prometheusrules.monitoring.coreos.com 2020-01-30T05:44:05Z servicemonitors.monitoring.coreos.com 2020-01-30T05:44:06Z 部署了一个prometheus-operator的deployments和services [root@node-1 ~]# kubectl get deployments -n monitoring NAME READY UP-TO-DATE AVAILABLE AGE prometheus-operator 1/1 1 1 3m5s [root@node-1 ~]# kubectl get services -n monitoring NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE prometheus-operator ClusterIP None \u003cnone\u003e 8080/TCP 3m13s 3、部署prometheus其他组件，包含kube-state-metric，grafana，node-exporter，alertmanager，prometheus-adapter，prometheus，组件包含在manifest所在目录，安装组件的角色如下： prometheus prometheus核心组件 prometheus-adapter prometheus适配器，做数据转换 kube-state-metrics kubernetes指标转换器，转换为apiserver能识别的指标 alertmanager 告警管理器，用于指标阀值告警实现 node-exporter exporters，客户端监控上报agent，用于实现数据上报 grafana 数据显示展板 configmaps grafana数据展板配置模版，封装在configmap中 clusterrole，clusterrolebinding prometheus访问kubernetes的RBAC授权 [root@node-1 ~]# kubectl apply -f kube-prometheus/manifests/ alertmanager.monitoring.coreos.com/main created secret/alertmanager-main created service/alertmanager-main created serviceaccount/alertmanager-main created servicemonitor.monitoring.coreos.com/alertmanager created secret/grafana-datasources created configmap/grafana-dashboard-apiserver created configmap/grafana-dashboard-cluster-total created configmap/grafana-dashboard-controller-manager created configmap/grafana-dashboard-k8s-resources-cluster created configmap/grafana-dashboard-k8s-resources-namespace created configmap/grafana-dashboard-k8s-resources-node created configmap/grafana-dashboard-k8s-resources-pod created configmap/grafana-dashboard-k8s-resources-workload created configmap/grafana-dashboard-k8s-resources-workloads-namespace created configmap/grafana-dashboard-kubelet created configmap/grafana-dashboard-namespace-by-pod created configmap/grafana-dashboard-namespace-by-workload created configmap/grafana-dashboard-node-cluster-rsrc-use created configmap/grafana-dashboard-node-rsrc-use created configmap/grafana-dashboard-nodes created configmap/grafana-dashboard-persistentvolumesusage created configmap/grafana-dashboard-pod-total created configmap/grafana-dashboard-pods created configmap/grafana-dashboard-prometheus-remote-write created configmap/grafana-dashboard-prometheus created configmap/grafana-dashboard-proxy created configmap/grafana-dashboard-scheduler created configmap/grafana-dashboard-statefulset created configmap/grafana-dashboard-workload-total created configmap/grafana-dashboards created deployment.apps/grafana created service/grafana created serviceaccount/grafana created servicemonitor.monitoring.coreos.com/grafana created clusterrole.rbac.authorization.k8s.io/kube-state-metric","date":"2019-08-04","objectID":"/18-prometheus%E6%8F%90%E4%BE%9B%E5%AE%8C%E5%A4%87%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/:4:0","tags":["kubernetes"],"title":"18 Prometheus提供完备监控系统","uri":"/18-prometheus%E6%8F%90%E4%BE%9B%E5%AE%8C%E5%A4%87%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/"},{"categories":["转载","kubernetes","基础教程"],"content":"3.1 prometheus原生指标 prometheus-k8s默认提供ClusterIP开放9090端口用于集群内部，修改为NodePort供集群外部访问，如下修改将prometheus-k8s的类型修改为NodePort类型 [root@node-1 ~]# kubectl patch -p '{\"spec\":{\"type\": \"NodePort\"}}' services -n monitoring prometheus-k8s service/prometheus-k8s patched [root@node-1 ~]# kubectl get services -n monitoring prometheus-k8s -o yaml apiVersion: v1 kind: Service metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"prometheus\":\"k8s\"},\"name\":\"prometheus-k8s\",\"namespace\":\"monitoring\"},\"spec\":{\"ports\":[{\"name\":\"web\",\"port\":9090,\"targetPort\":\"web\"}],\"selector\":{\"app\":\"prometheus\",\"prometheus\":\"k8s\"},\"sessionAffinity\":\"ClientIP\"}} creationTimestamp: \"2020-01-30T09:04:03Z\" labels: prometheus: k8s name: prometheus-k8s namespace: monitoring resourceVersion: \"18773330\" selfLink: /api/v1/namespaces/monitoring/services/prometheus-k8s uid: 272e8f9a-d412-4f42-9553-da5f7e71cf2f spec: clusterIP: 10.108.126.97 externalTrafficPolicy: Cluster ports: - name: web nodePort: 31924 #NodePort端口 port: 9090 protocol: TCP targetPort: web selector: app: prometheus prometheus: k8s sessionAffinity: ClientIP sessionAffinityConfig: clientIP: timeoutSeconds: 10800 type: NodePort status: loadBalancer: {} 1、查询prometheus监控指标，prometheus包含有丰富的指标，可以选择不同的监控指标 选择container_cpu_usage_seconds_total查询容器的cpu使用率为例，执行Excute执行查询，可以获取到所有容器的cpu使用数据，切换至Graph可以绘制简单的图像，图像显示相对简单，指标通过grafana显示会更绚烂，一般较少使用prometheus的绘图功能 2、服务发现，用于动态发现prometheus服务相关的组件，并定期向服务组件拉取数据 3、内置告警规则，默认内置定义有alert告警规则，用户实现监控告警切换到alerts为告警的内容，可以看到告警的指标。 prometheus提供了丰富的监控指标metric，并通过9090的http端口提供了外部访问监控指标和简单绘图的功能，相比grafana而言，图形界面的功能相对简单，主要用于查询prometheus数据，借助于PromQL语言查询监控指标数据。 ","date":"2019-08-04","objectID":"/18-prometheus%E6%8F%90%E4%BE%9B%E5%AE%8C%E5%A4%87%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/:5:0","tags":["kubernetes"],"title":"18 Prometheus提供完备监控系统","uri":"/18-prometheus%E6%8F%90%E4%BE%9B%E5%AE%8C%E5%A4%87%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/"},{"categories":["转载","kubernetes","基础教程"],"content":"3.2 grafana数据展示 相比于prometheus web UI，grafana能够提供更丰富的数据展示功能，起借助于PromQL语言实现丰富的数据查询并通过模版展示控制台，grafana默认的3000端口并未对外部开放，为了从集群外部访问grafana，需要将grafana的servcie类型修改为NodePort,开放NodePort端口为30923 [root@node-1 ~]# kubectl patch -p '{\"spec\": {\"type\": \"NodePort\"}}' services grafana -n monitoring service/grafana patched [root@node-1 ~]# kubectl get services grafana -n monitoring NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE grafana NodePort 10.105.229.156 \u003cnone\u003e 3000:30923/TCP 114m 外部通过30923端口访问grafana，初始默认登陆的用户名和密码均为admin，首次登陆grafana会提示修改用户密码，密码符合复杂性要求，如下为登陆后的grafana的展板显示 1、kubernetes集群监控，包含有整个集群CPU资源使用+分配，内存资源使用+分配，CPU配额，网络资源等，可以全局看到集群资源的使用情况 2、Node节点监控，可以看到kubernetes集群中特定某个节点的资源情况啊：CPU使用率，CPU负载，内存使用率，磁盘IO，磁盘空间，网络带宽，网络传输等指标 3、Pod监控，可以查看到命名空间下pod的资源情况：容器CPU使用率，内存使用，磁盘IO，磁盘空间等 4、kubernetes工作负载监控，可以和查看到Deployment，StatefulSets，DaemonSets 5、网络监控，可以看到集群Cluster级别网络监控、工作负载Workload级别网络监控和Pod级别网络监控，包括网络的发送数据，接受数据，出带宽和入带宽指标。 6、grafana默认还提供了其他很多的监控指标，比如apiserver，kubelet，pv等 写在最后 本文总结了在kubernetes中使用prometheus提供完备的自定义监控系统，通过grafana展示更丰富绚烂的图标内容，相比于核心监控指标metric-server而言，prometheus能够提供更加丰富的监控指标，且这些自定义监控指标能用于HPA V2（参考官方说明）中实现更丰富的弹性扩展伸缩能力，毫无疑问，prometheus的出现让kubernetes的监控变得更简单而功能丰富。 参考文献 prometheus官网：https://prometheus.io kube-prometheus安装官档：https://github.com/coreos/kube-prometheus TKE自动弹性伸缩指标说明：https://cloud.tencent.com/document/product/457/38929 HPA使用说明：https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/ 『 转载 』该文章来源于网络，侵删。 ","date":"2019-08-04","objectID":"/18-prometheus%E6%8F%90%E4%BE%9B%E5%AE%8C%E5%A4%87%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/:6:0","tags":["kubernetes"],"title":"18 Prometheus提供完备监控系统","uri":"/18-prometheus%E6%8F%90%E4%BE%9B%E5%AE%8C%E5%A4%87%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/"},{"categories":["转载","kubernetes","基础教程"],"content":"1. kubernetes简介 ","date":"2019-08-04","objectID":"/01-%E5%88%9D%E6%8E%A2kubernetes%E5%8A%9F%E8%83%BD%E4%B8%8E%E7%BB%84%E4%BB%B6/:0:0","tags":["kubernetes"],"title":"01 初探kubernetes功能与组件","uri":"/01-%E5%88%9D%E6%8E%A2kubernetes%E5%8A%9F%E8%83%BD%E4%B8%8E%E7%BB%84%E4%BB%B6/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.1 docker容器技术 Docker provides a way to run applications securely isolated in a container, packaged with all its dependencies and libraries.Build once, Run anywhwere. Docker提供了一种将应用程序安全，隔离运行的一种方式，能够将应用程序依赖和库文件打包在一个容器中，后续再任何地方运行起来即可，其包含了应用程序所依赖相关环境，一次构建，任意运行（build once，run anywhere） Docker组成： Docker Daemon 容器管理组件，守护进程，负载容器，镜像，存储，网络等管理 Docker Client 容器客户端，负责和Docker Daemon交互，完成容器生命周期管理 Docker Registry 容器镜像仓库，负责存储，分发，打包 Docker Object 容器对象，主要包含container和images 容器给应用程序开发环境带来很大的便利，从根本上解决了容器的环境依赖，打包等问题，然而，Docker带来的容器打包的便利，同时也带来了以下的挑战： 容器如何调度，分发 多台机器如何协同工作 Docker主机故障时应用如何恢复 如何保障应用高可用，横向扩展，动态伸缩 ","date":"2019-08-04","objectID":"/01-%E5%88%9D%E6%8E%A2kubernetes%E5%8A%9F%E8%83%BD%E4%B8%8E%E7%BB%84%E4%BB%B6/:1:0","tags":["kubernetes"],"title":"01 初探kubernetes功能与组件","uri":"/01-%E5%88%9D%E6%8E%A2kubernetes%E5%8A%9F%E8%83%BD%E4%B8%8E%E7%BB%84%E4%BB%B6/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2 kubernetes简介与功能 Kubernetes (K8s) is an open-source system for automating deployment, scaling, and management of containerized applications.It groups containers that make up an application into logical units for easy management and discovery. Kubernetes builds upon15 years of experience of running production workloads at Google, combined with best-of-breed ideas and practices from the community. Kubernetes是google开源的一套微服务，容器化的编排引擎，提供容器话应用的自动化部署，横向扩展和管理，是google内部容器十多年实战沉淀的结晶，已战胜Swarm，Mesos成为容器编排的行业标准。 三大容器编排引擎： Swarm Docker原生提供的容器化编排引擎，随着docker支持kubernetes逐渐废弃 Mesos 结合Marathon提供容器调度编排的能力，还能提供其他framwork的调度 Kubernetes 已成为容器编排引擎的唯一标准，越来越多程序支持kubernetes。 kuberntes内置有很多非常优秀的特性使开发者专注于业务本身，其包含的功能如下： Service discovery and load balancing，服务发现和负载均衡，通过DNS实现内部解析，service实现负载均衡 Storage orchestration，存储编排，通过plungin的形式支持多种存储，如本地，nfs，ceph，公有云快存储等 Automated rollouts and rollbacks，自动发布与回滚，通过匹配当前状态与目标状态一致，更新失败时可回滚 Automatic bin packing，自动资源调度，可以设置pod调度的所需（requests）资源和限制资源（limits） Self-healing，内置的健康检查策略，自动发现和处理集群内的异常，更换，需重启的pod节点 Secret and configuration management，密钥和配置管理，对于敏感信息如密码，账号的那个通过secret存储，应用的配置文件通过configmap存储，避免将配置文件固定在镜像中，增加容器编排的灵活性 Batch execution，批处理执行，通过job和cronjob提供单次批处理任务和循环计划任务功能的实现 Horizontal scaling,横向扩展功能，包含有HPA和AS，即应用的基于CPU利用率的弹性伸缩和基于平台级的弹性伸缩，如自动增加node和删除nodes节点。 ","date":"2019-08-04","objectID":"/01-%E5%88%9D%E6%8E%A2kubernetes%E5%8A%9F%E8%83%BD%E4%B8%8E%E7%BB%84%E4%BB%B6/:2:0","tags":["kubernetes"],"title":"01 初探kubernetes功能与组件","uri":"/01-%E5%88%9D%E6%8E%A2kubernetes%E5%8A%9F%E8%83%BD%E4%B8%8E%E7%BB%84%E4%BB%B6/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.3 kubernetes架构解析 kubernetes包含两种角色：master节点和node节点，master节点是集群的控制管理节点，作为整个k8s集群的大脑。 负责集群所有接入请求(kube-apiserver)，在整个集群的入口； 集群资源调度(kube-controller-scheduler)，通过watch监视pod的创建，负责将pod调度到合适的node节点； 集群状态的一致性(kube-controller-manager)，通过多种控制器确保集群的一致性，包含有Node Controller，Replication Controller，Endpoints Controller等； 元数据信息存储(etcd)，数据持久存储化，存储集群中包括node，pod，rc，service等数据； node节点是实际的工作节点，负责集群负载的实际运行，即pod运行的载体，其通常包含三个组件：Container Runtime，kubelet和kube-proxy Container Runtime是容器运行时，负责实现container生命周期管理，如docker，containerd，rktlet； kubelet负责镜像和pod的管理， kube-proxy是service服务实现的抽象，负责维护和转发pod的路由，实现集群内部和外部网络的访问。 其他组件还包括： cloud-controller-manager，用于公有云的接入实现，提供节点管理(node)，路由管理，服务管理(LoadBalancer和Ingress)，存储管理(Volume，如云盘，NAS接入)，需要由公有云厂商实现具体的细节，kubernetes提供实现接口的接入，如腾讯云目前提供CVM的node管理，节点的弹性伸缩(AS),负载均衡的接入(CLB),存储的管理(CBS和CFS)等产品的集成； DNS组件由kube-dns或coredns实现集群内的名称解析； kubernetes-dashboard用于图形界面管理； kubectl命令行工具进行API交互； 服务外部接入，通过ingress实现七层接入，由多种controller控制器组成 traefik nginx ingress controller haproxy ingress controller 公有云厂商ingress controller 监控系统用于采集node和pod的监控数据 metric-server 核心指标监控 prometheus 自定义指标监控，提供丰富功能 heapster+influxdb+grafana 旧核心指标监控方案，现已废弃 日志采集系统，用于收集容器的业务数据,实现日志的采集，存储和展示，由EFK实现 Fluentd 日志采集 ElasticSearch 日志存储+检索 Kiabana 数据展示 ","date":"2019-08-04","objectID":"/01-%E5%88%9D%E6%8E%A2kubernetes%E5%8A%9F%E8%83%BD%E4%B8%8E%E7%BB%84%E4%BB%B6/:3:0","tags":["kubernetes"],"title":"01 初探kubernetes功能与组件","uri":"/01-%E5%88%9D%E6%8E%A2kubernetes%E5%8A%9F%E8%83%BD%E4%B8%8E%E7%BB%84%E4%BB%B6/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.4 kubernetes高可用架构 kubernetes高可用集群通常由3或5个节点组成高可用集群，需要保障各个节点的高可用性 etcd 内置集群机制，保障数据持久存储 kube-apiserver 无状态api服务，有负载均衡调度器做负载分发，如haproxy或nginx kube-scheduler 内置选举机制，保障调度器高可用，确保同个时刻一个leader节点工作，其他处于阻塞，防止脑裂 kube-controller-manager 内置的选举机制保障控制器高可用，机制和kube-scheduler一致。 参考文档 \\1. kubernetes功能介绍，https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/ \\2. kubernetes组件介绍https://kubernetes.io/docs/concepts/overview/components/ 该系列文章声明 {% cq %} 该kubernetes系列教程均来源于 @happylau ，仅做略微修改，仅限于个人学习使用。 {% endcq %} 为防止原系列教程失效，故作此备份. 『 转载 』该文章来源于网络，侵删。 ","date":"2019-08-04","objectID":"/01-%E5%88%9D%E6%8E%A2kubernetes%E5%8A%9F%E8%83%BD%E4%B8%8E%E7%BB%84%E4%BB%B6/:4:0","tags":["kubernetes"],"title":"01 初探kubernetes功能与组件","uri":"/01-%E5%88%9D%E6%8E%A2kubernetes%E5%8A%9F%E8%83%BD%E4%B8%8E%E7%BB%84%E4%BB%B6/"},{"categories":["转载","kubernetes","基础教程"],"content":"写在前面 本章是kubernetes系列教程第二篇，要深入学习kubernetes，首先需要有一个k8s环境，然而，受制硬件环境，网络环境等因素，要搭建一个环境有一定的困难，让很多初学者望而却步，本章主要介绍通过kubeadm安装工具部署kubernetes集群，考虑到国内网络限制，已将安装镜像通过跳板机下载到本地，方便大家离线安装。 1. MiniKube快速部署环境 ","date":"2019-08-04","objectID":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/:0:0","tags":["kubernetes"],"title":"02 Kubeadm离线部署1.14","uri":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.1 安装概述 要学习kubernetes，首先需要有一个kubernetes集群，社区为了满足不同场景下，提供了不同的安装方法以适应各种场景需求，常见方法有： MiniKube，是一个将kubernetes单节点安装在本地虚拟化工具，MiniKube安装文档 二进制安装，通过已编译好的二进制文件安装，需设置参数，可定制化强，安装难度大 Kubeadm，一个自动化安装工具，以镜像的方式部署，使用简单，镜像在谷歌仓库，下载易失败 对于学习环境，Katacoda提供了一个在线的MiniKube环境，只需在控制台启用即可使用，当然也可以将MiniKube下载到本地使用。对于生产环境，推荐使用二进制安装或者Kubeadm，新版kubeadm目前已将kubernetes管理组件以pod的形式部署在集群中，不管用哪种方式，受限于GFW，大部分镜像下载，大家自行补脑和解决，本文以离线的方式安装部署，根据安装版本下载对应的安装镜像倒入系统即可。 1.14.1安装镜像下载链接 v1.17.0安装镜像下载链接 ","date":"2019-08-04","objectID":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/:1:0","tags":["kubernetes"],"title":"02 Kubeadm离线部署1.14","uri":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2 MiniKube在线环境 Katacoda使用MiniKube提供了一个在线部署kubernetres环境，当然也可以基于MiniKube本地安装，如果是初学者想初探一下kubernetes的功能，可以使用Katacoda提供的线上环境，达到快速入门学习的目的。参考文档，直接在Hello MiniKube文档中点击Open terminal即可自动创建一个kubernetes环境，其会自动拉取镜像并部署所需环境。 如上图所示，MiniKube的提供的优点如下： 快捷，自动部署环境 无需占用本地资源 适用于学习环境 2 kubeadm部署k8s集群 ","date":"2019-08-04","objectID":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/:2:0","tags":["kubernetes"],"title":"02 Kubeadm离线部署1.14","uri":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.1 环境说明和准备 【软件版本】 软件名 软件版本 OS CentOS Linux release 7.6.1810 (Core) Docker docker-ce-18.03.1.ce-1.el7 Kubernetes 1.14.1 Kubeadm kubeadm-1.14.1-0.x86_64 etcd 3.3.10 flannel v0.11.0 【环境说明】 三台机器均为腾讯云上购买的CVM（Cloud Virtual Machine），机器配置是2vcpu+4G memory+50G disk 主机名 角色 IP地址 软件 node-1 master 10.254.100.101 docker,kubelet,etcd,kube-apiserver,kube-controller-manager,kube-scheduler node-2 worker 10.254.100.102 docker,kubelet,kube-proxy,flannel node-3 worker 10.254.100.103 docker,kubelet,kube-proxy,flannel 【环境准备】 1、设置主机名，其他两个节点类似设置 root@VM_100_101_centos ~# hostnamectl set-hostname node-1 root@VM_100_101_centos ~# hostname node-1 2、设置hosts文件，其他两个节点设置相同内容 root@node-1 ~# vim /etc/hosts 127.0.0.1 localhost localhost.localdomain 10.254.100.101 node-1 10.254.100.102 node-2 10.254.100.103 node-3 3、设置SSH无密码登录，并通过ssh-copy-id将公钥拷贝到对端 #生成密钥对 root@node-1 .ssh# ssh-keygen -P '' Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: SHA256:zultDMEL8bZmpbUjQahVjthVAcEkN929w5EkUmPkOrU root@node-1 The key's randomart image is: +---RSA 2048----+ | .=O=+=o.. | | o+o..+.o+ | | .oo=. o. o | | . . * oo .+ | | oSOo.E . | | oO.o. | | o++ . | | . .o | | ... | +----SHA256-----+ #拷贝公钥到node-2和node-3节点 root@node-1 .ssh# ssh-copy-id -i /root/.ssh/id_rsa.pub node-2: /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/root/.ssh/id_rsa.pub\" The authenticity of host 'node-1 (10.254.100.101)' can't be established. ECDSA key fingerprint is SHA256:jLUH0exgyJdsy0frw9R+FiWy+0o54LgB6dgVdfc6SEE. ECDSA key fingerprint is MD5:f4:86:a8:0e:a6:03:fc:a6:04:df:91:d8:7a:a7:0d:9e. Are you sure you want to continue connecting (yes/no)? yes /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys root@node-1's password: Number of key(s) added: 1 Now try logging into the machine, with: \"ssh 'node-2'\" and check to make sure that only the key(s) you wanted were added. 4、关闭防火墙和SElinux [root@node-1 ~]# systemctl stop firewalld [root@node-1 ~]# systemctl disable firewalld [root@node-1 ~]# sed -i '/^SELINUX=/ s/enforcing/disabled/g' /etc/selinux/config [root@node-1 ~]# setenforce 0 ","date":"2019-08-04","objectID":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/:3:0","tags":["kubernetes"],"title":"02 Kubeadm离线部署1.14","uri":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.2 安装Docker环境 1、下载docker的yum源 [root@node-1 ~]# cd /etc/yum.repos.d/ [root@node-1 ~]# wget https://download.docker.com/linux/centos/docker-ce.repo 2、设置cgroup driver类型为systemd [root@node-1 ~]# cat \u003e /etc/docker/daemon.json \u003c\u003cEOF \u003e { \u003e \"exec-opts\": [\"native.cgroupdriver=systemd\"], \u003e \"log-driver\": \"json-file\", \u003e \"log-opts\": { \u003e \"max-size\": \"100m\" \u003e }, \u003e \"storage-driver\": \"overlay2\", \u003e \"storage-opts\": [ \u003e \"overlay2.override_kernel_check=true\" \u003e ] \u003e } \u003e EOF 3、启动docker服务并验证，可以通过docker info查看docker安装的版本等信息 [root@node-1 ~]# systemctl restart docker [root@node-1 ~]# systemctl enable docker 备注：如果机器不具备上网条件，或者访问docker的yum源仓库很慢，我已将docker相关rpm包依赖包下载到腾讯云cos中，下载链接，可以下载到本地，然后解压缩然后运行yum localinstall进行安装。 ","date":"2019-08-04","objectID":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/:4:0","tags":["kubernetes"],"title":"02 Kubeadm离线部署1.14","uri":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.3 安装kubeadm组件 1、安装kubernetes源，国内可以使用阿里的kubernetes源，速度会快一点 [root@node-1 ~]#cat \u003c\u003cEOF \u003e /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 2、安装kubeadm，kubelet，kubectl，会自动安装几个重要依赖包：socat，cri-tools，cni等包 [root@node-1 ~]# yum install kubeadm-1.14.1-0 kubectl-1.14.1-0 kubelet-1.14.1-0 --disableexcludes=kubernetes -y 3、设置iptables网桥参数 [root@node-1 ~]# cat \u003c\u003cEOF \u003e /etc/sysctl.d/k8s.conf \u003e net.bridge.bridge-nf-call-ip6tables = 1 \u003e net.bridge.bridge-nf-call-iptables = 1 \u003e EOF [root@node-1 ~]# sysctl --system,然后使用sysctl -a|grep 参数的方式验证是否生效 4、重新启动kubelet服务，使配置生效 [root@node-1 ~]# systemctl restart kubelet [root@node-1 ~]# systemctl enable kubelet 备注：如果本地下载很慢或者无法下载kubernetes中yum源rpm包，可以通过离线方式下载，下载路径 ","date":"2019-08-04","objectID":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/:5:0","tags":["kubernetes"],"title":"02 Kubeadm离线部署1.14","uri":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.4 导入kubernetes镜像 1、从cos中下载kubernetes安装镜像，并通过docker load命令将镜像导入到环境中 [root@node-1 v1.14.1]# docker image load -i etcd:3.3.10.tar [root@node-1 v1.14.1]# docker image load -i pause:3.1.tar [root@node-1 v1.14.1]# docker image load -i coredns:1.3.1.tar [root@node-1 v1.14.1]# docker image load -i flannel:v0.11.0-amd64.tar [root@node-1 v1.14.1]# docker image load -i kube-apiserver:v1.14.1.tar [root@node-1 v1.14.1]# docker image load -i kube-controller-manager:v1.14.1.tar [root@node-1 v1.14.1]# docker image load -i kube-scheduler:v1.14.1.tar [root@node-1 v1.14.1]# docker image load -i kube-proxy:v1.14.1.tar 2、检查镜像列表 [root@node-1 v1.14.1]# docker image list REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.1 20a2d7035165 3 months ago 82.1MB k8s.gcr.io/kube-apiserver v1.14.1 cfaa4ad74c37 3 months ago 210MB k8s.gcr.io/kube-scheduler v1.14.1 8931473d5bdb 3 months ago 81.6MB k8s.gcr.io/kube-controller-manager v1.14.1 efb3887b411d 3 months ago 158MB quay.io/coreos/flannel v0.11.0-amd64 ff281650a721 6 months ago 52.6MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 8 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB ","date":"2019-08-04","objectID":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/:6:0","tags":["kubernetes"],"title":"02 Kubeadm离线部署1.14","uri":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.5 kubeadm初始化集群 1、 kubeadm初始化集群，需要设置初始参数 –pod-network-cidr指定pod使用的网段，设置值根据不同的网络plugin选择，本文以flannel为例设置值为10.244.0.0/16 container runtime可以通过–cri-socket指定socket文件所属路径 如果有多个网卡可以通过–apiserver-advertise-address指定master地址，默认会选择访问外网的ip [root@node-1 ~]# kubeadm init --apiserver-advertise-address 10.254.100.101 --apiserver-bind-port 6443 --kubernetes-version 1.14.1 --pod-network-cidr 10.244.0.0/16 [init] Using Kubernetes version: v1.14.1 [preflight] Running pre-flight checks [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 18.03.1-ce. Latest validated version: 18.09 [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'#下载镜像 [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Activating the kubelet service [certs] Using certificateDir folder \"/etc/kubernetes/pki\"#生成CA等证书 [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [node-1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.254.100.101] [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [node-1 localhost] and IPs [10.254.100.101 127.0.0.1 ::1] [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [node-1 localhost] and IPs [10.254.100.101 127.0.0.1 ::1] [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [control-plane] Using manifest folder \"/etc/kubernetes/manifests\"#生成master节点静态pod配置文件 [control-plane] Creating static Pod manifest for \"kube-apiserver\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [apiclient] All control plane components are healthy after 18.012370 seconds [upload-config] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.14\" in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --experimental-upload-certs [mark-control-plane] Marking the node node-1 as control-plane by adding the label \"node-role.kubernetes.io/master=''\" [mark-control-plane] Marking the node node-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: r8n5f2.9mic7opmrwjakled [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles#配置RBAC授权 [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controlle","date":"2019-08-04","objectID":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/:7:0","tags":["kubernetes"],"title":"02 Kubeadm离线部署1.14","uri":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.6 验证kubernetes组件 1、验证node状态，获取当前安装节点，可以查看到状态， 角色，启动市场，版本， [root@node-1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION node-1 Ready master 46m v1.14.1 node-2 Ready \u003cnone\u003e 34m v1.14.1 node-3 Ready \u003cnone\u003e 32m v1.14.1 2、查看kubernetse服务组件状态,包括scheduler，controller-manager，etcd [root@node-1 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\":\"true\"} 3、查看pod的情况,master中的角色包括kube-apiserver，kube-scheduler，kube-controller-manager，etcd，coredns以pods形式部署在集群中，worker节点的kube-proxy也以pod的形式部署。实际上pod是以其他控制器如daemonset的形式控制的。 [root@node-1 ~]# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-fb8b8dccf-hrqm8 1/1 Running 0 50m coredns-fb8b8dccf-qwwks 1/1 Running 0 50m etcd-node-1 1/1 Running 0 48m kube-apiserver-node-1 1/1 Running 0 49m kube-controller-manager-node-1 1/1 Running 0 49m kube-proxy-lfckv 1/1 Running 0 38m kube-proxy-x5t6r 1/1 Running 0 50m kube-proxy-x8zqh 1/1 Running 0 36m kube-scheduler-node-1 1/1 Running 0 49m ","date":"2019-08-04","objectID":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/:8:0","tags":["kubernetes"],"title":"02 Kubeadm离线部署1.14","uri":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.7 配置kubectl命令补全 使用kubectl和kubernetes交互时候可以使用缩写模式也可以使用完整模式，如kubectl get nodes和kubectl get no能实现一样的效果，为了提高工作效率，可以使用命令补全的方式加快工作效率。 1、生成kubectl bash命令行补全shell [root@node-1 ~]# kubectl completion bash \u003e/etc/kubernetes/kubectl.sh [root@node-1 ~]# echo \"source /etc/kubernetes/kubectl.sh\" \u003e\u003e/root/.bashrc [root@node-1 ~]# cat /root/.bashrc # .bashrc # User specific aliases and functions alias rm='rm -i' alias cp='cp -i' alias mv='mv -i' # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi source /etc/kubernetes/kubectl.sh #添加环境变量配置 2、加载shell环境变量，使配置生效 [root@node-1 ~]# source /etc/kubernetes/kubectl.sh 3、校验命令行补全，命令行中输入kubectl get co再按TAB键就能自动补全了 [root@node-1~]# kubectl get co componentstatuses configmaps controllerrevisions.apps [root@node-1~]# kubectl get componentstatuses 除了支持命令行补全之外，kubectl还支持命令简写，如下是一些常见的命令行检测操作,更多通过kubectl api-resources命令获取，SHORTNAMES显示的是子命令中的简短用法。 kubectl get componentstatuses，简写kubectl get cs获取组件状态 kubectl get nodes，简写kubectl get no获取node节点列表 kubectl get services，简写kubectl get svc获取服务列表 kubectl get deployments,简写kubectl get deploy获取deployment列表 kubectl get statefulsets,简写kubectl get sts获取有状态服务列表 参考文档 Container Runtime安装文档：https://kubernetes.io/docs/setup/production-environment/container-runtimes/ kubeadm安装：https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ 初始化kubeadm集群：https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network 『 转载 』该文章来源于网络，侵删。 ","date":"2019-08-04","objectID":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/:9:0","tags":["kubernetes"],"title":"02 Kubeadm离线部署1.14","uri":"/02-kubeadm%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B21.14.1%E9%9B%86%E7%BE%A4/"},{"categories":["转载","kubernetes","基础教程"],"content":"写在前面 kubernetes中涉及很多概念，包含云生态社区中各类技术，学习成本比较高，k8s中通常以编写yaml文件完成资源的部署，对于较多入门的人来说是个较高的门坎，本文以命令行的形式带领大家快速入门，俯瞰kubernetes核心概念，快速入门。 1. 基础概念 ","date":"2019-08-04","objectID":"/03-kubernetes%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:0:0","tags":["kubernetes"],"title":"03 Kubernetes快速入门","uri":"/03-kubernetes%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.1 集群与节点 kubernetes是一个开源的容器引擎管理平台，实现容器化应用的自动化部署，任务调度，弹性伸缩，负载均衡等功能，cluster是由master和node两种角色组成 master负责管理集群，master包含kube-apiserver，kube-controller-manager，kube-scheduler，etcd组件 node节点运行容器应用，由Container Runtime，kubelet和kube-proxy组成，其中Container Runtime可能是Docker，rke，containerd，node节点可由物理机或者虚拟机组成。 kubernetes集群概念 1、查看master组件角色 [root@node-1 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\":\"true\"} 2、 查看node节点列表 [root@node-1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION node-1 Ready master 26h v1.14.1 node-2 Ready \u003cnone\u003e 26h v1.14.1 node-3 Ready \u003cnone\u003e 26h v1.14.1 3、查看node节点详情 [root@node-1 ~]# kubectl describe node node-3 Name: node-3 Roles: \u003cnone\u003e Labels: beta.kubernetes.io/arch=amd64。#标签和Annotations beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=node-3 kubernetes.io/os=linux Annotations: flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"22:f8:75:bb:da:4e\"} flannel.alpha.coreos.com/backend-type: vxlan flannel.alpha.coreos.com/kube-subnet-manager: true flannel.alpha.coreos.com/public-ip: 10.254.100.103 kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Sat, 10 Aug 2019 17:50:00 +0800 Taints: \u003cnone\u003e Unschedulable: false。#是否禁用调度，cordon命令控制的标识位。 Conditions: #资源调度能力，MemoryPressure内存是否有压力（即内存不足） #DiskPressure磁盘压力 #PIDPressure磁盘压力 #Ready，是否就绪，表明节点是否处于正常工作状态，表示资源充足+相关进程状态正常 Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Sun, 11 Aug 2019 20:32:07 +0800 Sat, 10 Aug 2019 17:50:00 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Sun, 11 Aug 2019 20:32:07 +0800 Sat, 10 Aug 2019 17:50:00 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Sun, 11 Aug 2019 20:32:07 +0800 Sat, 10 Aug 2019 17:50:00 +0800 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Sun, 11 Aug 2019 20:32:07 +0800 Sat, 10 Aug 2019 18:04:20 +0800 KubeletReady kubelet is posting ready status Addresses: #地址和主机名 InternalIP: 10.254.100.103 Hostname: node-3 Capacity: #容器的资源容量 cpu: 2 ephemeral-storage: 51473868Ki hugepages-2Mi: 0 memory: 3880524Ki pods: 110 Allocatable: #已分配资源情况 cpu: 2 ephemeral-storage: 47438316671 hugepages-2Mi: 0 memory: 3778124Ki pods: 110 System Info: #系统信息，如内核版本，操作系统版本，cpu架构，node节点软件版本 Machine ID: 0ea734564f9a4e2881b866b82d679dfc System UUID: D98ECAB1-2D9E-41CC-9A5E-51A44DC5BB97 Boot ID: 6ec81f5b-cb05-4322-b47a-a8e046d9bf79 Kernel Version: 3.10.0-957.el7.x86_64 OS Image: CentOS Linux 7 (Core) Operating System: linux Architecture: amd64 Container Runtime Version: docker://18.3.1 . #Container Runtime为docker，版本为18.3.1 Kubelet Version: v1.14.1 #kubelet版本 Kube-Proxy Version: v1.14.1 #kube-proxy版本 PodCIDR: 10.244.2.0/24 #pod使用的网络 Non-terminated Pods: (4 in total)。 #下面是每个pod资源占用情况 Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- kube-system coredns-fb8b8dccf-hrqm8 100m (5%) 0 (0%) 70Mi (1%) 170Mi (4%) 26h kube-system coredns-fb8b8dccf-qwwks 100m (5%) 0 (0%) 70Mi (1%) 170Mi (4%) 26h kube-system kube-flannel-ds-amd64-zzm2g 100m (5%) 100m (5%) 50Mi (1%) 50Mi (1%) 26h kube-system kube-proxy-x8zqh 0 (0%) 0 (0%) 0 (0%) 0 (0%) 26h Allocated resources: #已分配资源情况 (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 300m (15%) 100m (5%) memory 190Mi (5%) 390Mi (10%) ephemeral-storage 0 (0%) 0 (0%) Events: \u003cnone\u003e ","date":"2019-08-04","objectID":"/03-kubernetes%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:1:0","tags":["kubernetes"],"title":"03 Kubernetes快速入门","uri":"/03-kubernetes%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2 容器与应用 kubernetes是容器编排引擎，其负责容器的调度，管理和容器的运行，但kubernetes调度最小单位并非是container，而是pod，pod中可包含多个container，通常集群中不会直接运行pod，而是通过各种工作负载的控制器如Deployments，ReplicaSets，DaemonSets的方式运行，为啥？因为控制器能够保证pod状态的一致性，正如官方所描述的一样“make sure the current state match to the desire state”，确保当前状态和预期的一致，简单来说就是pod异常了，控制器会在其他节点重建，确保集群当前运行的pod和预期设定的一致。 pod是kubernetes中运行的最小单元 pod中包含一个容器或者多个容器 pod不会单独使用，需要有工作负载来控制，如Deployments，StatefulSets，DaemonSets，CronJobs等 container与pod Container，容器是一种轻量化的虚拟化技术，通过将应用封装在镜像中，实现便捷部署，应用分发。 Pod，kubernetes中最小的调度单位，封装容器，包含一个pause容器和应用容器，容器之间共享相同的命名空间，网络，存储，共享进程。 Deployments，部署组也称应用，严格上来说是无状态化工作负载，另外一种由状态化工组负载是StatefulSets，Deployments是一种控制器，可以控制工作负载的副本数replicas，通过kube-controller-manager中的Deployments Controller实现副本数状态的控制。 ","date":"2019-08-04","objectID":"/03-kubernetes%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:2:0","tags":["kubernetes"],"title":"03 Kubernetes快速入门","uri":"/03-kubernetes%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.3 服务访问 kubernetes中pod是实际运行的载体，pod依附于node中，node可能会出现故障，kubernetes的控制器如replicasets会在其他node上重新拉起一个pod，新的pod会分配一个新的IP；再者，应用部署时会包含多个副本replicas，如同个应用deployments部署了3个pod副本，pod相当于后端的Real Server，如何实现这三个应用访问呢？对于这种情况，我们一般会在Real Server前面加一个负载均衡Load Balancer，service就是pod的负载均衡调度器，service将动态的pod抽象为一个服务，应用程序直接访问service即可，service会自动将请求转发到后端的pod。负责service转发规则有两种机制：iptables和ipvs，iptables通过设置DNAT等规则实现负载均衡，ipvs通过ipvsadm设置转发规。 service概念 根据服务不同的访问方式，service分为如下几种类型：ClusterIP，NodePort，LoadBalancer和_ExternalName，可通过type设置。 ClusterIP，集群内部互访，与DNS结合实现集群内部的服务发现； NodePort，通过NAT将每个node节点暴露一个端口实现外部访问； LoadBalancer，实现云厂商外部接入方式的接口，需要依赖云服务提供商实现具体技术细节，如腾讯云实现与CLB集成； ExternalName，通过服务名字暴露服务名，当前可由ingress实现，将外部的请求以域名转发的形式转发到集群，需要依附具体的外部实现，如nginx，traefik,各大云计算厂商实现接入细节。 pod是动态变化的，ip地址可能会变化（如node故障），副本数可能会变化，如应用扩展scale up，应用锁容scale down等，service如何识别到pod的动态变化呢？答案是labels，通过labels自动会过滤出某个应用的Endpoints，当pod变化时会自动更新Endpoints，不同的应用会有由不同的label组成。labels相关可以参考下https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ service与labels 2. 创建应用 我们开始部署一个应用即deployments，kubernetes中包含各种workload如无状态话的Deployments，有状态化的StatefulSets，守护进程的DaemonSets，每种workload对应不同的应用场景，我们先以Deployments为例入门，其他workload均以此类似，一般而言，在kubernetes中部署应用均以yaml文件方式部署，对于初学者而言，编写yaml文件太冗长，不适合初学，我们先kubectl命令行方式实现API的接入。 1、部署nginx应用，部署三个副本 [root@node-1 ~]# kubectl run nginx-app-demo --image=nginx:1.7.9 --port=80 --replicas=3 kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead. deployment.apps/nginx-app-demo created 2、查看应用列表,可以看到当前pod的状态均已正常，Ready是当前状态，AVAILABLE是目标状态 [root@node-1 ~]# kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nginx-app-demo 3/3 3 3 72s 3、查看应用的详细信息,如下我们可以知道Deployments是通过ReplicaSets控制副本数的，由Replicaset控制pod数 [root@node-1 ~]# kubectl describe deployments nginx-app-demo Name: nginx-app-demo #应用名称 Namespace: default #命名空间 CreationTimestamp: Sun, 11 Aug 2019 21:52:32 +0800 Labels: run=nginx-app-demo #labels，很重要，后续service通过labels实现访问 Annotations: deployment.kubernetes.io/revision: 1 #滚动升级版本号 Selector: run=nginx-app-demo #labels的选择器selector Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable #副本控制器 StrategyType: RollingUpdate #升级策略为RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge #RollingUpdate升级策略，即最大不超过25%的pod Pod Template: #容器应用模版，包含镜像，port，存储等 Labels: run=nginx-app-demo Containers: nginx-app-demo: Image: nginx:1.7.9 Port: 80/TCP Host Port: 0/TCP Environment: \u003cnone\u003e Mounts: \u003cnone\u003e Volumes: \u003cnone\u003e Conditions: #当前状态 Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u003cnone\u003e NewReplicaSet: nginx-app-demo-7bdfd97dcd (3/3 replicas created) #ReplicaSets控制器名称 Events: #运行事件 Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 3m24s deployment-controller Scaled up replica set nginx-app-demo-7bdfd97dcd to 3 4、查看replicasets情况，通过查看可知replicasets副本控制器生成了三个pod 1. 查看replicasets列表 [root@node-1 ~]# kubectl get replicasets NAME DESIRED CURRENT READY AGE nginx-app-demo-7bdfd97dcd 3 3 3 9m9s 2. 查看replicasets详情 [root@node-1 ~]# kubectl describe replicasets nginx-app-demo-7bdfd97dcd Name: nginx-app-demo-7bdfd97dcd Namespace: default Selector: pod-template-hash=7bdfd97dcd,run=nginx-app-demo Labels: pod-template-hash=7bdfd97dcd #labels，增加了一个hash的label识别replicasets run=nginx-app-demo Annotations: deployment.kubernetes.io/desired-replicas: 3 #滚动升级的信息，副本树，最大数，应用版本 deployment.kubernetes.io/max-replicas: 4 deployment.kubernetes.io/revision: 1 Controlled By: Deployment/nginx-app-demo #副本的父控制，为nginx-app-demo这个Deployments Replicas: 3 current / 3 desired Pods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: #容器模板，继承于deployments Labels: pod-template-hash=7bdfd97dcd run=nginx-app-demo Containers: nginx-app-demo: Image: nginx:1.7.9 Port: 80/TCP Host Port: 0/TCP Environment: \u003cnone\u003e Mounts: \u003cnone\u003e Volumes: \u003cnone\u003e Events:","date":"2019-08-04","objectID":"/03-kubernetes%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:3:0","tags":["kubernetes"],"title":"03 Kubernetes快速入门","uri":"/03-kubernetes%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"3.1 ClusterIP访问 通过pod的ip直接访问应用，对于单个pod的应用可以实现，对于多个副本replicas的应用则不符合要求，需要通过service来实现负载均衡，service需要设置不同的type，默认为ClusterIP即集群内部访问，如下通过expose子命令将服务暴露到service。 1、暴露service,其中port表示代理监听端口，target-port代表是容器的端口，type设置的是service的类型 [root@node-1 ~]# kubectl expose deployment nginx-app-demo --name nginx-service-demo \\ --port=80 \\ --protocol=TCP \\ --target-port=80 \\ --type ClusterIP service/nginx-service-demo exposed 2、查看service的详情，可以看到service通过labels选择器selector自动将pod的ip生成endpoints 查看service列表，显示有两个，kubernetes为默认集群创建的service [root@node-1 ~]# kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 29h nginx-service-demo ClusterIP 10.102.1.1 \u003cnone\u003e 80/TCP 2m54s 查看service详情，可以看到Labels的Seletor和前面Deployments设置一致，Endpoints将pod组成一个列表 [root@node-1 ~]# kubectl describe services nginx-service-demo Name: nginx-service-demo #名称 Namespace: default #命名空间 Labels: run=nginx-app-demo #标签名称 Annotations: \u003cnone\u003e Selector: run=nginx-app-demo #标签选择器 Type: ClusterIP #service类型为ClusterIP IP: 10.102.1.1 #服务的ip，即vip，集群内部会自动分配一个 Port: \u003cunset\u003e 80/TCP #服务端口，即ClusterIP对外访问的端口 TargetPort: 80/TCP #容器端口 Endpoints: 10.244.1.2:80,10.244.1.3:80,10.244.2.4:80 #访问地址列表 Session Affinity: None #负载均衡调度算法 Events: \u003cnone\u003e 3、访问service的地址，可以访问的内容可知，service自动实现了pods的负载均衡，调度策略为轮询，为何？因为service默认的调度策略Session Affinity为None，即是轮训，可以设置为ClientIP，实现会话保持，相同客户端IP的请求会调度到相同的pod上。 [root@node-1 ~]# curl http://10.102.1.1 web3 [root@node-1 ~]# curl http://10.102.1.1 web1 [root@node-1 ~]# curl http://10.102.1.1 web2 [root@node-1 ~]# curl http://10.102.1.1 4、ClusterIP原理深入剖析，service后端实现有两种机制：iptables和ipvs，环境安装采用iptables，iptables通过nat的链生成访问规则，KUBE-SVC-R5Y5DZHD7Q6DDTFZ为入站DNAT转发规则，KUBE-MARK-MASQ为出站转发 [root@node-1 ~]# iptables -t nat -L -n Chain KUBE-SERVICES (2 references) target prot opt source destination KUBE-MARK-MASQ tcp -- !10.244.0.0/16 10.102.1.1 /* default/nginx-service-demo: cluster IP */ tcp dpt:80 KUBE-SVC-R5Y5DZHD7Q6DDTFZ tcp -- 0.0.0.0/0 10.102.1.1 /* default/nginx-service-demo: cluster IP */ tcp dpt:80 出站：KUBE-MARK-MASQ源地址段不是10.244.0.0/16访问10.102.1.1的目标端口80时，将请求转发给KUBE-MARK-MASQ链 入站：KUBE-SVC-R5Y5DZHD7Q6DDTFZ任意原地址访问目标10.102.1.1的目标端口80时将请求转发给KUBE-SVC-R5Y5DZHD7Q6DDTFZ链 5、查看入站请求规则，入站请求规则将会映射到不同的链，不同链将会转发到不同pod的ip上。 1. 查看入站规则KUBE-SVC-R5Y5DZHD7Q6DDTFZ，请求将转发至三条链 [root@node-1 ~]# iptables -t nat -L KUBE-SVC-R5Y5DZHD7Q6DDTFZ -n Chain KUBE-SVC-R5Y5DZHD7Q6DDTFZ (1 references) target prot opt source destination KUBE-SEP-DSWLUQNR4UPH24AX all -- 0.0.0.0/0 0.0.0.0/0 statistic mode random probability 0.33332999982 KUBE-SEP-56SLMGHHOILJT36K all -- 0.0.0.0/0 0.0.0.0/0 statistic mode random probability 0.50000000000 KUBE-SEP-K6G4Z74HQYF6X7SI all -- 0.0.0.0/0 0.0.0.0/0 2. 查看实际转发的三条链的规则,实际映射到不同的pod的ip地址上 [root@node-1 ~]# iptables -t nat -L KUBE-SEP-DSWLUQNR4UPH24AX -n Chain KUBE-SEP-DSWLUQNR4UPH24AX (1 references) target prot opt source destination KUBE-MARK-MASQ all -- 10.244.1.2 0.0.0.0/0 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp to:10.244.1.2:80 [root@node-1 ~]# iptables -t nat -L KUBE-SEP-56SLMGHHOILJT36K -n Chain KUBE-SEP-56SLMGHHOILJT36K (1 references) target prot opt source destination KUBE-MARK-MASQ all -- 10.244.1.3 0.0.0.0/0 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp to:10.244.1.3:80 [root@node-1 ~]# iptables -t nat -L KUBE-SEP-K6G4Z74HQYF6X7SI -n Chain KUBE-SEP-K6G4Z74HQYF6X7SI (1 references) target prot opt source destination KUBE-MARK-MASQ all -- 10.244.2.4 0.0.0.0/0 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp to:10.244.2.4:80 ","date":"2019-08-04","objectID":"/03-kubernetes%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:4:0","tags":["kubernetes"],"title":"03 Kubernetes快速入门","uri":"/03-kubernetes%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"3.2 NodePort访问 Service通过ClusterIP只能提供集群内部的应用访问，外部无法直接访问应用，如果需要外部访问有如下几种方式：NodePort，LoadBalancer和Ingress，其中LoadBalancer需要由云服务提供商实现，Ingress需要安装单独的Ingress Controller，日常测试可以通过NodePort的方式实现，NodePort可以将node的某个端口暴露给外部网络访问。 1、修改type的类型由ClusterIP修改为NodePort类型（或者重新创建，指定type的类型为NodePort） 1. 通过patch修改type的类型 [root@node-1 ~]# kubectl patch services nginx-service-demo -p '{\"spec\":{\"type\": \"NodePort\"}}' service/nginx-service-demo patched 2. 确认yaml文件配置，分配了一个NodePort端口，即每个node上都会监听该端口 [root@node-1 ~]# kubectl get services nginx-service-demo -o yaml apiVersion: v1 kind: Service metadata: creationTimestamp: \"2019-08-11T14:35:59Z\" labels: run: nginx-app-demo name: nginx-service-demo namespace: default resourceVersion: \"157676\" selfLink: /api/v1/namespaces/default/services/nginx-service-demo uid: 55e29b78-bc45-11e9-b073-525400490421 spec: clusterIP: 10.102.1.1 externalTrafficPolicy: Cluster ports: - nodePort: 32416 #自动分配了一个NodePort端口 port: 80 protocol: TCP targetPort: 80 selector: run: nginx-app-demo sessionAffinity: None type: NodePort #类型修改为NodePort status: loadBalancer: {} 3. 查看service列表，可以知道service的type已经修改为NodePort,同时还保留ClusterIP的访问IP [root@node-1 ~]# kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 30h nginx-service-demo NodePort 10.102.1.1 \u003cnone\u003e 80:32416/TCP 68m 2、通过NodePort访问应用程序,每个node的地址相当于vip，可以实现相同的负载均衡效果，同时CluserIP功能依可用 1. NodePort的负载均衡 [root@node-1 ~]# curl http://node-1:32416 web1 [root@node-1 ~]# curl http://node-2:32416 web1 [root@node-1 ~]# curl http://node-3:32416 web1 [root@node-1 ~]# curl http://node-3:32416 web3 [root@node-1 ~]# curl http://node-3:32416 web2 2. ClusterIP的负载均衡 [root@node-1 ~]# curl http://10.102.1.1 web2 [root@node-1 ~]# curl http://10.102.1.1 web1 [root@node-1 ~]# curl http://10.102.1.1 web1 [root@node-1 ~]# curl http://10.102.1.1 web3 3、NodePort转发原理，每个node上通过kube-proxy监听NodePort的端口，由后端的iptables实现端口的转发 1. NodePort监听端口 [root@node-1 ~]# netstat -antupl |grep 32416 tcp6 0 0 :::32416 :::* LISTEN 32052/kube-proxy 2. 查看nat表的转发规则，有两条规则KUBE-MARK-MASQ出口和KUBE-SVC-R5Y5DZHD7Q6DDTFZ入站方向。 Chain KUBE-NODEPORTS (1 references) target prot opt source destination KUBE-MARK-MASQ tcp -- 0.0.0.0/0 0.0.0.0/0 /* default/nginx-service-demo: */ tcp dpt:32416 KUBE-SVC-R5Y5DZHD7Q6DDTFZ tcp -- 0.0.0.0/0 0.0.0.0/0 /* default/nginx-service-demo: */ tcp dpt:32416 3. 查看入站的请求规则链KUBE-SVC-R5Y5DZHD7Q6DDTFZ [root@node-1 ~]# iptables -t nat -L KUBE-SVC-R5Y5DZHD7Q6DDTFZ -n Chain KUBE-SVC-R5Y5DZHD7Q6DDTFZ (2 references) target prot opt source destination KUBE-SEP-DSWLUQNR4UPH24AX all -- 0.0.0.0/0 0.0.0.0/0 statistic mode random probability 0.33332999982 KUBE-SEP-56SLMGHHOILJT36K all -- 0.0.0.0/0 0.0.0.0/0 statistic mode random probability 0.50000000000 KUBE-SEP-K6G4Z74HQYF6X7SI all -- 0.0.0.0/0 0.0.0.0/0 4. 继续查看转发链，包含有DNAT转发和KUBE-MARK-MASQ和出站返回的规则 [root@node-1 ~]# iptables -t nat -L KUBE-SEP-DSWLUQNR4UPH24AX -n Chain KUBE-SEP-DSWLUQNR4UPH24AX (1 references) target prot opt source destination KUBE-MARK-MASQ all -- 10.244.1.2 0.0.0.0/0 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp to:10.244.1.2:80 [root@node-1 ~]# iptables -t nat -L KUBE-SEP-56SLMGHHOILJT36K -n Chain KUBE-SEP-56SLMGHHOILJT36K (1 references) target prot opt source destination KUBE-MARK-MASQ all -- 10.244.1.3 0.0.0.0/0 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp to:10.244.1.3:80 [root@node-1 ~]# iptables -t nat -L KUBE-SEP-K6G4Z74HQYF6X7SI -n Chain KUBE-SEP-K6G4Z74HQYF6X7SI (1 references) target prot opt source destination KUBE-MARK-MASQ all -- 10.244.2.4 0.0.0.0/0 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp to:10.244.2.4:80 4. 扩展应用 当应用程序的负载比较高无法满足应用请求时，一般我们会通过扩展RS的数量来实现，在kubernetes中，扩展RS实际上通过扩展副本数replicas来实现，扩展RS非常便利，快速实现弹性伸缩。kubernets能提供两种方式的伸缩能力：1. 手动伸缩能力scale up和scale down，2. 动态的弹性伸缩horizontalpodautoscalers,基于CPU的利用率实现自动的弹性伸缩，需要依赖与监控组件如metrics server，当前未实现，后续再做深入探讨，本文以手动的scale的方式扩展应用的副本数。 Deployments副本扩展 1、手动扩展副本数 [root@node-1 ~]# kubectl scale --replicas=4 deployment nginx-app-demo deployment.extensions/nginx-app-demo scaled 2、查看副本扩展情况,deployme","date":"2019-08-04","objectID":"/03-kubernetes%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:5:0","tags":["kubernetes"],"title":"03 Kubernetes快速入门","uri":"/03-kubernetes%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["转载","kubernetes","基础教程"],"content":"1. kubernetes集群升级 ","date":"2019-08-04","objectID":"/04-%E7%A6%BB%E7%BA%BF%E5%8D%87%E7%BA%A7kubernetes%E9%9B%86%E7%BE%A4/:0:0","tags":["kubernetes"],"title":"04 离线升级kubernetes集群","uri":"/04-%E7%A6%BB%E7%BA%BF%E5%8D%87%E7%BA%A7kubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.1 kubernetes升级概述 kubernetes版本升级迭代非常快，每三个月更新一个版本,很多新的功能在新版本中快速迭代，为了与社区版本功能保持一致，升级kubernetes集群，社区已通过kubeadm工具统一升级集群，升级步骤简单易行。首先来看下升级kubernetes集群需要升级那些组件： 升级管理节点，管理节点上的kube-apiserver，kuber-controller-manager，kube-scheduler，etcd等； 其他管理节点，管理节点如果以高可用的方式部署，多个高可用节点需要一并升级； worker工作节点，升级工作节点上的Container Runtime如docker，kubelet和kube-proxy。 版本升级通常分为两类：小版本升级和跨版本升级，小版本升级如1.14.1升级只1.14.2，小版本之间可以跨版本升级如1.14.1直接升级至1.14.3；跨版本升级指大版本升级，如1.14.x升级至1.15.x。本文以离线的方式将1.14.1升级至1.1.5.1版本，升级前需要满足条件如下： 当前集群版本需要大于1.14.x，可升级至1.14.x和1.15.x版本，小版本和跨版本之间升级； 关闭swap空间； 备份数据，将etcd数据备份，以及一些重要目录如/etc/kubernetes,/var/lib/kubelet； 升级过程中pod需要重启，确保应用使用RollingUpdate滚动升级策略，避免业务有影响。 ","date":"2019-08-04","objectID":"/04-%E7%A6%BB%E7%BA%BF%E5%8D%87%E7%BA%A7kubernetes%E9%9B%86%E7%BE%A4/:1:0","tags":["kubernetes"],"title":"04 离线升级kubernetes集群","uri":"/04-%E7%A6%BB%E7%BA%BF%E5%8D%87%E7%BA%A7kubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2 升级前准备工作 1、查看当前版本,系统上部署的版本是1.1.4.1 [root@node-1 ~]# kubectl version Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:11:31Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"} Server Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:02:58Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"} [root@node-1 ~]# kubeadm version kubeadm version: \u0026version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:08:49Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"} 2、查看node节点的版本,node上的kubelet和kube-proxy使用1.1.4.1版本 [root@node-1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION node-1 Ready master 25h v1.14.1 node-2 Ready \u003cnone\u003e 25h v1.14.1 node-3 Ready \u003cnone\u003e 25h v1.14.1 3、其他组件状态，确保当前组件，应用状态正常 [root@node-1 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\":\"true\"} [root@node-1 ~]# [root@node-1 ~]# kubectl get deployments --all-namespaces NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE default demo 3/3 3 3 37m kube-system coredns 2/2 2 2 25h 4、查看kubernetes最新版本（配置kubernetes的yum源，需要合理上网才可以访问），使用yum list –showduplicates kubeadm –disableexcludes=kubernetes查看当前能升级版本，绿色为当前版本，蓝色为可以升级的版本，如下图： kubernetes可升级版本列表 ","date":"2019-08-04","objectID":"/04-%E7%A6%BB%E7%BA%BF%E5%8D%87%E7%BA%A7kubernetes%E9%9B%86%E7%BE%A4/:2:0","tags":["kubernetes"],"title":"04 离线升级kubernetes集群","uri":"/04-%E7%A6%BB%E7%BA%BF%E5%8D%87%E7%BA%A7kubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.3 升级master节点 1、倒入安装镜像，先从cos中下载安装镜像并通过docker load导入到系统中，下载地址，解压并进入到v1.15.1目录下，将镜像倒入到三个节点中,以node-2为例倒入镜像： 1.15.1下载地址 1.17.2下载地址 倒入镜像： [root@node-2 v1.15.1]# docker image load -i kube-apiserver\\:v1.15.1.tar [root@node-2 v1.15.1]# docker image load -i kube-scheduler\\:v1.15.1.tar [root@node-2 v1.15.1]# docker image load -i kube-controller-manager\\:v1.15.1.tar [root@node-2 v1.15.1]# docker image load -i kube-proxy\\:v1.15.1.tar 查看当前系统导入镜像列表： [root@node-1 ~]# docker image list REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.15.1 89a062da739d 8 weeks ago 82.4MB k8s.gcr.io/kube-controller-manager v1.15.1 d75082f1d121 8 weeks ago 159MB k8s.gcr.io/kube-scheduler v1.15.1 b0b3c4c404da 8 weeks ago 81.1MB k8s.gcr.io/kube-apiserver v1.15.1 68c3eb07bfc3 8 weeks ago 207MB k8s.gcr.io/kube-proxy v1.14.1 20a2d7035165 5 months ago 82.1MB k8s.gcr.io/kube-apiserver v1.14.1 cfaa4ad74c37 5 months ago 210MB k8s.gcr.io/kube-scheduler v1.14.1 8931473d5bdb 5 months ago 81.6MB k8s.gcr.io/kube-controller-manager v1.14.1 efb3887b411d 5 months ago 158MB quay.io/coreos/flannel v0.11.0-amd64 ff281650a721 7 months ago 52.6MB k8s.gcr.io/coredns 1.3.1 eb516548c180 8 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 9 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 21 months ago 742kB 2、更新kubeadm版本至1.15.1，国内可以参考https://blog.51cto.com/2157217/1983992设置kubernetes源 更新kubeadm版本至1.15.1 3、校验kubeadm版本,已升级至1.1.5.1版本 [root@node-1 ~]# kubeadm version kubeadm version: \u0026version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.1\", GitCommit:\"4485c6f18cee9a5d3c3b4e523bd27972b1b53892\", GitTreeState:\"clean\", BuildDate:\"2019-07-18T09:15:32Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"} 4、查看升级计划，通过kubeadm可以查看当前集群的升级计划，会显示当前小版本最新的版本以及社区最新的版本 [root@node-1 ~]# kubeadm upgrade plan [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [preflight] Running pre-flight checks. [upgrade] Making sure the cluster is healthy: [upgrade] Fetching available versions to upgrade to [upgrade/versions] Cluster version: v1.14.1 #当前集群版本 [upgrade/versions] kubeadm version: v1.15.1 #当前kubeadm版本 [upgrade/versions] Latest stable version: v1.15.3 #社区最新版本 [upgrade/versions] Latest version in the v1.14 series: v1.14.6 #1.14.x中最新的版本 Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT AVAILABLE Kubelet 3 x v1.14.1 v1.14.6 Upgrade to the latest version in the v1.14 series: COMPONENT CURRENT AVAILABLE #可升级的版本信息，当前可从1.14.1升级至1.14.6版本 API Server v1.14.1 v1.14.6 Controller Manager v1.14.1 v1.14.6 Scheduler v1.14.1 v1.14.6 Kube Proxy v1.14.1 v1.14.6 CoreDNS 1.3.1 1.3.1 Etcd 3.3.10 3.3.10 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.14.6 #升级至1.14.6执行的操作命令 _____________________________________________________________________ Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT AVAILABLE Kubelet 3 x v1.14.1 v1.15.3 Upgrade to the latest stable version: COMPONENT CURRENT AVAILABLE #跨版本升级的版本，当前最新的版本是1.15.3 API Server v1.14.1 v1.15.3 Controller Manager v1.14.1 v1.15.3 Scheduler v1.14.1 v1.15.3 Kube Proxy v1.14.1 v1.15.3 CoreDNS 1.3.1 1.3.1 Etcd 3.3.10 3.3.10 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.15.3 #升级至社区最新的版本执行的操作 Note: Before you can perform this upgrade, you have to update kubeadm to v1.15.3. _____________________________________________________________________ 5、当前镜像没有下载最新镜像，本文以升级1.1.5.1版本为例，升级其他版本相类似，需要确保当前集群已获取到相关镜像，升级过程中也会更新证书，可通过--certificate-renewal=false关闭证书升级，升级至1.15.1版本操作如下： [root@node-1 ~]# kubeadm upgrade apply v1.15.1 [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/co","date":"2019-08-04","objectID":"/04-%E7%A6%BB%E7%BA%BF%E5%8D%87%E7%BA%A7kubernetes%E9%9B%86%E7%BE%A4/:3:0","tags":["kubernetes"],"title":"04 离线升级kubernetes集群","uri":"/04-%E7%A6%BB%E7%BA%BF%E5%8D%87%E7%BA%A7kubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.4 升级worker节点 1、升级kubeadm和kubelet软件包 [root@node-2 ~]# yum install -y kubelet-1.15.1-0 --disableexcludes=kubernetes [root@node-2 ~]# yum install -y kubeadm-1.15.1-0 --disableexcludes=kubernetes [root@node-2 ~]# yum install -y kubectl-1.15.1-0 --disableexcludes=kubernetes 2、设置节点进入维护模式并驱逐worker节点上的应用，会将出了DaemonSets之外的其他应用迁移到其他节点上 设置维护和驱逐： [root@node-1 ~]# kubectl drain node-2 --ignore-daemonsets node/node-2 cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-amd64-tm6wj, kube-system/kube-proxy-2wqhj evicting pod \"coredns-5c98db65d4-86gg7\" evicting pod \"demo-7b86696648-djvgb\" pod/demo-7b86696648-djvgb evicted pod/coredns-5c98db65d4-86gg7 evicted node/node-2 evicted 查看node的情况，此时node-2多了SchedulingDisabled标识位，即新的node都不会调度到该节点上 [root@node-1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION node-1 Ready master 26h v1.15.1 node-2 Ready,SchedulingDisabled \u003cnone\u003e 26h v1.14.1 node-3 Ready \u003cnone\u003e 26h v1.14.1 查看node-2上的pods，pod都已经迁移到其他node节点上 [root@node-1 ~]# kubectl get pods --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default demo-7b86696648-6f22r 1/1 Running 0 30s 10.244.2.5 node-3 \u003cnone\u003e \u003cnone\u003e default demo-7b86696648-fjmxn 1/1 Running 0 116m 10.244.2.2 node-3 \u003cnone\u003e \u003cnone\u003e default demo-7b86696648-nwwxf 1/1 Running 0 116m 10.244.2.3 node-3 \u003cnone\u003e \u003cnone\u003e kube-system coredns-5c98db65d4-cqbbl 1/1 Running 0 30s 10.244.0.6 node-1 \u003cnone\u003e \u003cnone\u003e kube-system coredns-5c98db65d4-g59qt 1/1 Running 2 28m 10.244.2.4 node-3 \u003cnone\u003e \u003cnone\u003e kube-system etcd-node-1 1/1 Running 0 13m 10.254.100.101 node-1 \u003cnone\u003e \u003cnone\u003e kube-system kube-apiserver-node-1 1/1 Running 0 13m 10.254.100.101 node-1 \u003cnone\u003e \u003cnone\u003e kube-system kube-controller-manager-node-1 1/1 Running 0 13m 10.254.100.101 node-1 \u003cnone\u003e \u003cnone\u003e kube-system kube-flannel-ds-amd64-99tjl 1/1 Running 1 26h 10.254.100.101 node-1 \u003cnone\u003e \u003cnone\u003e kube-system kube-flannel-ds-amd64-jp594 1/1 Running 0 26h 10.254.100.103 node-3 \u003cnone\u003e \u003cnone\u003e kube-system kube-flannel-ds-amd64-tm6wj 1/1 Running 0 26h 10.254.100.102 node-2 \u003cnone\u003e \u003cnone\u003e kube-system kube-proxy-2wqhj 1/1 Running 0 28m 10.254.100.102 node-2 \u003cnone\u003e \u003cnone\u003e kube-system kube-proxy-k7c4f 1/1 Running 1 27m 10.254.100.101 node-1 \u003cnone\u003e \u003cnone\u003e kube-system kube-proxy-zffgq 1/1 Running 0 28m 10.254.100.103 node-3 \u003cnone\u003e \u003cnone\u003e kube-system kube-scheduler-node-1 1/1 Running 0 13m 10.254.100.101 node-1 \u003cnone\u003e \u003cnone\u003e 3、升级worker节点 [root@node-2 ~]# kubeadm upgrade node [upgrade] Reading configuration from the cluster... [upgrade] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [upgrade] Skipping phase. Not a control plane node[kubelet-start] Downloading configuration for the kubelet from the \"kubelet-config-1.15\" ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [upgrade] The configuration for this node was successfully updated! [upgrade] Now you should go ahead and upgrade the kubelet package using your package manager. 4、重启kubelet服务 [root@node-2 ~]# systemctl daemon-reload [root@node-2 ~]# systemctl restart kubelet 5、取消节点调度标志，确保worker节点可正常调度 [root@node-1 ~]# kubectl uncordon node-2 node/node-2 uncordoned [root@node-1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION node-1 Ready master 27h v1.15.1 node-2 Ready \u003cnone\u003e 27h v1.15.1 #已升级成功 node-3 Ready \u003cnone\u003e 27h v1.14.1 按照上述步骤升级node-3节点，如下是升级完成后所有节点版本状态： [root@node-1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION node-1 Ready master 27h v1.15.1 node-2 Ready \u003cnone\u003e 27h v1.15.1 node-3 Ready \u003cnone\u003e 27h v1.15.1 ","date":"2019-08-04","objectID":"/04-%E7%A6%BB%E7%BA%BF%E5%8D%87%E7%BA%A7kubernetes%E9%9B%86%E7%BE%A4/:4:0","tags":["kubernetes"],"title":"04 离线升级kubernetes集群","uri":"/04-%E7%A6%BB%E7%BA%BF%E5%8D%87%E7%BA%A7kubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.5 升级原理 1、kubeadm upgrade apply执行动作 检查集群是否具备更新条件，检查apiserver处于可用状态，所有node处于ready状态，确保cs组件正常 强制版本更新策略 检查更新所需镜像是否下载或者可拉取 更新所有控制节点组件，确保异常时能回滚到原有状态 更新kube-dns和kube-proxy的配置文件，确保所需的的RBAC授权配置正常 生成新的证书文件并备份证书（当证书超时超过180天） 2、kubeadm upgrade node执行动作 从kubeadm中获取ClusterConfiguration，即从集群中获取到更新集群的配置文件并应用 更新node节点上的kubelet配置信息和软件 2. 更新集群至1.15.3 截止至2019.9.15，当前kubernetes社区最新版本是1.15.3，本文演示以在线的方式升级kubernetes集群至1.15.3版本，步骤和前文操作类似。 1、安装最新软件包，kubeadm，kubelet，kubectl,三个节点均需要安装 [root@node-1 ~]# yum install kubeadm kubectl kubelet 已加载插件：fastestmirror, langpacks Loading mirror speeds from cached hostfile 正在解决依赖关系 --\u003e 正在检查事务 ---\u003e 软件包 kubeadm.x86_64.0.1.15.1-0 将被 升级 ---\u003e 软件包 kubeadm.x86_64.0.1.15.3-0 将被 更新 ---\u003e 软件包 kubectl.x86_64.0.1.15.1-0 将被 升级 ---\u003e 软件包 kubectl.x86_64.0.1.15.3-0 将被 更新 ---\u003e 软件包 kubelet.x86_64.0.1.15.1-0 将被 升级 ---\u003e 软件包 kubelet.x86_64.0.1.15.3-0 将被 更新 --\u003e 解决依赖关系完成 依赖关系解决 ======================================================================================================================================================================== Package 架构 版本 源 大小 ======================================================================================================================================================================== 正在更新: kubeadm x86_64 1.15.3-0 kubernetes 8.9 M kubectl x86_64 1.15.3-0 kubernetes 9.5 M kubelet x86_64 1.15.3-0 kubernetes 22 M 事务概要 ======================================================================================================================================================================== 升级 3 软件包 2、升级master节点 查看升级计划 [root@node-1 ~]# kubeadm upgrade plan [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [preflight] Running pre-flight checks. [upgrade] Making sure the cluster is healthy: [upgrade] Fetching available versions to upgrade to [upgrade/versions] Cluster version: v1.15.1 [upgrade/versions] kubeadm version: v1.15.3 [upgrade/versions] Latest stable version: v1.15.3 [upgrade/versions] Latest version in the v1.15 series: v1.15.3 Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT AVAILABLE Kubelet 3 x v1.15.1 v1.15.3 Upgrade to the latest version in the v1.15 series: COMPONENT CURRENT AVAILABLE API Server v1.15.1 v1.15.3 Controller Manager v1.15.1 v1.15.3 Scheduler v1.15.1 v1.15.3 Kube Proxy v1.15.1 v1.15.3 CoreDNS 1.3.1 1.3.1 Etcd 3.3.10 3.3.10 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.15.3 _____________________________________________________________________ 升级master节点 [root@node-1 ~]# kubeadm upgrade apply v1.15.3 3、升级worker节点,以此升级node-2和node-3节点 设置五污点去驱逐 [root@node-1 ~]# kubectl drain node-2 --ignore-daemonsets 执行升级操作 [root@node-2 ~]# kubeadm upgrade node [root@node-2 ~]# systemctl daemon-reload [root@node-2 ~]# systemctl restart kubelet 取消调度标志位 [root@node-1 ~]# kubectl uncordon node-2 node/node-2 uncordoned 确认版本升级 [root@node-1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION node-1 Ready master 27h v1.15.3 node-2 Ready \u003cnone\u003e 27h v1.15.3 node-3 Ready \u003cnone\u003e 27h v1.15.1 4、所有节点升级后的状态 所有node状态 [root@node-1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION node-1 Ready master 27h v1.15.3 node-2 Ready \u003cnone\u003e 27h v1.15.3 node-3 Ready \u003cnone\u003e 27h v1.15.3 查看组件状态 [root@node-1 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\":\"true\"} 查看应用状态 [root@node-1 ~]# kubectl get deployments --all-namespaces NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE default demo 3/3 3 3 160m kube-system coredns 2/2 2 2 27h 查看DaemonSets状态 [root@node-1 ~]# kubectl get daemonsets --all-namespaces NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system kube-flannel-ds-amd64 3 3 3 3 3 beta.kubernetes.io/arch","date":"2019-08-04","objectID":"/04-%E7%A6%BB%E7%BA%BF%E5%8D%87%E7%BA%A7kubernetes%E9%9B%86%E7%BE%A4/:5:0","tags":["kubernetes"],"title":"04 离线升级kubernetes集群","uri":"/04-%E7%A6%BB%E7%BA%BF%E5%8D%87%E7%BA%A7kubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["转载","kubernetes","基础教程"],"content":"写在前面 前面的系列文章已介绍kubernetes架构，安装，升级和快速入门，读者通过文章的实操已对kubernetes已有初步的认识和理解，从本章开始逐步介绍kubernetes中的基础概念概念和核心概念，基础概念包括：namespace，labels，annotations，pods，volumes等；核心概念包含kubernetes中各种controller，包含以下几种： 应用副本控制器有：Deployments，ReplicaSets，DaemonSets，StatefulSets； 批处理任务控制器Jobs和CronJob 存储控制器PersistentVoloume，PersistentVolumeClaim，StorageClass； 服务负载均衡Service，Ingress，NetworkPolicy和DNS名称解析； 配置和密钥ConfigMaps和Secrets 本文从最基础的概念pod开始讲解，后续逐步介绍应用部署，存储，负载均衡等相关的控制器，kubernetes内部由多个不同的控制器组成，每个控制器完成不同的功能。 1. 深入学习pod ","date":"2019-08-04","objectID":"/05-%E5%88%9D%E8%AF%86%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5pod/:0:0","tags":["kubernetes"],"title":"05 初识核心概念pod","uri":"/05-%E5%88%9D%E8%AF%86%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5pod/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.1 Container和Pod概念 容器是一种便携式，轻量级别的容器虚拟化技术，使用linux cggroup技术实现各种资源的隔离，如cpu，memory，pid，mount，IPC等，相比于虚拟化技术如KVM，容器技术更加轻量级，它的产生主要解决环境的环境发布的问题，目前主流的容器技术是docker，说到容器，一般都等同于docker。 要运行容器首先需要有镜像，应用和应用依赖的环境运行在容器中，在kubernetes中不会直接运行container，而是运行pod，一个pod里面包含多个container，container之间共享相同的namespace，network，storage等。镜像存储在私有镜像或者公有镜像中，运行时通过docker image pull的方式拉取到本地运行，images的拉取策略包含有两种： ImagePullPolicy为Always，不管本地是否有直接下载 ImagePullPolicy为IfNotPresent，默认镜像拉取得策略，本地不存在再拉取 Pods是kubernetes中最小的调度单位，Pods内运行一个或者多个container，container之间共享pod的网络ip资源，存储volume资源，计算等资源，方便pod内部的container之间能够实现快速的访问和交互。 Pod概念介绍 如上图所示，Pod的使用方式通常包含两种： Pod中运行一个容器，最经常使用的模式，container封装在pod中调度，两者几乎等同，但k8s不直接管理容器 Pod中运行多个容器，多个容器封装在pod中一起调度，适用于容器之间有数据交互和调用的场景，如app+redis，pod内部共享相同的网络命名空间，存储命名空间，进程命名空间等。 ","date":"2019-08-04","objectID":"/05-%E5%88%9D%E8%AF%86%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5pod/:1:0","tags":["kubernetes"],"title":"05 初识核心概念pod","uri":"/05-%E5%88%9D%E8%AF%86%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5pod/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2 如何创建pod kubernetes交互的方式通常分为四种： 命令行，kubectl和kubernetes交互，完成资源的管理，命令行入门简单，但只能支持部分资源创建 API，通过resfulAPI以http的方式和kubernetes交互，适用于基于API做二次开发 SDK，提供各种语言原生的SDK，实现各种语言编程接入 YAML，通过易于理解的YAML文件格式，描述资源的定义，功能最丰富，最终转换为json格式 kubernetes中通过定义生申明式的方式定义资源，即通过在yaml文件中定义所需的资源，kubernetes通过controller-manager按照yaml文件中定义的资源去生成所需的资源（match the current state to desired state）。通常在kubernetes中通过yaml文件的方式定义资源，然后通过kubectl create -f 文件.yaml的方式应用配置，如下演示创建一个nginx应用的操作。 1、编写yaml文件，定义一个pod资源 [root@node-1 demo]# cat nginx.yaml apiVersion: v1 kind: Pod metadata: name: nginx-demo labels: name: nginx-demo spec: containers: - name: nginx-demo image: nginx:1.7.9 imagePullPolicy: IfNotPresent ports: - name: nginx-port-80 protocol: TCP containerPort: 80 关于配置文件，说明如下： apiVersion api使用的版本,kubectl api-versions可查看到当前系统能支持的版本列表 kind 指定资源类型，表示为Pod的资源类型 metadata 指定Pod的元数据，metadata.name指定名称，metadata.labels指定Pod的所属的标签 spec 指定Pod的模版属性，spec.containers配置容器的信息，spec.containers.name指定名字，spec.containers.image指定容器镜像的名称，spec.containers.imagePullPolicy是镜像的下载方式，IfNotPresent表示当镜像不存在时下载，spec.containers.ports.name指定port的名称，spec.containers.ports.protocol协议类型为TCP，spec.containers.ports.containerPort为容器端口。 2、创建pod应用 [root@node-1 demo]# kubectl apply -f nginx.yaml pod/nginx-demo created 3、访问应用 获取容器的IP地址 [root@node-1 demo]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-7b86696648-8bq7h 1/1 Running 0 8h 10.244.1.11 node-2 \u003cnone\u003e \u003cnone\u003e demo-7b86696648-8qp46 1/1 Running 0 8h 10.244.1.10 node-2 \u003cnone\u003e \u003cnone\u003e demo-7b86696648-d6hfw 1/1 Running 0 8h 10.244.1.12 node-2 \u003cnone\u003e \u003cnone\u003e nginx-demo 1/1 Running 0 50s 10.244.2.11 node-3 \u003cnone\u003e \u003cnone\u003e 访问站点内容： [root@node-1 demo]# curl http://10.244.2.11 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e 前面我们我们学习过kubernetes支持滚动升级RollingUpdate，弹性扩容replicas等特性，如何给Pod做滚动升级保障业务不中断，如何提高Pod的副本个数保障高可用呢？答案是：不支持。Pod是单个，无法支持一些高级特性，高级特性需要通过高级的副本控制器如ReplicaSets，Deployments，StatefulSets，DaemonSets等才能支持。Pod在实际应用中很少用，除了测试和运行一些简单的功能外，实际使用建议使用Deployments代替，Pod的定义以Template的方式嵌入在副本控制器中。 ","date":"2019-08-04","objectID":"/05-%E5%88%9D%E8%AF%86%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5pod/:2:0","tags":["kubernetes"],"title":"05 初识核心概念pod","uri":"/05-%E5%88%9D%E8%AF%86%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5pod/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.3. 如何编写yaml文件 前面我们提到过kubernetse是申明式的方式部署应用，应用的部署都定义在yaml文件中来实现，如何来编写应用的yaml文件呢，下面我来分享两个实际使用的技巧： 1、通过定义模版快速生成，kubectl create apps -o yaml –dry-run的方式生成，–dry-run仅仅是试运行，并不实际在k8s集群中运行，通过指定-o yaml输出yaml格式文件，生成后给基于模版修改即可，如下： [root@node-1 demo]# kubectl create deployment demo --image=nginx:latest --dry-run -o yaml apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: demo name: demo spec: replicas: 1 selector: matchLabels: app: demo strategy: {} template: metadata: creationTimestamp: null labels: app: demo spec: containers: - image: nginx:latest name: nginx resources: {} status: {} 2、explain命令，explain命令堪称是语法查询器，可以查到每个字段的含义，使用说明和使用方式，如想要查看Pod的spec中containers其他支持的字段，可以通过kubectl explain Pod.spec.containers的方式查询，如下： [root@node-1 demo]# kubectl explain Pods.spec.containers KIND: Pod VERSION: v1 RESOURCE: containers \u003c[]Object\u003e DESCRIPTION: List of containers belonging to the pod. Containers cannot currently be added or removed. There must be at least one container in a Pod. Cannot be updated. A single application container that you want to run within a pod. FIELDS: args \u003c[]string\u003e #命令参数 Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell image \u003cstring\u003e #镜像定义 Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets. ports \u003c[]Object\u003e #端口定义 List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \"0.0.0.0\" address inside a container will be accessible from the network. Cannot be updated. readinessProbe \u003cObject\u003e #可用健康检查 Periodic probe of container service readiness. Container will be removed from service endpoints if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes resources \u003cObject\u003e #资源设置 Compute Resources required by this container. Cannot be updated. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ ...省略部分输出... volumeMounts \u003c[]Object\u003e #挂载存储 Pod volumes to mount into the container's filesystem. Cannot be updated. workingDir \u003cstring\u003e Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image. Cannot be updated. 关于explain内容解释说明 表示后面接一个字符串 \u003c[]Object\u003e 表示后面是一个列表的对象，列表需要以-开始，且可以写多个 表示一个对象，对象内部包含多个属性 如继续上面的内容，如果需要查看resource资源定义，可以通过explain pods.spec.containers.resource来查看具体的使用方法。 通过上面两个工具的介绍，平时在日常工作中找到编写yaml文件部署应用的地图，建议手工多写几次，注意语法锁进，多写几次就熟悉了。Pod中设计到有很多的特性，如资源分配，健康检查，存储挂载等（参考附录文章），后续我们做详细介绍，Pod将以Template的方式嵌入到副本控制器如Deployments中。 附录 容器镜像介绍：https://kubernetes.io/docs/concepts/containers/images/ Pod介绍：https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/ Resource限定内存资源：https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/ Resource限定CPU资源：https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/ Pod挂载存储：https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/ Pod配置健康检查：https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ 『 转载 』该文章来源于网络，侵删。 ","date":"2019-08-04","objectID":"/05-%E5%88%9D%E8%AF%86%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5pod/:3:0","tags":["kubernetes"],"title":"05 初识核心概念pod","uri":"/05-%E5%88%9D%E8%AF%86%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5pod/"},{"categories":["转载","kubernetes","基础教程"],"content":"写在前面 上一篇文章中kubernetes系列教程（五）深入掌握核心概念pod初步介绍了yaml学习kubernetes中重要的一个概念pod，接下来介绍kubernetes系列教程pod的resource资源管理和pod的Quality of service服务质量。 1. Pod资源管理 ","date":"2019-08-04","objectID":"/06-kubernetes%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/:0:0","tags":["kubernetes"],"title":"06 Kubernetes资源管理和服务质量","uri":"/06-kubernetes%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.1 resource定义 容器运行过程中需要分配所需的资源，如何与cggroup联动配合呢？答案是通过定义resource来实现资源的分配，资源的分配单位主要是cpu和memory，资源的定义分两种：requests和limits，requests表示请求资源，主要用于初始kubernetes调度pod时的依据，表示必须满足的分配资源;limits表示资源的限制，即pod不能超过limits定义的限制大小，超过则通过cggroup限制，pod中定义资源可以通过下面四个字段定义： spec.container[].resources.requests.cpu 请求cpu资源的大小，如0.1个cpu和100m表示分配1/10个cpu； spec.container[].resources.requests.memory 请求内存大小，单位可用M，Mi，G，Gi表示； spec.container[].resources.limits.cpu 限制cpu的大小，不能超过阀值，cggroup中限制的值； spec.container[].resources.limits.memory 限制内存的大小，不能超过阀值，超过会发生OOM； 1、开始学习如何定义pod的resource资源,如下以定义nginx-demo为例，容器请求cpu资源为250m，限制为500m，请求内存资源为128Mi，限制内存资源为256Mi，当然也可以定义多个容器的资源，多个容器相加就是pod的资源总资源，如下： [root@node-1 demo]#cat nginx-resource.yaml apiVersion: v1 kind: Pod metadata: name: nginx-demo labels: name: nginx-demo spec: containers: - name: nginx-demo image: nginx:1.7.9 imagePullPolicy: IfNotPresent ports: - name: nginx-port-80 protocol: TCP containerPort: 80 resources: requests: cpu: 0.25 memory: 128Mi limits: cpu: 500m memory: 256Mi 2、应用pod的配置定义(如之前的pod还存在，先将其删除kubectl delete pod ),或pod命名为另外一个名 [root@node-1 demo]# kubectl apply -f nginx-resource.yaml pod/nginx-demo created 3、查看pod资源的分配详情 [root@node-1 demo]# kubectl get pods NAME READY STATUS RESTARTS AGE demo-7b86696648-8bq7h 1/1 Running 0 12d demo-7b86696648-8qp46 1/1 Running 0 12d demo-7b86696648-d6hfw 1/1 Running 0 12d nginx-demo 1/1 Running 0 94s [root@node-1 demo]# kubectl describe pods nginx-demo Name: nginx-demo Namespace: default Priority: 0 Node: node-3/10.254.100.103 Start Time: Sat, 28 Sep 2019 12:10:49 +0800 Labels: name=nginx-demo Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"labels\":{\"name\":\"nginx-demo\"},\"name\":\"nginx-demo\",\"namespace\":\"default\"},\"sp... Status: Running IP: 10.244.2.13 Containers: nginx-demo: Container ID: docker://55d28fdc992331c5c58a51154cd072cd6ae37e03e05ae829a97129f85eb5ed79 Image: nginx:1.7.9 Image ID: docker-pullable://nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 Port: 80/TCP Host Port: 0/TCP State: Running Started: Sat, 28 Sep 2019 12:10:51 +0800 Ready: True Restart Count: 0 Limits: #限制资源 cpu: 500m memory: 256Mi Requests: #请求资源 cpu: 250m memory: 128Mi Environment: \u003cnone\u003e ...省略... 4、Pod的资源如何分配呢？毫无疑问是从node上分配的，当我们创建一个pod的时候如果设置了requests，kubernetes的调度器kube-scheduler会执行两个调度过程：filter过滤和weight称重，kube-scheduler会根据请求的资源过滤，把符合条件的node筛选出来，然后再进行排序，把最满足运行pod的node筛选出来，然后再特定的node上运行pod。调度算法和细节可以参考下kubernetes调度算法介绍。如下是node-3节点资源的分配详情： [root@node-1 ~]# kubectl describe node node-3 ...省略... Capacity: #节点上资源的总资源情况，1个cpu，2g内存，110个pod cpu: 1 ephemeral-storage: 51473888Ki hugepages-2Mi: 0 memory: 1882352Ki pods: 110 Allocatable: #节点容许分配的资源情况，部分预留的资源会排出在Allocatable范畴 cpu: 1 ephemeral-storage: 47438335103 hugepages-2Mi: 0 memory: 1779952Ki pods: 110 System Info: Machine ID: 0ea734564f9a4e2881b866b82d679dfc System UUID: FFCD2939-1BF2-4200-B4FD-8822EBFFF904 Boot ID: 293f49fd-8a7c-49e2-8945-7a4addbd88ca Kernel Version: 3.10.0-957.21.3.el7.x86_64 OS Image: CentOS Linux 7 (Core) Operating System: linux Architecture: amd64 Container Runtime Version: docker://18.6.3 Kubelet Version: v1.15.3 Kube-Proxy Version: v1.15.3 PodCIDR: 10.244.2.0/24 Non-terminated Pods: (3 in total) #节点上运行pod的资源的情况，除了nginx-demo之外还有多个pod Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- default nginx-demo 250m (25%) 500m (50%) 128Mi (7%) 256Mi (14%) 63m kube-system kube-flannel-ds-amd64-jp594 100m (10%) 100m (10%) 50Mi (2%) 50Mi (2%) 14d kube-system kube-proxy-mh2gq 0 (0%) 0 (0%) 0 (0%) 0 (0%) 12d Allocated resources: #已经分配的cpu和memory的资源情况 (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 350m (35%) 600m (60%) memory 178Mi (10%) 306Mi (17%) ephemeral-storage 0 (0%) 0 (0%) Events: \u003cnone\u003e ","date":"2019-08-04","objectID":"/06-kubernetes%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/:1:0","tags":["kubernetes"],"title":"06 Kubernetes资源管理和服务质量","uri":"/06-kubernetes%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2 资源分配原理 Pod的定义的资源requests和limits作用于kubernetes的调度器kube-sheduler上，实际上cpu和内存定义的资源会应用在container上，通过容器上的cggroup实现资源的隔离作用，接下来我们介绍下资源分配的原理。 spec.containers[].resources.requests.cpu 作用在CpuShares，表示分配cpu 的权重，争抢时的分配比例 spec.containers[].resources.requests.memory 主要用于kube-scheduler调度器，对容器没有设置意义 spec.containers[].resources.limits.cpu 作用CpuQuota和CpuPeriod，单位为微秒，计算方法为：CpuQuota/CpuPeriod，表示最大cpu最大可使用的百分比，如500m表示允许使用1个cpu中的50%资源 spec.containers[].resources.limits.memory 作用在Memory，表示容器最大可用内存大小，超过则会OOM 以上面定义的nginx-demo为例，研究下pod中定义的requests和limits应用在docker生效的参数： 1、查看pod所在的node节点，nginx-demo调度到node-3节点上 [root@node-1 ~]# kubectl get pods -o wide nginx-demo NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-demo 1/1 Running 0 96m 10.244.2.13 node-3 \u003cnone\u003e \u003cnone\u003e 2、获取容器的id号，可以通过kubectl describe pods nginx-demo的containerID获取到容器的id，或者登陆到node-3节点通过名称过滤获取到容器的id号，默认会有两个pod：一个通过pause镜像创建，另外一个通过应用镜像创建 [root@node-3 ~]# docker container list |grep nginx 55d28fdc9923 84581e99d807 \"nginx -g 'daemon of…\" 2 hours ago Up 2 hours k8s_nginx-demonginx-demo_default_66958ef7-507a-41cd-a688-7a4976c6a71e_0 2fe0498ea9b5 k8s.gcr.io/pause:3.1 \"/pause\" 2 hours ago Up 2 hours k8s_POD_nginx-demo_default_66958ef7-507a-41cd-a688-7a4976c6a71e_0 3、查看docker容器详情信息 [root@node-3 ~]# docker container inspect 55d28fdc9923 [ ...部分输出省略... { \"Image\": \"sha256:84581e99d807a703c9c03bd1a31cd9621815155ac72a7365fd02311264512656\", \"ResolvConfPath\": \"/var/lib/docker/containers/2fe0498ea9b5dfe1eb63eba09b1598a8dfd60ef046562525da4dcf7903a25250/resolv.conf\", \"HostConfig\": { \"Binds\": [ \"/var/lib/kubelet/pods/66958ef7-507a-41cd-a688-7a4976c6a71e/volumes/kubernetes.io~secret/default-token-5qwmc:/var/run/secrets/kubernetes.io/serviceaccount:ro\", \"/var/lib/kubelet/pods/66958ef7-507a-41cd-a688-7a4976c6a71e/etc-hosts:/etc/hosts\", \"/var/lib/kubelet/pods/66958ef7-507a-41cd-a688-7a4976c6a71e/containers/nginx-demo/1cc072ca:/dev/termination-log\" ], \"ContainerIDFile\": \"\", \"LogConfig\": { \"Type\": \"json-file\", \"Config\": { \"max-size\": \"100m\" } }, \"UTSMode\": \"\", \"UsernsMode\": \"\", \"ShmSize\": 67108864, \"Runtime\": \"runc\", \"ConsoleSize\": [ 0, 0 ], \"Isolation\": \"\", \"CpuShares\": 256, CPU分配的权重，作用在requests.cpu上 \"Memory\": 268435456, 内存分配的大小，作用在limits.memory上 \"NanoCpus\": 0, \"CgroupParent\": \"kubepods-burstable-pod66958ef7_507a_41cd_a688_7a4976c6a71e.slice\", \"BlkioWeight\": 0, \"BlkioWeightDevice\": null, \"BlkioDeviceReadBps\": null, \"BlkioDeviceWriteBps\": null, \"BlkioDeviceReadIOps\": null, \"BlkioDeviceWriteIOps\": null, \"CpuPeriod\": 100000, CPU分配的使用比例，和CpuQuota一起作用在limits.cpu上 \"CpuQuota\": 50000, \"CpuRealtimePeriod\": 0, \"CpuRealtimeRuntime\": 0, \"CpusetCpus\": \"\", \"CpusetMems\": \"\", \"Devices\": [], \"DeviceCgroupRules\": null, \"DiskQuota\": 0, \"KernelMemory\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 268435456, \"MemorySwappiness\": null, \"OomKillDisable\": false, \"PidsLimit\": 0, \"Ulimits\": null, \"CpuCount\": 0, \"CpuPercent\": 0, \"IOMaximumIOps\": 0, \"IOMaximumBandwidth\": 0, }, } ] ","date":"2019-08-04","objectID":"/06-kubernetes%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/:2:0","tags":["kubernetes"],"title":"06 Kubernetes资源管理和服务质量","uri":"/06-kubernetes%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.3. cpu资源测试 pod中cpu的限制主要通过requests.cpu和limits.cpu来定义，limits是不能超过的cpu大小，我们通过stress镜像来验证，stress是一个cpu和内存的压侧工具，通过指定args参数的定义压侧cpu的大小。监控pod的cpu和内存可通过kubectl top的方式来查看，依赖于监控组件如metric-server或promethus，当前没有安装，我们通过docker stats的方式来查看。 1、通过stress镜像定义一个pod，分配0.25个cores和最大限制0.5个core使用比例 [root@node-1 demo]# cat cpu-demo.yaml apiVersion: v1 kind: Pod metadata: name: cpu-demo namespace: default annotations: kubernetes.io/description: \"demo for cpu requests and\" spec: containers: - name: stress-cpu image: vish/stress resources: requests: cpu: 250m limits: cpu: 500m args: - -cpus - \"1\" 2、应用yaml文件生成pod [root@node-1 demo]# kubectl apply -f cpu-demo.yaml pod/cpu-demo created 3、查看pod资源分配详情 [root@node-1 demo]# kubectl describe pods cpu-demo Name: cpu-demo Namespace: default Priority: 0 Node: node-2/10.254.100.102 Start Time: Sat, 28 Sep 2019 14:33:12 +0800 Labels: \u003cnone\u003e Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{\"kubernetes.io/description\":\"demo for cpu requests and\"},\"name\":\"cpu-demo\",\"nam... kubernetes.io/description: demo for cpu requests and Status: Running IP: 10.244.1.14 Containers: stress-cpu: Container ID: docker://14f93767ad37b92beb91e3792678f60c9987bbad3290ae8c29c35a2a80101836 Image: progrium/stress Image ID: docker-pullable://progrium/stress@sha256:e34d56d60f5caae79333cee395aae93b74791d50e3841986420d23c2ee4697bf Port: \u003cnone\u003e Host Port: \u003cnone\u003e Args: -cpus 1 State: Waiting Reason: CrashLoopBackOff Last State: Terminated Reason: Error Exit Code: 1 Started: Sat, 28 Sep 2019 14:34:28 +0800 Finished: Sat, 28 Sep 2019 14:34:28 +0800 Ready: False Restart Count: 3 Limits: #cpu限制使用的比例 cpu: 500m Requests: #cpu请求的大小 cpu: 250m 4、登陆到特定的node节点，通过docker container stats查看容器的资源使用详情 在pod所属的node上通过top查看，cpu的使用率限制百分比为50%。 通过上面的验证可以得出结论，我们在stress容器中定义使用1个core，通过limits.cpu限定可使用的cpu大小是500m，测试验证pod的资源已在容器内部或宿主机上都严格限制在50%（node机器上只有一个cpu，如果有2个cpu则会分摊为25%）。 ","date":"2019-08-04","objectID":"/06-kubernetes%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/:3:0","tags":["kubernetes"],"title":"06 Kubernetes资源管理和服务质量","uri":"/06-kubernetes%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.4 memory资源测试 1、通过stress镜像测试验证requests.memory和limits.memory的生效范围，limits.memory定义容器可使用的内存资源大小，当超过内存设定的大小后容器会发生OOM，如下定义一个测试的容器，最大内存不能超过512M,使用stress镜像–vm-bytes定义压侧内存大小为256Mi [root@node-1 demo]# cat memory-demo.yaml apiVersion: v1 kind: Pod metadata: name: memory-stress-demo annotations: kubernetes.io/description: \"stress demo for memory limits\" spec: containers: - name: memory-stress-limits image: polinux/stress resources: requests: memory: 128Mi limits: memory: 512Mi command: [\"stress\"] args: [\"--vm\", \"1\", \"--vm-bytes\", \"256M\", \"--vm-hang\", \"1\"] 2、应用yaml文件生成pod [root@node-1 demo]# kubectl apply -f memory-demo.yaml pod/memory-stress-demo created [root@node-1 demo]# kubectl get pods memory-stress-demo -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES memory-stress-demo 1/1 Running 0 41s 10.244.1.19 node-2 \u003cnone\u003e \u003cnone\u003e 3、查看资源的分配情况 [root@node-1 demo]# kubectl describe pods memory-stress-demo Name: memory-stress-demo Namespace: default Priority: 0 Node: node-2/10.254.100.102 Start Time: Sat, 28 Sep 2019 15:13:06 +0800 Labels: \u003cnone\u003e Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{\"kubernetes.io/description\":\"stress demo for memory limits\"},\"name\":\"memory-str... kubernetes.io/description: stress demo for memory limits Status: Running IP: 10.244.1.16 Containers: memory-stress-limits: Container ID: docker://c7408329cffab2f10dd860e50df87bd8671e65a0f8abb4dae96d059c0cb6bb2d Image: polinux/stress Image ID: docker-pullable://polinux/stress@sha256:6d1825288ddb6b3cec8d3ac8a488c8ec2449334512ecb938483fc2b25cbbdb9a Port: \u003cnone\u003e Host Port: \u003cnone\u003e Command: stress Args: --vm 1 --vm-bytes 256Mi --vm-hang 1 State: Waiting Reason: CrashLoopBackOff Last State: Terminated Reason: Error Exit Code: 1 Started: Sat, 28 Sep 2019 15:14:08 +0800 Finished: Sat, 28 Sep 2019 15:14:08 +0800 Ready: False Restart Count: 3 Limits: #内存限制大小 memory: 512Mi Requests: #内存请求大小 memory: 128Mi Environment: \u003cnone\u003e Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-5qwmc (ro) 4、查看容器内存资源的使用情况，分配256M内存，最大可使用为512Mi，利用率为50%，此时没有超过limits限制的大小，容器运行正常 5、当容器内部超过内存的大小会怎么样呢，我们将–vm-byte设置为513M，容器会尝试运行，超过内存后会OOM，kube-controller-manager会不停的尝试重启容器,RESTARTS的次数会不停的增加。 [root@node-1 demo]# cat memory-demo.yaml apiVersion: v1 kind: Pod metadata: name: memory-stress-demo annotations: kubernetes.io/description: \"stress demo for memory limits\" spec: containers: - name: memory-stress-limits image: polinux/stress resources: requests: memory: 128Mi limits: memory: 512Mi command: [\"stress\"] args: [\"--vm\", \"1\", \"--vm-bytes\", \"520M\", \"--vm-hang\", \"1\"] . #容器中使用内存为520M 查看容器的状态为OOMKilled，RESTARTS的次数不断的增加，不停的尝试重启 [root@node-1 demo]# kubectl get pods memory-stress-demo NAME READY STATUS RESTARTS AGE memory-stress-demo 0/1 OOMKilled 3 60s 2. Pod服务质量 服务质量QOS（Quality of Service）主要用于pod调度和驱逐时参考的重要因素，不同的QOS其服务质量不同，对应不同的优先级，主要分为三种类型的Qos： BestEffort 尽最大努力分配资源，默认没有指定resource分配的Qos，优先级最低； Burstable 可波动的资源，至少需要分配到requests中的资源，常见的QOS； Guaranteed 完全可保障资源，requests和limits定义的资源相同，优先级最高。 ","date":"2019-08-04","objectID":"/06-kubernetes%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/:4:0","tags":["kubernetes"],"title":"06 Kubernetes资源管理和服务质量","uri":"/06-kubernetes%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.1 BestEffort最大努力 1、Pod中没有定义resource，默认的Qos策略为BestEffort,优先级别最低，当资源比较进展是需要驱逐evice时，优先驱逐BestEffort定义的Pod，如下定义一个BestEffort的Pod [root@node-1 demo]# cat nginx-qos-besteffort.yaml apiVersion: v1 kind: Pod metadata: name: nginx-qos-besteffort labels: name: nginx-qos-besteffort spec: containers: - name: nginx-qos-besteffort image: nginx:1.7.9 imagePullPolicy: IfNotPresent ports: - name: nginx-port-80 protocol: TCP containerPort: 80 resources: {} 2、创建pod并查看Qos策略，qosClass为BestEffort [root@node-1 demo]# kubectl apply -f nginx-qos-besteffort.yaml pod/nginx-qos-besteffort created 查看Qos策略 [root@node-1 demo]# kubectl get pods nginx-qos-besteffort -o yaml apiVersion: v1 kind: Pod metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"labels\":{\"name\":\"nginx-qos-besteffort\"},\"name\":\"nginx-qos-besteffort\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"nginx:1.7.9\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"nginx-qos-besteffort\",\"ports\":[{\"containerPort\":80,\"name\":\"nginx-port-80\",\"protocol\":\"TCP\"}],\"resources\":{}}]}} creationTimestamp: \"2019-09-28T11:12:03Z\" labels: name: nginx-qos-besteffort name: nginx-qos-besteffort namespace: default resourceVersion: \"1802411\" selfLink: /api/v1/namespaces/default/pods/nginx-qos-besteffort uid: 56e4a2d5-8645-485d-9362-fe76aad76e74 spec: containers: - image: nginx:1.7.9 imagePullPolicy: IfNotPresent name: nginx-qos-besteffort ports: - containerPort: 80 name: nginx-port-80 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log ...省略... status: hostIP: 10.254.100.102 phase: Running podIP: 10.244.1.21 qosClass: BestEffort #Qos策略 startTime: \"2019-09-28T11:12:03Z\" 3、删除测试Pod [root@node-1 demo]# kubectl delete pods nginx-qos-besteffort pod \"nginx-qos-besteffort\" deleted ","date":"2019-08-04","objectID":"/06-kubernetes%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/:5:0","tags":["kubernetes"],"title":"06 Kubernetes资源管理和服务质量","uri":"/06-kubernetes%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.2 Burstable可波动 1、Pod的服务质量为Burstable，仅次于Guaranteed的服务质量，至少需要一个container定义了requests，且requests定义的资源小于limits资源 [root@node-1 demo]# cat nginx-qos-burstable.yaml apiVersion: v1 kind: Pod metadata: name: nginx-qos-burstable labels: name: nginx-qos-burstable spec: containers: - name: nginx-qos-burstable image: nginx:1.7.9 imagePullPolicy: IfNotPresent ports: - name: nginx-port-80 protocol: TCP containerPort: 80 resources: requests: cpu: 100m memory: 128Mi limits: cpu: 200m memory: 256Mi 2、应用yaml文件生成pod并查看Qos类型 [root@node-1 demo]# kubectl apply -f nginx-qos-burstable.yaml pod/nginx-qos-burstable created 查看Qos类型 [root@node-1 demo]# kubectl describe pods nginx-qos-burstable Name: nginx-qos-burstable Namespace: default Priority: 0 Node: node-2/10.254.100.102 Start Time: Sat, 28 Sep 2019 19:27:37 +0800 Labels: name=nginx-qos-burstable Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"labels\":{\"name\":\"nginx-qos-burstable\"},\"name\":\"nginx-qos-burstable\",\"namespa... Status: Running IP: 10.244.1.22 Containers: nginx-qos-burstable: Container ID: docker://d1324b3953ba6e572bfc63244d4040fee047ed70138b5a4bad033899e818562f Image: nginx:1.7.9 Image ID: docker-pullable://nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 Port: 80/TCP Host Port: 0/TCP State: Running Started: Sat, 28 Sep 2019 19:27:39 +0800 Ready: True Restart Count: 0 Limits: cpu: 200m memory: 256Mi Requests: cpu: 100m memory: 128Mi Environment: \u003cnone\u003e Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-5qwmc (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-5qwmc: Type: Secret (a volume populated by a Secret) SecretName: default-token-5qwmc Optional: false QoS Class: Burstable #服务质量是可波动的Burstable Node-Selectors: \u003cnone\u003e Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 95s default-scheduler Successfully assigned default/nginx-qos-burstable to node-2 Normal Pulled 94s kubelet, node-2 Container image \"nginx:1.7.9\" already present on machine Normal Created 94s kubelet, node-2 Created container nginx-qos-burstable Normal Started 93s kubelet, node-2 Started container nginx-qos-burstable ","date":"2019-08-04","objectID":"/06-kubernetes%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/:6:0","tags":["kubernetes"],"title":"06 Kubernetes资源管理和服务质量","uri":"/06-kubernetes%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/"},{"categories":["转载","kubernetes","基础教程"],"content":"2.3 Guaranteed完全保障 1、resource中定义的cpu和memory必须包含有requests和limits，切requests和limits的值必须相同，其优先级别最高，当出现调度和驱逐时优先保障该类型的Qos,如下定义一个nginx-qos-guaranteed的容器，requests.cpu和limits.cpu相同，同理requests.memory和limits.memory. [root@node-1 demo]# cat nginx-qos-guaranteed.yaml apiVersion: v1 kind: Pod metadata: name: nginx-qos-guaranteed labels: name: nginx-qos-guaranteed spec: containers: - name: nginx-qos-guaranteed image: nginx:1.7.9 imagePullPolicy: IfNotPresent ports: - name: nginx-port-80 protocol: TCP containerPort: 80 resources: requests: cpu: 200m memory: 256Mi limits: cpu: 200m memory: 256Mi 2、应用yaml文件生成pod并查看pod的Qos类型为可完全保障Guaranteed [root@node-1 demo]# kubectl apply -f nginx-qos-guaranteed.yaml pod/nginx-qos-guaranteed created [root@node-1 demo]# kubectl describe pods nginx-qos-guaranteed Name: nginx-qos-guaranteed Namespace: default Priority: 0 Node: node-2/10.254.100.102 Start Time: Sat, 28 Sep 2019 19:37:15 +0800 Labels: name=nginx-qos-guaranteed Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"labels\":{\"name\":\"nginx-qos-guaranteed\"},\"name\":\"nginx-qos-guaranteed\",\"names... Status: Running IP: 10.244.1.23 Containers: nginx-qos-guaranteed: Container ID: docker://cf533e0e331f49db4e9effb0fbb9249834721f8dba369d281c8047542b9f032c Image: nginx:1.7.9 Image ID: docker-pullable://nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 Port: 80/TCP Host Port: 0/TCP State: Running Started: Sat, 28 Sep 2019 19:37:16 +0800 Ready: True Restart Count: 0 Limits: cpu: 200m memory: 256Mi Requests: cpu: 200m memory: 256Mi Environment: \u003cnone\u003e Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-5qwmc (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-5qwmc: Type: Secret (a volume populated by a Secret) SecretName: default-token-5qwmc Optional: false QoS Class: Guaranteed #服务质量为可完全保障Guaranteed Node-Selectors: \u003cnone\u003e Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 25s default-scheduler Successfully assigned default/nginx-qos-guaranteed to node-2 Normal Pulled 24s kubelet, node-2 Container image \"nginx:1.7.9\" already present on machine Normal Created 24s kubelet, node-2 Created container nginx-qos-guaranteed Normal Started 24s kubelet, node-2 Started container nginx-qos-guaranteed 写在最后 本章是kubernetes系列教程第六篇文章，通过介绍resource资源的分配和服务质量Qos，关于resource有节点使用建议： requests和limits资源定义推荐不超过1:2，避免分配过多资源而出现资源争抢，发生OOM； pod中默认没有定义resource，推荐给namespace定义一个limitrange，确保pod能分到资源； 防止node上资源过度而出现机器hang住或者OOM，建议node上设置保留和驱逐资源，如保留资源–system-reserved=cpu=200m,memory=1G，驱逐条件–eviction hard=memory.available\u003c500Mi。 附录 容器计算资源管理：https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ pod内存资源管理：https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/ pod cpu资源管理：https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/ 服务质量QOS：https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/ Docker关于CPU的限制：https://www.cnblogs.com/sparkdev/p/8052522.html 『 转载 』该文章来源于网络，侵删。 ","date":"2019-08-04","objectID":"/06-kubernetes%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/:7:0","tags":["kubernetes"],"title":"06 Kubernetes资源管理和服务质量","uri":"/06-kubernetes%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/"},{"categories":["转载","kubernetes","基础教程"],"content":"写在前面 上一篇文章中kubernetes系列教程（六）kubernetes资源管理和服务质量初步介绍了kubernetes中的resource资源调度和服务质量Qos，介绍了kubernetes中如何定义pod的资源和资源调度，以及设置resource之后的优先级别Qos，接下来介绍kubernetes系列教程pod的调度机制。 1. Pod调度 ","date":"2019-08-04","objectID":"/07-%E6%B7%B1%E5%85%A5%E7%8E%A9%E8%BD%ACpod%E8%B0%83%E5%BA%A6/:0:0","tags":["kubernetes"],"title":"07 深入玩转pod调度","uri":"/07-%E6%B7%B1%E5%85%A5%E7%8E%A9%E8%BD%ACpod%E8%B0%83%E5%BA%A6/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.1 pod调度概述 kubernets是容器编排引擎，其中最主要的一个功能是容器的调度，通过kube-scheduler实现容器的完全自动化调度，调度周期分为：调度周期Scheduling Cycle和绑定周期Binding Cycle，其中调度周期细分为过滤filter和weight称重，按照指定的调度策略将满足运行pod节点的node赛选出来，然后进行排序；绑定周期是经过kube-scheduler调度优选的pod后，由特定的node节点watch然后通过kubelet运行。 过滤阶段包含预选Predicate和scoring排序，预选是筛选满足条件的node，排序是最满足条件的node打分并排序，预选的算法包含有： CheckNodeConditionPred 节点是否ready MemoryPressure 节点内存是否压力大（内存是否足够） DiskPressure 节点磁盘压力是否大（空间是否足够） PIDPressure 节点Pid是否有压力（Pid进程是否足够） GeneralPred 匹配pod.spec.hostname字段 MatchNodeSelector 匹配pod.spec.nodeSelector标签 PodFitsResources 判断resource定义的资源是否满足 PodToleratesNodeTaints 能容忍的污点pod.spec.tolerations CheckNodeLabelPresence CheckServiceAffinity CheckVolumeBinding NoVolumeZoneConflict 过滤条件需要检查node上满足的条件，可以通过kubectl describe node node-id方式查看，如下图： 优选调度算法有： least_requested 资源消耗最小的节点 balanced_resource_allocation 各项资源消耗最均匀的节点 node_prefer_avoid_pods 节点倾向 taint_toleration 污点检测，检测有污点条件的node，得分越低 selector_spreading 节点selector interpod_affinity pod亲和力遍历 most_requested 资源消耗最大的节点 node_label node标签 ","date":"2019-08-04","objectID":"/07-%E6%B7%B1%E5%85%A5%E7%8E%A9%E8%BD%ACpod%E8%B0%83%E5%BA%A6/:1:0","tags":["kubernetes"],"title":"07 深入玩转pod调度","uri":"/07-%E6%B7%B1%E5%85%A5%E7%8E%A9%E8%BD%ACpod%E8%B0%83%E5%BA%A6/"},{"categories":["转载","kubernetes","基础教程"],"content":"1. 2 指定nodeName调度 nodeName是PodSpec中的一个字段，可以通过pod.spec.nodeName指定将pod调度到某个具体的node节点上，该字段比较特殊一般都为空，如果有设置nodeName字段，kube-scheduler会直接跳过调度，在特定节点上通过kubelet启动pod。通过nodeName调度并非是集群的智能调度，通过指定调度的方式可能会存在资源不均匀的情况，建议设置Guaranteed的Qos，防止资源不均时候Pod被驱逐evince。如下以创建一个pod运行在node-3上为例： 编写yaml将pod指定在node-3节点上运行 [root@node-1 demo]# cat nginx-nodeName.yaml apiVersion: v1 kind: Pod metadata: name: nginx-run-on-nodename annotations: kubernetes.io/description: \"Running the Pod on specific nodeName\" spec: containers: - name: nginx-run-on-nodename image: nginx:latest ports: - name: http-80-port protocol: TCP containerPort: 80 nodeName: node-3 #通过nodeName指定将nginx-run-on-nodename运行在特定节点node-3上 运行yaml配置使之生效 [root@node-1 demo]# kubectl apply -f nginx-nodeName.yaml pod/nginx-run-on-nodename created 查看确认pod的运行情况，已运行在node-3节点 [root@node-1 demo]# kubectl get pods nginx-run-on-nodename -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-run-on-nodename 1/1 Running 0 6m52s 10.244.2.15 node-3 \u003cnone\u003e \u003cnone\u003e ","date":"2019-08-04","objectID":"/07-%E6%B7%B1%E5%85%A5%E7%8E%A9%E8%BD%ACpod%E8%B0%83%E5%BA%A6/:2:0","tags":["kubernetes"],"title":"07 深入玩转pod调度","uri":"/07-%E6%B7%B1%E5%85%A5%E7%8E%A9%E8%BD%ACpod%E8%B0%83%E5%BA%A6/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.2. 通过nodeSelector调度 nodeSelector是PodSpec中的一个字段，nodeSelector是最简单实现将pod运行在特定node节点的实现方式，其通过指定key和value键值对的方式实现，需要node设置上匹配的Labels，节点调度的时候指定上特定的labels即可。如下以node-2添加一个app:web的labels，调度pod的时候通过nodeSelector选择该labels： 给node-2添加labels [root@node-1 demo]# kubectl label node node-2 app=web node/node-2 labeled 查看校验labels设置情况，node-2增加多了一个app=web的labels [root@node-1 demo]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS node-1 Ready master 15d v1.15.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node-1,kubernetes.io/os=linux,node-role.kubernetes.io/master= node-2 Ready \u003cnone\u003e 15d v1.15.3 app=web,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node-2,kubernetes.io/os=linux node-3 Ready \u003cnone\u003e 15d v1.15.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node-3,kubernetes.io/os=linux 通过nodeSelector将pod调度到app=web所属的labels [root@node-1 demo]# cat nginx-nodeselector.yaml apiVersion: v1 kind: Pod metadata: name: nginx-run-on-nodeselector annotations: kubernetes.io/description: \"Running the Pod on specific node by nodeSelector\" spec: containers: - name: nginx-run-on-nodeselector image: nginx:latest ports: - name: http-80-port protocol: TCP containerPort: 80 nodeSelector: #通过nodeSelector将pod调度到特定的labels app: web 应用yaml文件生成pod [root@node-1 demo]# kubectl apply -f nginx-nodeselector.yaml pod/nginx-run-on-nodeselector created 检查验证pod的运行情况，已经运行在node-2节点 [root@node-1 demo]# kubectl get pods nginx-run-on-nodeselector -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-run-on-nodeselector 1/1 Running 0 51s 10.244.1.24 node-2 \u003cnone\u003e \u003cnone\u003e 系统默认预先定义有多种内置的labels，这些labels可以标识node的属性，如arch架构，操作系统类型，主机名等 beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=node-3 kubernetes.io/os=linux ","date":"2019-08-04","objectID":"/07-%E6%B7%B1%E5%85%A5%E7%8E%A9%E8%BD%ACpod%E8%B0%83%E5%BA%A6/:3:0","tags":["kubernetes"],"title":"07 深入玩转pod调度","uri":"/07-%E6%B7%B1%E5%85%A5%E7%8E%A9%E8%BD%ACpod%E8%B0%83%E5%BA%A6/"},{"categories":["转载","kubernetes","基础教程"],"content":"1.3 node Affinity and anti-affinity affinity/anti-affinity和nodeSelector功能相类似，相比于nodeSelector，affinity的功能更加丰富，未来会取代nodeSelector，affinity增加了如下的一些功能增强： 表达式更加丰富，匹配方式支持多样，如In,NotIn, Exists, DoesNotExist. Gt, and Lt； 可指定soft和preference规则，soft表示需要满足的条件，通过requiredDuringSchedulingIgnoredDuringExecution来设置，preference则是优选选择条件，通过preferredDuringSchedulingIgnoredDuringExecution指定 affinity提供两种级别的亲和和反亲和：基于node的node affinity和基于pod的inter-pod affinity/anti-affinity，node affinity是通过node上的labels来实现亲和力的调度，而pod affinity则是通过pod上的labels实现亲和力的调度，两者作用的范围有所不同。 下面通过一个例子来演示node affinity的使用，requiredDuringSchedulingIgnoredDuringExecution指定需要满足的条件，preferredDuringSchedulingIgnoredDuringExecution指定优选的条件，两者之间取与关系。 查询node节点的labels，默认包含有多个labels，如kubernetes.io/hostname [root@node-1 ~]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS node-1 Ready master 15d v1.15.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node-1,kubernetes.io/os=linux,node-role.kubernetes.io/master= node-2 Ready \u003cnone\u003e 15d v1.15.3 app=web,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node-2,kubernetes.io/os=linux node-3 Ready \u003cnone\u003e 15d v1.15.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node-3,kubernetes.io/os=linux 通过node affiinity实现调度，通过requiredDuringSchedulingIgnoredDuringExecution指定满足条件kubernetes.io/hostname为node-2和node-3，通过preferredDuringSchedulingIgnoredDuringExecution优选条件需满足app=web的labels [root@node-1 demo]# cat nginx-node-affinity.yaml apiVersion: v1 kind: Pod metadata: name: nginx-run-node-affinity annotations: kubernetes.io/description: \"Running the Pod on specific node by node affinity\" spec: containers: - name: nginx-run-node-affinity image: nginx:latest ports: - name: http-80-port protocol: TCP containerPort: 80 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node-1 - node-2 - node-3 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: app operator: In values: [\"web\"] 应用yaml文件生成pod [root@node-1 demo]# kubectl apply -f nginx-node-affinity.yaml pod/nginx-run-node-affinity created 确认pod所属的node节点，满足require和 preferre条件的节点是node-2 [root@node-1 demo]# kubectl get pods --show-labels nginx-run-node-affinity -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELS nginx-run-node-affinity 1/1 Running 0 106s 10.244.1.25 node-2 \u003cnone\u003e \u003cnone\u003e \u003cnone\u003e 写在最后 本文介绍了kubernetes中的调度机制，默认创建pod是全自动调度机制，调度由kube-scheduler实现，调度过程分为两个阶段调度阶段（过滤和沉重排序）和绑定阶段（在node上运行pod）。通过干预有四种方式： 指定nodeName 通过nodeSelector 通过node affinity和anti-affinity 通过pod affinity和anti-affinity 附录 调度框架介绍：https://kubernetes.io/docs/concepts/configuration/scheduling-framework/ Pod调度方法：https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ 『 转载 』该文章来源于网络，侵删。 ","date":"2019-08-04","objectID":"/07-%E6%B7%B1%E5%85%A5%E7%8E%A9%E8%BD%ACpod%E8%B0%83%E5%BA%A6/:4:0","tags":["kubernetes"],"title":"07 深入玩转pod调度","uri":"/07-%E6%B7%B1%E5%85%A5%E7%8E%A9%E8%BD%ACpod%E8%B0%83%E5%BA%A6/"},{"categories":null,"content":"岂能尽如人意，但求无愧我心。 本站地址：http://x.agou-ops.top 主站地址：https://agou-ops.top my Docs：https://agou-ops.top/beforeWork Docs 备用站点1： http://bak.agou-ops.top/mydocs Docs 备用站点2：https://d.agou-ops.top myStudyNote：https://agou-ops.top/myStudyNote 发布站点：http://fabu.agou-ops.top/ ","date":"2019-08-02","objectID":"/about/:0:0","tags":null,"title":"关于博主","uri":"/about/"},{"categories":["docs","shortcodes","index"],"content":"What a Shortcode is Hugo loves Markdown because of its simple content format, but there are times when Markdown falls short. Often, content authors are forced to add raw HTML (e.g., video \u003ciframes\u003e) to Markdown content. We think this contradicts the beautiful simplicity of Markdown’s syntax. Hugo created shortcodes to circumvent these limitations. A shortcode is a simple snippet inside a content file that Hugo will render using a predefined template. Note that shortcodes will not work in template files. If you need the type of drop-in functionality that shortcodes provide but in a template, you most likely want a [partial template][partials] instead. In addition to cleaner Markdown, shortcodes can be updated any time to reflect new classes, techniques, or standards. At the point of site generation, Hugo shortcodes will easily merge in your changes. You avoid a possibly complicated search and replace operation. More details: https://gohugo.io/content-management/shortcodes/ ","date":"2019-03-04","objectID":"/shortcodes-preview/:1:0","tags":["preview","shortcodes"],"title":"Shortcodes Preview","uri":"/shortcodes-preview/"},{"categories":["docs","shortcodes","index"],"content":"blockquotes Normal quote: This is a simple quote. Quote with author: This is a quote with only an Author named Author2. Quote with author and source: This is a quote from Author3 and source “source.” Quote with author and link: This is a quote from Author4 and links to https://www.google.com. Quote with author, link and title: This is a quote from Author5 and links to https://www.google.com with title “Google.” Quote with author and a link longer than 32 characters, string is first cut at 32 characters then everything after the last forward slash is removed This is a quote from Author5 and links to https://twitter.com/CryptoGangsta/status/716427930126196737 which is longer than 32 characters. And this is a new line in the quote with the HTML br tag. Test from the Octopress blockquote page at: http://octopress.org/docs/plugins/blockquote/ Over the past 24 hours I’ve been reflecting on my life \u0026 I’ve realized only one thing. I need a medieval battle axe. ","date":"2019-03-04","objectID":"/shortcodes-preview/:2:0","tags":["preview","shortcodes"],"title":"Shortcodes Preview","uri":"/shortcodes-preview/"},{"categories":["docs","shortcodes","index"],"content":"music ","date":"2019-03-04","objectID":"/shortcodes-preview/:3:0","tags":["preview","shortcodes"],"title":"Shortcodes Preview","uri":"/shortcodes-preview/"},{"categories":["docs","shortcodes","index"],"content":"gist(+1) We can embed the gist in our content via username and gist ID pulled from the URL: {{\u003c gist AGou-ops 5e0b2df2f0d23c65742a633ed0225cf6 \u003e}} Display: ","date":"2019-03-04","objectID":"/shortcodes-preview/:4:0","tags":["preview","shortcodes"],"title":"Shortcodes Preview","uri":"/shortcodes-preview/"},{"categories":["docs","shortcodes","index"],"content":"expand The Expand shortcode displays an expandable/collapsible section of text on your page. Here is an example Yes !. ","date":"2019-03-04","objectID":"/shortcodes-preview/:5:0","tags":["preview","shortcodes"],"title":"Shortcodes Preview","uri":"/shortcodes-preview/"},{"categories":["docs","shortcodes","index"],"content":"Usage this shortcode takes exactly one optional parameter to define the text that appears next to the expand/collapse icon. (default is “Click to expand”) Yes !. ","date":"2019-03-04","objectID":"/shortcodes-preview/:5:1","tags":["preview","shortcodes"],"title":"Shortcodes Preview","uri":"/shortcodes-preview/"},{"categories":["docs","shortcodes","index"],"content":"youtube ","date":"2019-03-04","objectID":"/shortcodes-preview/:6:0","tags":["preview","shortcodes"],"title":"Shortcodes Preview","uri":"/shortcodes-preview/"},{"categories":["docs","shortcodes","index"],"content":"vimeo ","date":"2019-03-04","objectID":"/shortcodes-preview/:7:0","tags":["preview","shortcodes"],"title":"Shortcodes Preview","uri":"/shortcodes-preview/"},{"categories":["docs","shortcodes","index"],"content":"youku","date":"2019-03-04","objectID":"/shortcodes-preview/:8:0","tags":["preview","shortcodes"],"title":"Shortcodes Preview","uri":"/shortcodes-preview/"},{"categories":["docs","shortcodes","index"],"content":"Thanks for liwenyip/hugo-easy-gallery \u0026 Zebradil · Pull Request #48 . Now, we could use {{\u003c gallery \u003e}} shortcode in hugo-theme-jane. ","date":"2019-03-03","objectID":"/image-preview/:0:0","tags":["preview","image","shortcodes"],"title":"Image Preview","uri":"/image-preview/"},{"categories":["docs","shortcodes","index"],"content":"Normal Image This is an image in static/image folder. ![This is an image in `static/image` folder.](https://agou-images.oss-cn-qingdao.aliyuncs.com/violet-evergarden/Screenshot_2020-03-29-21-40-19.png) ","date":"2019-03-03","objectID":"/image-preview/:1:0","tags":["preview","image","shortcodes"],"title":"Image Preview","uri":"/image-preview/"},{"categories":["docs","shortcodes","index"],"content":"{{\u003c figure \u003e}} shortcode ","date":"2019-03-03","objectID":"/image-preview/:2:0","tags":["preview","image","shortcodes"],"title":"Image Preview","uri":"/image-preview/"},{"categories":["docs","shortcodes","index"],"content":"figure image with title {{\u003c figure src=\"/image/test.png\" title=\"figure image with title\" \u003e}} ","date":"2019-03-03","objectID":"/image-preview/:2:1","tags":["preview","image","shortcodes"],"title":"Image Preview","uri":"/image-preview/"},{"categories":["docs","shortcodes","index"],"content":"figure image with caption {{\u003c figure src=\"/image/example.jpg\" caption=\"figure image with caption figure image with caption figure image with caption figure image with caption figure image with caption\" \u003e}} ","date":"2019-03-03","objectID":"/image-preview/:2:2","tags":["preview","image","shortcodes"],"title":"Image Preview","uri":"/image-preview/"},{"categories":["docs","shortcodes","index"],"content":"more {{\u003c figure \u003e}} shortcode usage Specifying your image files: {{\u003c figure src=\"thumb.jpg\" link=\"image.jpg\" \u003e}} will use thumb.jpg for thumbnail and image.jpg for lightbox {{\u003c figure src=\"image.jpg\" \u003e}} or {{\u003c figure link=\"image.jpg\" \u003e}} will use image.jpg for both thumbnail and lightbox {{\u003c figure link=\"image.jpg\" thumb=\"-small\" \u003e}} will use image-small.jpg for thumbnail and image.jpg for lightbox Optional parameters: All the features/parameters of Hugo’s built-in figure shortcode work as normal, i.e. src, link, title, caption, class, attr (attribution), attrlink, alt size (e.g. size=\"1024x768\") pre-defines the image size for PhotoSwipe. Use this option if you don’t want to pre-load the linked image to determine its size. class allows you to set any custom classes you want on the \u003cfigure\u003e tag. Optional parameters for standalone {{\u003c figure \u003e}} shortcodes only (i.e. don’t use on {{\u003c figure \u003e}} inside {{\u003c gallery \u003e}} - strange things may happen if you do): caption-position and caption-effect work the same as for the {{\u003c gallery \u003e}} shortcode (see below). width defines the max-width of the image displayed on the page. If using a thumbnail for a standalone figure, set this equal to your thumbnail’s native width to make the captions behave properly (or feel free to come up with a better solution and submit a pull request :-)). Also use this option if you don’t have a thumbnail and you don’t want the hi-res image to take up the entire width of the screen/container. class=\"no-photoswipe\" prevents a \u003cfigure\u003e from being loaded into PhotoSwipe. If you click on the figure you’ll instead a good ol’ fashioned hyperlink to a bigger image (or - if you haven’t specified a bigger image - the same one). ","date":"2019-03-03","objectID":"/image-preview/:2:3","tags":["preview","image","shortcodes"],"title":"Image Preview","uri":"/image-preview/"},{"categories":["docs","shortcodes","index"],"content":"{{\u003c gallery \u003e}} shortcode ","date":"2019-03-03","objectID":"/image-preview/:3:0","tags":["preview","image","shortcodes"],"title":"Image Preview","uri":"/image-preview/"},{"categories":["docs","shortcodes","index"],"content":"simple gallery To specify a directory of image files: {{\u003c gallery dir=\"/img/your-directory-of-images/\" \u003e}} The images are automatically captioned with the file name. [image].jpg is used for the hi-res image, and [image]-thumb.jpg is used for the thumbnails. If [image]-thumb.jpg doesn’t exist, then [image].jpg will be used for both hi-res and thumbnail images. The default thumbnail suffix is -thumb, but you can specify a different one e.g. thumb=\"-small\" or thumb=\"_150x150\". ","date":"2019-03-03","objectID":"/image-preview/:3:1","tags":["preview","image","shortcodes"],"title":"Image Preview","uri":"/image-preview/"},{"categories":["docs","shortcodes","index"],"content":"To specify individual image files {{\u003c gallery \u003e}} {{\u003c figure src=\"image1.jpg\" \u003e}} {{\u003c figure src=\"image2.jpg\" \u003e}} {{\u003c figure src=\"image3.jpg\" \u003e}} {{\u003c /gallery \u003e}} Optional parameters: caption-position - determines the captions’ position over the image. Options: bottom (default) center none hides captions on the page (they will only show in PhotoSwipe) caption-effect - determines if/how captions appear upon hover. Options: slide (default) fade none (captions always visible) hover-effect - determines if/how images change upon hover. Options: zoom (default) grow shrink slideup slidedown none hover-transition - determines if/how images change upon hover. Options: not set - smooth transition (default) none - hard transition ","date":"2019-03-03","objectID":"/image-preview/:3:2","tags":["preview","image","shortcodes"],"title":"Image Preview","uri":"/image-preview/"}]